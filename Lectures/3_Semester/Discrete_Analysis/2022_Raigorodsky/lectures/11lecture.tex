\textcolor{red}{Ждать теоремы Боллобаша до конца сессии не стоит. Я не готов ботать это за 4 дня с техом для отл10, меня устроит хор7 или отл8}

\subsection{Связность случайного графа}

\begin{note}
	Если явно не оговорено иного, то всегда подразумевается модель Эрдеша-Реньи $G(n, p)$.
\end{note}

\begin{note}
	Для чего изучать связность случайного графа? Ответ прост: в жизни это, например, стабильность сети серверов компании.
\end{note}

\begin{theorem} (без доказательства, просто как факт для приложения)
	Если $c > 3$, то $P(G \text{ связен}) \ge 1 - \frac{1}{n}$ при $n \ge 100$.
\end{theorem}

\begin{note}
	Скажем, у нас $n = 2000$ серверов. Идёт атака, где мы с вероятностью $q \approx 0.99$ теряем соединение между двумя серверами. Это соответствует $c \approx 3$, но что можно сказать про возможность передать информацию с одного сервера на другой? Это будет аж $1999 / 2000$, то есть почти всегда мы остаёмся в полном строю.
\end{note}

\begin{theorem}
	Пусть $p = p(n) = \frac{c\ln n}{n}$, $c > 0$. Тогда верно несколько утверждений:
	\begin{enumerate}
		\item Если $c < 1$, то асимптотически почти наверное $G$ не связен.
		
		\item Если $c > 1$, то асимптотически почти наверное случайный граф $G$ связен.
	\end{enumerate}
\end{theorem}

\begin{proof}~
	\begin{enumerate}
		\item Интуитивно: какую компоненту проще всего отколоть в графе? Естественно ответить, что вершину. Пусть $X = X(G)$ --- это число изолированных вершин в случайном графе, посчитаем матожидание этой величины:
		\[
			\E X = n \cdot (1 - p)^{n - 1} = n\exp\ps{-pn(1 + o(1))} = n\exp\ps{-\frac{c\ln n}{n} \cdot n(1 + o(1))} = n \cdot n^{-c(1 + o(1))}
		\]
		Последнее выражение стремится к $+\infty$ при $c < 1$ и к 0 при $c > 1$. Однако, как мы знаем, нельзя делать поспешных выводов из матожидания, нужны вероятности и дисперсия. Поэтому мы сделаем оценку на то, что есть хотя бы 1 <<оторванная>> вершина:
		\[
			P(X \ge 1) = 1 - P(X \le 0) = 1 - P(-X \ge 0) = 1 - P(\E X - X \ge \E X) \ge 1 - \frac{DX}{(\E X)^2}
		\]
		Снова говорим, что $X = X_1 + \ldots + X_n$, и расписываем дисперсию, раскрывая матожидание квадрата:
		\[
			\E X^2 = \E X + \sum_{i \neq j} \E X_i X_j
		\]
		$X_i X_j = 1 \Lra X_i = 1 \wedge X_j = 1 \Lra$ отсуствует $2(n - 2) + 1$ ребро (от каждой выбранной вершины в оставшиеся $n - 2$ и ещё одно между ними). Итого:
		\[
			\E X^2 = \E X + n(n - 1)(1 - p)^{2n - 3}
		\]
		Осталось показать, что дробь с дисперсией стремится к нулю:
		\begin{multline*}
			\frac{DX}{(\E X)^2} = \frac{\E X^2 - (\E X)^2}{(\E X)^2} = \frac{\E X + n(n - 1)(1 - p)^{2n - 3}}{(\E X)^2} - 1 =
			\\
			\frac{1}{\E X} + \frac{n(n - 1)(1 - p)^{2n - 3}}{n^2(1 - p)^{2n - 2}} - 1 = \frac{1}{\E X} + \ps{1 - \frac{1}{n}}(1 - p)^{-1} - 1 \xrightarrow[n \to \infty]{} 0
		\end{multline*}
		
		\item Как понять, что граф разорван? У него есть нетривиальная компонента. Следовательно, рассмотрим $Y = Y(G)$ --- число нетривиальных компонент случайного графа. Эту величину можно расписать по индикаторам, где номер отражает размер компоненты:
		\[
			Y = Y_1 + \ldots + Y_{n - 1}
		\]
		Чтобы снова оценить $P(Y \ge 1)$, нам потребуется неравенство Маркова и матожидание $\E Y$. Для него нам достаточно найти $\E Y_i$:
		\begin{multline*}
			\E Y_t = \sum_{I \subset V \colon |I| = t} P(I \text{ является компонентой в } G) \le
			\\
			\sum_{I \subset V \colon |I| = t} P(\text{Из $I$ в $G \bs I$ нет рёбер}) = C_n^t (1 - p)^{t(n - t)}
		\end{multline*}
		Итого, $\E Y \le \sum_{t = 1}^{n - 1} C_n^t (1 - p)^{t(n - t)}$. Методом пристального взгляда можно заметить 2 вещи:
		\begin{enumerate}
			\item Нам достаточно суммировать лишь половину слагаемых в силу симметрии
			
			\item При $t = 1$ мы получаем $\E X$ из предыдущего пункта, которое, как известно, стремится к нулю. Но это пока ничего не говорит про сумму слагаемых.
		\end{enumerate}
		Интуитивно понятно, что каждое следующее должно быть меньше предыдущего: отколоть 2 вершины тяжелее 1, 3 - ещё тяжелее и так далее. Однако, если мы попробуем вынести первое слагаемое за сумму, а оставшееся представить как сумму геометрической прогрессии, то ничего не получится. Нужно воспользоваться старым методом разбиения суммы на 2 части --- введём неизвестное $l(n)$ (а также обозначим выражение под знаком суммы за $a_t(n)$):
		\[
			\E Y \le \sum_{t = 1}^{l(n)} a_t(n) + \sum_{t = l(n) + 1}^{\floor{n / 2}} a_t(n)
		\]
		\begin{itemize}
			\item Для первой половины оценка через геометрическую прогрессию получится хорошей. Что предлагается сделать:
			\[
				\sum_{t = 1}^{l(n)} a_t(n) = a_1(n)\ps{1 + \frac{a_2(n)}{a_1(n)} + \frac{a_3(n)}{a_2(n)} \cdot \frac{a_2(n)}{a_1(n)} + \ldots}
			\]
			Посчитаем отношение соседей уже конкретно:
			\[
				\frac{a_{t + 1}}{a_t} = \frac{C_n^{t + 1}(1 - p)^{(t + 1)(n - t - 1)}}{C_n^t(1 - p)^{t(n - t)}} = \frac{n - t}{t + 1} (1 - p)^{-t + n - t - 1} < n(1 - p)^{n - 2t - 1} < n(1 - p)^{n - 2l(n) - 1}
			\]
			Обозначим последнюю оценку за $q(n)$. Тогда у нас есть такая оценка:
			\[
				\sum_{t = 1}^{l(n)} a_t(n) = a_1(n)(1 + q(n) + q^2(n) + \ldots + q^{l(n) - 1}(n))
			\]
			Для оценки надо, чтобы при достаточно большом $n$ множитель $q(n)$ ушёл в ноль. Нам подойдёт что угодно, что мало по сравнению с $n$, то есть $o(n)$. Тогда, начиная с больших $n$ и требования $l(n) = o(n)$, верна оценка:
			\[
				\sum_{t = 1}^{l(n)} a_t(n) \le a_1(n) \cdot \frac{1}{1 - q(n)} \xrightarrow[n \to \infty]{} 0
			\]
			
			\item Для остатка можем сделать максимально драконовскую оценку:
			\[
				\sum_{t = l(n) + 1}^{\floor{n / 2}} < n \cdot 2^n \cdot (1 - p)^{l(n) \cdot \frac{n}{2}}
			\]
			Пояснение: $n$ --- это оценка на число слагаемых, $2^n$ есть оценка на любой коэффициент, показатель у $(1 - p)$ --- это оценка $n - t \ge n / 2$ и $t \ge l(n)$. Теперь соберём все в экспоненту и ещё немного оценим:
			\[
				n \cdot 2^n \cdot (1 - p)^{l(n) \cdot \frac{n}{2}} \le \exp\ps{\ln n + n\ln 2 + \ps{-\frac{c\ln n}{n}} \cdot l(n) \cdot \frac{n}{2}}
			\]
			Осталось выбрать такое $l(n)$, что оно и $o(n)$, и в то же время сильнее $n$ при домножении на $\ln n$. Это, например, $l(n) = \floor{n / \ln n}$
		\end{itemize}
	\end{enumerate}
\end{proof}

\begin{note}
	Остаётся вопрос: <<Что будет при $c = 1$?>> На это отвечает следующая теорема, которая в курсе даётся без доказательства
\end{note}

\begin{theorem} (без доказательства)
	Пусть $p(n) = \frac{\ln n + \gamma}{n}$. Тогда верен предел:
	\[
		P(G \text{ связен}) \xrightarrow[n \to \infty]{} \exp(-e^{-\gamma})
	\]
\end{theorem}
\section{Введение}
\textbf{Определение.} Пусть $\Omega \subset \mathbb R \times \underbrace{\mathbb R^n \times \dots \times \mathbb R^n}_{\text{$k + 1$ раз}}$, $F: \Omega \to \mathbb R^m$.
\textit{Обыкновенным дифференциальным уравнением (или системой) $k$-ого порядка} (ОДУ) называется уравнение
\begin{equation}
    F(t, x, x', \dots, x^{(k)}) = 0.
\end{equation}
Его \textit{решением} называется функция $x: I \to \mathbb R^n$, где:
\begin{itemize}
    \item $I \subset \mathbb R$ --- интервал.
    \item $x \in C^k(I, \mathbb R^n)$ --- $k$ раз непрерывно дифференцируемая функция.
    \item Для любого $t \in I$ выполнено $(t, x(t), \dots, x^{(k)}(t)) \in \Omega$, то есть попадает в область определения $F$.
    \item $F(t, x(t), \dots, x^{(k)}(t)) \equiv 0$ на $I$.
\end{itemize}

Как правило, рассматривать совсем общие уравнения неинтерестно, так как они слишком сложные, поэтому мы обычно будем рассматривать частный случай.

\textbf{Определение.} Пусть $\Gamma \subset \mathbb R \times \underbrace{\mathbb R^n \times \dots \times \mathbb R^n}_{\text{$k$ раз}}$, $F: \Gamma \to \mathbb R^n$.
\textit{Нормальным ОДУ} (или системой) $k$-ого порядка называется уравнение
\begin{equation}
    x^{(k)} = F(t, x, x', \dots, x^{(k-1)}).
\end{equation}

\textbf{Примеры:}
\begin{enumerate}
    \item $x' = x$, решение --- $x(t) = ce^t$ для всех $c \in \mathbb R$ и интервалов $I \subset \mathbb R$.
        Решением является и функция и область определения.
    \item $t^2 + x^2 + (x')^2 = 0$. Решений нет, так как решение должно быть определено на \textit{интервале}, то есть только в нуле определить нельзя.
    \item $x' = 1 + x^2$. Понятно, что $x(t) = \tg(t)$ подходит, но что с областью определения?
        Нам подходят $t \in \mathbb R \setminus \{ \frac{\pi}{2} + \pi n~|~n \in \mathbb Z\}$, откуда получается, что решения --- это все функции, определённые на подынтервалах этого множества.
\end{enumerate}

\subsection{Простейшие типы дифференциальных уравнений}
\subsubsection{Уравнения с разделяющимися переменными}
Пусть $I, J$ --- интервалы, $f: I \to \mathbb R$, $g: J \to \mathbb R$ --- непрерывные функции. 

\textbf{Определение.} \textit{Уравнение с разделяющимися переменными} --- это уравнение вида $x' = f(t) g(x)$.


Рассмотрим два случая. 

Второй --- $g(\cdot)$ не обнуляется.
Зафиксируем какое-то решение $x(\cdot)$. Заметим, что получится тождество $x'(t) \equiv f(t) g(x)$. Тогда можно разделить на $g(x(t))$:
\[
    \frac{x'(t)}{g(x(t))} \equiv f(t)
\]
Проинтегрируем обе части. Здесь и далее под интегралом подразумевается какая-то фиксированная первообразная.
\[
    \int \frac{x'(t)}{g(x(t))} dt \equiv \int f(t)dt + C
\]
Внесём $x$ под дифференциал.
\[
    \int \frac{dx}{g(x(t))} \equiv \int f(t)dt + C
\]
Положим $G(t) = \int \frac{dx}{g(x)}$ и $F(t) = \int f(t) dt$, тогда получаем
\[
    G(x(t)) \equiv F(t) + C
\]
Теперь заметим, что $g(\cdot)$ --- это непрерывная функция на интервале, которая не обнуляется.
Иными словами, она строго больше нуля или строго меньше, а это значит, что $G(\cdot)$ строго возрастает или строго убывает, то есть к ней применима теорема об обратной функции.
\[
    x(t) \equiv G^{-1}(F(t) + C)
\]
Так как везде были тождества, сие уравнение эквивалентно исходному.
Важно только помнить, что $G^{-1}$ определена на $G(J)$, так что подходят не совсем все пары $(t, C)$.

Но что делать со случаем, когда $g(\cdot)$ всё-таки обнуляется?
Тут возникают всякие неприятности, как минимум, если для какого-то $\overline{x} \in J$ выполнено $g(\overline{x}) = 0$, то $x(t) \equiv \overline{x}$ --- решение для всех $t \in I$.
Но можно пойти дальше.

\textbf{Упражнение.} Пусть существует $\overline x \in J$, такое что $g(\overline x) = 0$, а при $x > \overline x$ выполнено $g(x) \ne 0$.
Пусть также найдётся $\overline t \in I$, такое что $f(\overline t) \ne 0$, а интеграл $\int_{\overline x}^x \frac{d\xi}{g(\xi)}$ сходится при $x > \overline x$.
Тогда найдётся функция $y(t)$ для $t > \overline t$ и функция
\[
    \widehat x(t) =
    \begin{cases}
        \overline x, & t \le \overline t \\
        y(t), & t > \overline t
    \end{cases},
\]
являющаяся решением.

Что это всё значит: мы нашли тривиальное решение $x(t) \equiv \overline x$, потом нашли решение уравнения $y(\cdot)$ методом выше (так как там уже $g(\cdot)$ не обнуляется), а теперь взяли, склеили их и получили новое решение.
В виде картинки:

\begin{figure}[ht]
    \centering
    \incfig{001}{0.4\linewidth}
    \caption{Третье решение.}
\end{figure}

\subsubsection{Линейные уравнения I порядка}
Пусть задан интервал $I \subset \mathbb R$ и две непрерывные функции $a, b: I \to \mathbb R$.

\textbf{Определение.} Линейное уравнение первого порядка --- уравнение вида $x' + a(t)x = b(t)$.
Если $b(t) \equiv 0$, то уравнение называется \textit{однородным}, иначе --- \textit{неоднородным}.

Однородное уравнение --- это уравнение с разделяющей переменной. Его мы уже умеем решать:
\[
    \frac{dx}{dt} = -a(t)x
\]
При $x(t) \equiv 0$ верно, далее рассматриваем на интервалах, на которых не обнуляется.
\[
    \frac{dx}{x} = -a(t) dt
\]
\[
    \ln \left( |x| \right) = -\int a(t) dt + C
\]
\[
    |x(t)| = \widetilde{C} \exp \left(-\int_{t_0}^t a(s) ds \right)
\]
\[
    x(t) = C \exp \left(- \int_{t_0}^t a(s) ds \right), t \in I
\]
Здесь $C$ несколько раз переопределялась, конкретные переходы должны быть понятны.

Для решения неоднородных уравнений можно использовать \textit{метод вариации произвольной постоянной}. Пусть $C = C(t)$, тогда
\[
    x(t) = C(t) \exp \left(- \int_{t_0}^t a(s) ds \right), t \in I
\]
Подставим в исходное неоднородное уравнение:
\[
    C'(t) \exp \left(- \int_{t_0}^t a(s) ds \right) - C(t) \exp \left(- \int_{t_0}^t a(s) ds \right) a(t) + a(t) C \exp (\dots) = b(t)
\]
\[
    C' = b(t) \exp \left( \int_{t_0}^t a(s) ds \right)
\]
\[
    C(t) = \int_{t_0}^t b(s) \exp \left( \int_{t_0}^s a(\xi) d\xi \right) ds + C
\]
По итогу
\[
    x(t) = \left( \int_{t_0}^t b(s) \exp \left( \int_{t_0}^s a(\xi) d\xi \right) ds + C \right) \cdot \exp \left( - \int_{t_0}^t a(s) ds \right).
\]
При желании эту формулу можно запомнить, но лучше просто знать метод и применять его.

\subsubsection{Уравнение Бернулли}
Пусть задан интервал $I \subset \mathbb R$, константа $\alpha \in (0, 1) \cup (1, +\infty)$ и две непрерывные функции $a, b: I \to \mathbb R$.
Рассмотрим \textit{уравнение Бернулли}: 
\[
    x' = a(t) x + b(t) x^\alpha.
\]
Для решения разделим на $x^\alpha$:
\[
    x^{-\alpha} x' = x^{1 - \alpha} a(t) + b(t)
\]
Замена: $y(t) = x(t)^{1 - \alpha}$, тогда уравнение выше перепишется в виде $y'(t) = (1 - \alpha)x^{-\alpha} x'(t)$ или же
\[
    \frac{y'}{1 - \alpha} = a(t) y + b(t)
\]
Получили линейное уравнение первого порядка.

\subsubsection{Уравнение Риккати}
Пусть $a, b, c: I \to \mathbb R$ --- непрерывные функции, рассмотрим уравнение
\[
    x' = a(t) x^2 + b(t) x + c(t).
\]
Пусть с небес нам дали одно из его решений $y(\cdot)$.
Тогда можно сделать замену $z(t) = x(t) - y(t)$, и уравнение перепишется в виде
\[
    z' + y' = az^2 + 2azy + ay^2 + bz + by + c =
\]
(По условию $ay^2 + by + c = y'$)
\[
    = az^2 + 2azy + bz + y'.
\]
По итогу получается
\[
    z' = az^2 + (2ay + b)z
\]
--- уравнение Бернулли.

\setcounter{equation}{0}
\subsection{Уравнения в дифференциалах}
Пусть $\Omega \subset \mathbb R^2$, $M, N: \Omega \to \mathbb R$. Рассмотрим уравнение 
\[
    M(t, x) dt + N(t, x) dx = 0.
\]
Здесь решением может является и функция $x = x(t)$, и функция $t = t(x)$, удовлетворяющая всем условиям.

\subsubsection{Однородные уравнения}
Пусть найдётся $p \ge 0$, такое что для любого $k > 0$ выполнено $M(kt, kx) \equiv k^p(t, x)$ и $N(kt, kx) = k^p N(t, x)$.
Тогда уравнение будет называться \textit{однородным}, и решается оно заменой $x(t) = t \cdot z(t)$.
При $t > 0$ уравнение перепишется в виде
\[
    t \cdot z' + z = - \frac{M(1, z)}{N(1, z)},
\]
а при $t < 0$ будет
\[
    t \cdot z' + z = - \frac{M(-1, -z)}{N(-1, -z)}.
\]
Получили уравнение с разделяющимися переменными.

\textbf{Замечание.} Да, нужно рассматривать два случая отдельно, просто выносить $k < 0$ не получится.
Например, для $M(t, x) = |x|$ отрицательные $k$ не вынесутся, а с положительными всё в порядке.

\subsubsection{Уравнения в полных дифференциалах}
Пусть $M$ и $N$ непрерывны, $\Omega$ открыто и существует непрерывно дифференцируемая функция $f: \Omega \to \mathbb R$, такая что $\frac{\partial f}{\partial t} (t, x) \equiv M(t, x)$ и $\frac{\partial f}{\partial x} (t, x) \equiv N(t, x)$.
Тогда уравнение (1) будет называться \textit{уравнением в полных дифференциалах}.

\textbf{Теорема.} (Из матанализа, б/д) Пусть множество $\Omega$ выпукло и $\frac{\partial M}{\partial x} (t, x) \equiv \frac{\partial N}{\partial t} (t, x)$. 
Тогда найдётся дважды непрерывно дифференцируемая функция $f: \Omega \to \mathbb R$, такая что $\frac{\partial f}{\partial t} = M$ и $\frac{\partial f}{\partial x} = N$.

\textbf{Утверждение.} Уравнение в полных дифференциалах эквивалентно уравнению $f(t, x) = C$, где $C$ --- константа.

\textbf{Доказательство.} Пусть $x(\cdot)$ --- решение уравнения (1). Запишем цепочку эквивалентных утверждений:
\[
    x'(t) \equiv -\frac{M(t, x(t))}{N(t, x(t))}
\]
\[
    N(t, x(t)) x' + M(t, x(t)) \equiv 0
\]
\[
    \frac{\partial f}{\partial x} (t, x(t)) x'(t) + \frac{\partial f}{\partial t} (t, x(t)) \equiv 0
\]
(Заметим, что это производная композиции)
\[
    \frac{d}{dt}(f(t, x(t))) \equiv 0
\]
\[
    f(t, x(t)) = C.
\]

\QED

\subsubsection{Интегрирующий множитель}
Пусть $M, N: \Omega \to \mathbb R$ --- непрерывно дифференцируемые функции, $\Omega$ --- односвязная область, уравнение то же.
Уравнение в дифференциалах можно свести к уравнению в полных дифференциалах умножением на специальную функцию $\mu(t, x)$, правда, новое уравнение не обязательно будет эквивалентно исходному.

\textbf{Определение.} Функция $\mu: \Omega \to \mathbb R$ называется \textit{интегрирующим множителем}, если $\mu \ne 0$ на $\Omega$ и уравнение $\mu M dt + \mu N dx = 0$ является уравнением в полных дифференциалах.

Общего способа подбора интегрирующего множителя нет (как правило, это не проще, чем решить само уравнение), и здесь будет рассмотрен только самый простой случай.

\textbf{Утверждение.} Если $M(t, x) \ne 0$ при всех $(t, x) \in \Omega$ и $(\frac{\partial M}{\partial x} - \frac{\partial N}{\partial t})\frac{1}{N}$ зависит только от $t$, то существует интегрирующий множитель $\mu$, зависящий только от $t$ и такой, что $\mu(t) \ne 0$ при всех $t$.

\textbf{Доказательство.} Чтобы уравнение было уравнением в полных дифференциалах, дифференциалы должны совпадать (по теореме выше). То есть
\[
    \frac{\partial}{\partial x} \left( \mu(t) M(t, x) \right) \equiv \frac{\partial}{\partial t} \left( \mu(t) N(t, x) \right)
\]
Раскроем производные:
\[
    \mu \frac{\partial M}{\partial x} \equiv \mu'_t N + \mu \frac{\partial N}{\partial t}
\]
\[
    \mu'_t = \frac{\mu}{N} \left( \frac{\partial M}{\partial x} - \frac{\partial N}{\partial t} \right)
\]
По условию дробь зависит только от $t$, откуда получается уравнение с разделяющимися переменными для функции $\mu(t)$, и её можно найти стандартными техниками.

\QED

\section{Методы понижения порядка ОДУ}
(Не знаю, откуда взялся этот параграф в конспекте, но на всякий случай оставлю его)

Пусть у нас есть ОДУ $F(t, x, x', x'', \dots, x^{(n)}) = 0$ (1). Рассмотрим случаи:

\textbf{1)} $F(t, x^{(k)}, \dots, x^{(n)}) = 0$. Сделаем замену $y(t) = x^{(k)}$, порядок понизился.

\textbf{2)} $F(x, x', \dots, x^{(n)}) = 0$. Введём новую функцию $p(x) = x'$, тогда $x'' = p'_x(x) x' = p'_x p$.
Исходное уравнение можно записать в виде $F(x, p, p'_xp, \dots) = 0$. Теперь решаем уравнение относительно $p$ и $x$.

\textbf{3)} $F(t, x, x', \dots, x^{(n)}) = 0$ и при фиксированном $t$ функция $F(t, \cdot)$ \textit{положительно однородная}, то есть
\[
    F(t, \lambda x, \lambda x', \dots, \lambda x^{(n)}) = 0 \iff F(t, x, x', \dots, x^{(n)}) = 0~\forall \lambda > 0, x, x', \dots
\]
Пример положительно однородной функции: $t(x')^2 + (x'')^2 + e^tx^2 = 0$.
Решается заменой: $x' = xz$, где $z$ --- новая функция от $t$. Тогда $x'' = x'z + xz' = x(z^2 + z')$.
Подставляя в исходное уравнение, имеем $F(t, x, xz, x(z^2 + z'), \dots) = 0$.
Вспоминая про положительную однородность, мы можем избавиться от $x$, рассмотрев случаи, когда $x > 0$ и $x < 0$:
\[
    \left[
        \begin{array}{ll}
            F(t, 1, z, z^2 + z', \dots) = 0 \\
            F(t, -1, -z, -z^2 - z', \dots) = 0
        \end{array}
    \right ..
\]
Эта система \textbf{не эквивалентна} исходной, у неё могут быть новые решения.

\section{Задача Коши}
\textbf{Определение.} Пусть у нас есть нормальная система ОДУ, а также $t_0 \in \mathbb R$ и $x_0, x_0^1, \dots, x_0^{k-1} \in \mathbb R^n$. Система уравнений
\[
    \begin{cases}
        F(t, x, x', \dots, x^{(k)}) = 0 \\
        x(t_0) = x_0 \\
        x'(t_0) = x_0^1 \\
        \vdots \\
        x^{(k-1)}(t_0) = x_0^{k-1}
    \end{cases} .
\]
называется \textit{задачей Коши}.
Соотношения про значения в точке $t_0$ принято называть \textit{начальным условием}. 
Важно, что в задаче Коши $k$-ого порядка должно быть ровно $k$ начальных условий.
Её решением называется решение вышеуказанной системы, то есть решение дифференциального уравнения, удовлетворяющее начальному условию.
На данный момент у дифференциального уравнения есть одно решение с точностью до области определения. Чтобы убрать и эту неоднозначность, введём следующее

\textbf{Определение.} Функция $x: I \to \mathbb R^n$ называется \textit{непродолжаемым (глобальным)} решением системы, если для любого решения $\tilde{x}: \tilde{I} \to \mathbb R^n$, такого что $I \subset \tilde{I}$ и $x \equiv \tilde{x}$ на $I$, выполняется $I = \tilde{I}$.

Дальше нас будут в основном интересовать непродолжаемые решения.

\subsection{Теоремы о существовании и о единственности решения}
В этом пункте мы будем рассматривать только нормальные задачи Коши первого порядка. Напоминание:

\textbf{Определение.} Пусть $X, Y$ --- метрические пространства, $\beta \ge 0$, $g: X \to Y$. Отображение $g$ называется \textit{липшицевым} (с константной Липшица $\beta$), если для всех $x, u \in X$ выполняется неравенство
\[
    \rho_Y(g(x), g(u)) \le \beta \rho_X(x, u).
\]

\textbf{Обозначение.} $O(x, R)$ --- открытый шар радиуса $R$ с центром в $x$. $B(x, R)$ --- замкнутый.

\textbf{Теорема.} (О существовании и единственности решения задачи Коши)
Пусть $\Gamma \subset \mathbb R^2$ открыто, $f: \Gamma \to \mathbb R^n$, --- непрерывное отображение, $(t_0, x_0) \in \Gamma$.
Зафиксируем $r > 0$, такое что $B := B((t_0, x_0), r) \subset \Gamma$ (найдётся из открытости).
Положим $m := \sup_{(t, x) \in B} |f(t, x)|$ и $d := \frac{r}{\sqrt {1 + m^2}}$.
Пусть существуют $\frac{\partial f_i}{\partial x_j}(t, x)$ для всех $(t, x) \in \Gamma$ и все $\frac{\partial f_i}{\partial x_j}$ непрерывны на $\Gamma$.
Тогда задача Коши имеет решение $x: (t_0 - d, t_0 + d) \to \mathbb R^n$, такое что для любого другого решения $\widehat{x}: J \to \mathbb R^n$ верно, что $x \equiv \widehat{x}$ на $(t_0 - d, t_0 + d) \cap J$.

Для доказательства нам потребуется несколько вспомогательных фактов.

\textbf{Теорема.} (Принцип сжимающих отображений) Пусть $(X, \rho)$ --- полное метрическое пространство, и существует такое отображение $\Phi: X \to X$, что $f \circ \Phi$ --- липшицево с $\beta \in [0, 1)$.
Тогда у $\Phi$ существует единственная стационарная точка.

\textbf{Доказательство.} Рассмотрим последовательность $\{x_n\}$, такую что $x_{n+1} = \Phi(x_n)$.
Будем доказывать, что последовательность фундаментальна, и поэтому она сходится в силу полноты пространства.
Оценим $\rho(x_n, x_{n+k})$. По неравенству треугольника
\[
    \rho(x_n, x_{n+k}) \le \sum_{i=n}^{n+k-1} \rho(x_i, x_{i+1}) = 
    \sum_{i=n}^{n+k-1} \rho(\Phi^i(x_0), \Phi^{i+1}(x_0)) =
\]
\[
    = \sum_{i=n}^{n+k-1} \rho(\Phi^i(x_0), \Phi^{i}(\Phi(x_0))) \le
\]
Применим определение сжимающего отображения:
\[
    \le \sum_{i=n}^{n+k-1} \beta^i \rho(x_0, \Phi(x_0))
    \le \sum_{i=n}^{\infty} \beta^i \rho(x_0, \Phi(x_0))
    = \frac{\beta^n}{1-\beta} \rho(x_0, \Phi(x_0)).
\]
Следовательно, последовательность $\{x_n\}$ фундаментальна, и в силу полноты пространства $\exists x \in X: x_n \to x$.
Докажем, что эта точка $x$ и есть искомая неподвижная.
Зафиксируем $n \in \mathbb N$, тогда
\[
    \rho(x, \Phi(x)) \le \rho(x, x_n) + \rho(x_n, \Phi(x_n)) + \rho(\Phi(x_n), \Phi(x)) \le
\]
\[
    \le \rho(x, x_n) + \frac{\beta^n}{1 - \beta} \rho(x_0, \Phi(x_0)) + \beta \rho(x_n, x).
\]
Заметим, что левая часть не зависит от $n$, а правая стремится к нулю при росте $n$, поэтому $\rho(x, \Phi(x))$ можно сделать меньше любого $\varepsilon > 0$.

Докажем единственность. Пусть $\xi = \Phi(\xi)$.
Тогда
\[
    \rho(x, \xi) = \rho(\Phi(x), \Phi(\xi)) \le \beta \rho(x, \xi).
\]
Так как $\beta < 1$, это возможно только при $x = \xi$.

\QED

\textbf{Пример.} Как найти решение уравнения $x = \cos(x)$, используя только калькулятор с тригонометрическими функциями?
Можно просто применять косинус к нулю, пока не сойдётся, так как $X = [0, \frac{\pi}{2} - \varepsilon]$ является подходящим под условие теоремы полным метрическим пространством.

\textbf{Следствие.} Пусть $(X, \rho)$ --- полное метрические пространство, $\Phi: X \to X$ --- произвольное отображение, но для какого-то $N$ отображение $\Phi^N$ сжимающее.
Тогда у $\Phi$ существует единственная неподвижная точка.

\textbf{Доказательство.} Существование. Возьмём единственный $x$ из принципа сжимающих отображений, такой что $x = \Phi^N(x)$.
Тогда $\Phi(x) = \Phi^N(\Phi(x))$.
Следовательно, $\Phi(x)$ --- неподвижная точка $\Phi^N$, то есть $x = \Phi(x)$ в силу единственности.

Единственность. Пусть $\xi$ --- неподвижная точка $\Phi$, то есть $\xi = \Phi(x)$.
Применим $N - 1$ раз отображение $\Phi$: $\xi = \Phi^N(\xi)$, то есть $\xi = x$, опять же в силу единственности стационарной точки у отображения $\Phi^N$.

\QED

\textbf{Утверждение.} (б/д) Пусть $T \subset \mathbb R$, $K \subset \mathbb R^n$ --- компакт, $K \ne \varnothing$. 
$C(T, K)$ --- пространство непрерывных функций, действующих из $T$ в $K$ с метрикой $\rho(x_1, x_2) = \sup_{t \in T} |x_1(t) - x_2(t)|$, и оно является полным.

\textbf{Утверждение.} (б/д) Для всех $A \in \mathbb R^{n \times n}$ и $x \in \mathbb R^n$ выполнено $|Ax| \le \| A \| \cdot |x|$.

\textbf{Утверждение.} Пусть $\Gamma$ открыто, $f$ --- непрерывное отображение, частные производные $\frac{\partial f_i}{x_j}$ существуют и непрерывны.
Тогда найдётся $l \ge 0$, такой что для всех $(t, x_1), (t, x_2) \in B$ (где $B$ определяли в теореме о существовании и единственности) выполнено $|f(t, x_1) - f(t, x_2)| \le l|x_1 - x_2|$ (отображение липшицево по $x$ при каждом $t$).

\textbf{Доказательство.} Зафиксируем $t, x_1, x_2$. Положим $a(s) := f(t, x_1 + s(x_2 - x_1))$ для $s \in [0, 1]$.
Тогда, беря интеграл вектор-функции, как покоординатный интеграл,
\[
    |f(t, x_2) - f(t, x_1)| = |a(1) - a(0)| = \left| \int_{0}^{1} a'(s) ds \right| =
\]
\[
    = \left| \int_{0}^{1} \frac{\partial f}{\partial x} (t, x_1 + s(x_2 - x_1)) (x_2 - x_1) ds \right| \le
\]
(по утверждению про норму матрицы)
\[
    \le \left| \int_{0}^{1} \left\| \frac{\partial f}{\partial x} (t, x_1 + s(x_2 - x_1)) \right\| \cdot |x_2 - x_1| ds \right| =
\]
\[
    = \left| \int_{0}^{1} \left\| \frac{\partial f}{\partial x} (t, x_1 + s(x_2 - x_1)) \right\| ds \right| \cdot |x_2 - x_1| \le
    l |x_2 - x_1|,
\]
где $l = \max_{(\tau, x) \in B} \| \frac{\partial f}{\partial x} (\tau, x) \|$, он достигается в силу замкнутости шара $B$.

\QED

\textbf{Доказательство теоремы.} Возьмём произвольный интервал $T$, такой что $t_0 \in T \subset (t_0 - d, t_0 + d)$.
Положим $R := \sqrt {r^2 - d^2}$ (можно проверить, что выражение под корнем положительное), $X := C(R, B(x_0, R))$ --- полное метрическое пространство.

Замечание: для всех $x \in X$ и $t \in T$ выполняется $(t, x(t)) \in B$, так как
\[
    |t - t_0|^2 + |x(t) - x_0|^2 \le d^2 + R^2 = r^2.
\]
Зададим отображение $\Phi: X \to X$ так, что $\Phi(x)(t) = x_0 + \int_{t_0}^{t} f(s, x(s))ds$.
Необходимо проверить корректность определения, ибо $\Phi(\cdot)$ не обязательно лежит в $X$, то есть для всех $x, t$ значение $\Phi(x)(t)$ должно лежать в $B(x_0, R)$:
\[
    |\Phi(x)(t) - x_0| = \left| \int_{t_0}^{t} f(s, x(s)) ds \right| \le \left| \int_{t_0}^{t} |f(s, x(s))|ds \right| \le
\]
(два модуля нужны, так как $t_0$ может превосходить $t$)
\[
    \le \left| \int_{t_0}^{t} m \cdot ds \right| = m|t - t_0| \le md = \frac{rm}{\sqrt {1 + m^2}} = R.
\]
Последнее равенство получено подстановкой $d$ в определение $R$.
Докажем, что 
\[
    |\Phi^N(x_1)(t) - \Phi^N(x_2)(t)| \le \frac{(l |t - t_0|)^N}{N!} \rho(x_1, x_2)
\]
для всех $x_1, x_2 \in X, N \in \mathbb N, t \in T$. Здесь $l$ берётся из утверждения про липшицевость.
Имеем 
\[
    |\Phi^N(x_1)(t) - \Phi^N(x_2)(t)| = \left| \int_{t_0}^{t} \left( f(x, \Phi^{N-1}(x_1)(s)) - f(s, \Phi^{N-1}(x_2)(s) \right) ds \right| \le
\]
\[
    \le l \left| \int_{t_0}^{t} |\Phi^{N-1}(x_1)(s) - \Phi^{N-1}(x_2)(s)ds| \right|
\]
Посмотрим на первые несколько $N$. При $N = 1$:
\[
    \left| \Phi^1(x_1)(t) - \Phi^1(x_2)(t) \right| \le l \left| \int_{t_0}^{t} |x_1(s) - x_2(s)|ds \right| \le
\]
(по определению расстояния между функциями)
\[
    \le l \left| \int_{t_0}^{t} \rho(x_1, x_2) ds \right| = l \rho(x_1, x_2) |t - t_0|.
\]
При $N = 2$:
\[
    \left| \Phi^2(x_1)(t) - \Phi^2(x_2)(t) \right| \le l \left| \int_{t_0}^{t} |\Phi^1(x_1)(s) - \Phi^1(x_2(s)| ds \right| \le
\]
(применим доказанное в предыдущем пункте)
\[
    \le l^2 \rho(x_1, x_2) \left| \int_{t_0t}^{t} |s-t_0|ds \right| = \frac{l^2}{2!} |t - t_0|^2 \rho(x_1, x_2).
\]
При $N = 3$:
\[
    \left| \Phi^3(x_1)(t) - \Phi^3(x_2)(t) \right| \le l \left| \int_{t_0}^{t} |\Phi^2(x_1)(s) - \Phi^2(x_2(s)| ds \right| \le
\]
(применим доказанное)
\[
    \le \frac{l^3}{2!} \rho(x_1, x_2) \left| \int_{t_0}^{t} |s-t_0|^2 ds \right| = \frac{l^3}{3!} |t - t_0|^3 \rho(x_1, x_2).
\]
Аналогично можно доказать и для всех $N$, формально нужно применить математическую индукцию.

Из доказанной формулы получаем, что для всех $x_1, x_2 \in X$
\[
    \rho(\Phi^N(x_1), \Phi^N(x_2)) = \sup_{t \in T} \left| \Phi^N(x_1)(t) - \Phi^N(x_2)(t) \right| \le
\]
\[
    \le \frac{l^N}{N!} \sup_{t \in T} |t - t_0|^N \rho(x_1, x_2) \le \frac{l^Nd^N}{N!} \rho(x_1, x_2).
\]
Последний переход сделан из соображения, что $T \subset B(t_0, d)$. Так как последнее выражение стремится к нулю при увеличении $N$, найдётся $N$, такое что $\frac{l^Nd^N}{N!} < 1$, и, значит, $\Phi^N$ --- сжимающее отображение.
По следствию из принципа сжимающих отображений существует единственная неподвижная точка $x$ у $\Phi(\cdot)$.
То есть для всех $t \in T$
\[
    x = \Phi(x) \iff x(t) \equiv \Phi(x)(t) \iff x(t) \equiv x_0 + \int_{t_0}^{t} f(s, x(s))ds \iff
\]
\[
    \iff 
    \begin{cases}
        x'(t) \equiv f(t, x(t)), \\
        x(t_0) = x_0.
    \end{cases}
\]
Последний переход получен дифференцированием, теоремы про дифференцирование интегралов будут позже в матанализе.
Следовательно, $x$ --- решение задачи Коши, и оно единственно на любом интервале $T \subset (t_0 - d, t_0 + d)$.

Докажем вторую часть теоремы. Возьмём произвольное решение $\widehat{x}: J \to \mathbb R^n$ и положим $T := (t_0 - d, t_0 + d) \cap J$.
Из доказанного следует, что $x(t) \equiv \widehat{x}(t)$ на $T$, что и требовалось доказать.

\QED

\section{Теоремы о продолжении решений}
Пусть дано открытое множество $\Gamma \subset \mathbb R \times \mathbb R^n$ и отображение $f: \cl(\Gamma) \to \mathbb R^n$ (замыкание $\Gamma$), $(t_0, x_0) \in \Gamma$ --- начальное условие задачи Коши.

\textbf{Теорема.} (О продолжении решения до границы компакта) 
Пусть $\cl(\Gamma)$ --- компакт, $f$ непрерывно, существуют непрерывные $\frac{\partial f_i}{\partial x_j}$ на $\Gamma$.
Тогда существует решение задачи Коши $x: (a, b) \to \mathbb R^n$, такое что $(a, x(a + 0)), (b, x(b - 0)) \in \partial \Gamma$ (нули --- это пределы справа/слева, $\partial \Gamma$ --- граница $\Gamma$).

Иными словами, решение можно продолжить до границ области определения функции.

\textbf{Доказательство.} Поскольку $\cl(\Gamma)$ --- компакт и $f$ непрерывно, функция $f$ ограничена, то есть $\exists M \ge 0: |f(t, x)| \le M$.
Положим $r_0 := \rho((t_0, x_0), \partial \Gamma)$ --- инфимум расстояния от точки до точки во множестве, $d_0 := \frac{r_0}{\sqrt {1 + M^2}}$.
Тогда по теореме о существовании и единственности найдётся решение задачи Коши $\phi_1: (t_0 - d_0, t_0 + d_0) \to \mathbb R^n$.
Построим решение $x(t)$. Определим $x(t) := \phi_1(t)$ при $t \in [t_0, t_0 + \frac{d_0}{2})$

Теперь положим $t_1 := t_0 + \frac{d_0}{2}$, $x_1 := \phi_1(t_1)$, $r_1 := \rho((t_1, x_1), \partial \Gamma)$, $d_1 := \frac{r_1}{\sqrt {1 + M^2}}$.
Вновь по теореме о существовании и единственности найдётся единственное решение задачи Коши $\phi_2: (t_1 - d_1, t_1 + d_1) \to \mathbb R^n$.
Определим $x(t) = \phi_2(t)$ при $t \in [t_1, t_1 + \frac{d_1}{2})$.
% Тогда имеем, что $x$ --- единственное решение задачи Коши на $(t_0 - d_0, t_0 + d_0) \cap (t_1 - d_1, t_1 + d_1)$, так как на этом интервале $\phi_1 \equiv \phi_2$.

Продолжаем так строить и доопределять $x$ на полуинтервалах вида $[t_k, t_k + \frac{d_k}{2})$.
Теперь у нас есть два случая. Первый --- процесс завершился, то есть $\exists k: (t_k, x(t_k - 0)) \in \partial \Gamma$. Тогда $b := t_k$.
Второй --- такого $k$ нет.
Так как $\cl(\Gamma)$ --- компакт, последовательность $\{t_k\}$ ограничена, а по построению она возрастает.
Значит, существует предел $b := \lim_{k \to \infty}(t_k)$.

Докажем, что существует предел $x(b - 0)$.
Функция $x$ определена на $\bigcup_{k=0}^{\infty} [t_k, t_{k+1})$, поэтому $x( \cdot )$ определена на $[t_0, b)$.
Тогда имеем
\[
    |x'(t)| = |f(t, x(t))| \le M \Rightarrow |x(t) - x(s)| \le M |t - s|.
\]
Иными словами, функция $x$ липшицева, и по критерию Коши существует предел $x(b - 0)$.

Докажем, что $(b, x(b - 0)) \in \partial \Gamma$.
По построению имеем $t_{k+1} = t_0 + d_0 + \frac{d_1}{2} + \dots + \frac{d_k}{2} \to_{k \to \infty} b$, то есть $d_k \to 0$.
По определению $d_k$ --- это расстояние до границы, умноженное на константу, поэтому $\rho((t_k, x_k), \partial \Gamma) \to 0$.
Так как $\cl(\Gamma)$ --- компакт, $\partial \Gamma$ --- это тоже компакт.
По определению расстояния до множества найдётся точка $(t_k', x_k') \in \partial \Gamma$, такая что $\rho((t_k, x_k), \partial \Gamma) = |(t_k, x_k) - (t_k', x_k')|$.
Отсюда получаем при $k \to \infty$
\[
    |(b, x(b - 0)) - (t_k', x_k')| \le |(b, x(b - 0)) - (t_k, x_k)| + \rho((t_k, x_k), \partial \Gamma) \to 0.
\]
(Это просто неравенство треугольника)

Первое слагаемое стремится к нулю, так как это предел, второе --- так как точки стремятся к границе.
Следовательно, $(t_k', x_k') \to (b, x(b - 0))$, и, так как $\partial \Gamma$ --- компакт, предел этой последовательности тоже лежит в $\partial \Gamma$, то есть $(b, x(b - 0)) \in \partial \Gamma$.

Аналогично можно продолжить $x$ и до левой границы.

\QED

\textbf{Замечание.} Данная теорема не гарантирует существование решения на всей проекции множества по одной из координат, ибо $x(\cdot)$ может продолжиться до границ по разным координатам.
Пример:

\begin{figure}[ht]
    \centering
    \incfig{continuation-theorem}{0.5\linewidth}
    \caption{Продолжение решения не до всей проекции}
\end{figure}

\textbf{Лемма.} (О дифференциальном неравенстве) Пусть $I \subset \mathbb R$ --- интервал, $t_0 \in I$, $z \in C^1(I, \mathbb R^n)$, $A > 0$, $B \ge 0$, $|z'(t)| \le |A(z(t))| + B$ для всех $t \in I$.
Тогда
\[
    |z(t)| \le |z(t_0)|e^{A(t - t_0)} + \frac{B}{A} \left( e^{A|t - t_0|} - 1 \right).
\]

\textbf{Замечание.} При $A = 0$ имеем $|z'(t)| \le B$. Тогда по теореме Лагранжа $|z(t)| \le |z(t_0)| + B|t - t_0|$ для всех $t \in I$.

\textbf{Доказательство.} Будем доказывать при $t > t_0$ (при $t = t_0$ очевидно, при $t < t_0$ аналогично).
Пусть $z(t) \ne 0$, при равенстве нулю очевидно.
Если $z(\tau) \ne 0$ при $\tau \in (t_0, t)$, то положим $t^* := t_0$.
Иначе положим $t^* = \sup \{\tau \in (t_0, t): z(\tau) = 0\}$.
В любом случае получаем, что $z(t) \ne 0$ на интервале $(t^*, t)$.
Докажем, что $\frac{d}{d t} |z(\tau)| \le |z'(\tau)|$ при $\tau \in (t^*, t)$.
Замечание: продифференцировать модуль можно, так как он не обращается в ноль.

Как известно, $|z(\tau)|^2 \equiv \left< z(\tau), z(\tau) \right>$ (скалярное произведение).
Продифференцируем:
\[
    2|z(\tau)| \frac{d}{dt} |z(\tau)| \equiv 2 \left< z(\tau), z'(\tau) \right> \le 2|z(\tau)| \cdot |z'(\tau)|.
\]
Последнее неравенство --- по неравенству Коши-Буняковского.
В процессе мы продифференцировали скалярное произведение. Выведем его: пусть $\gamma(\tau) := \left< z(\tau), z(\tau) \right>$.
Распишем, как сумму координат и заметим, что получится ровно $\gamma' = \sum_{i=1}^{n} 2z_iz_i'$.

Обратно к исходной задаче. Мы получили, что $\frac{d}{dt} |z(\tau)| \le |z'(\tau)| \le A|z(\tau)| + B$.
Перенесём $A|z(\tau)|$ в левую часть неравенство и домножим:
\[
    \left(\frac{d}{dt} |z(\tau)| - Az(\tau) \right) e^{-A(\tau - t^*)} \le Be^{-A(\tau - t^*)}.
\]
Дальше можно вынести производную и получить
\[
    \frac{d}{dt} \left( |z(\tau)| e^{-A(\tau - t^*)} \right) \le Be^{-A(\tau - t^*)}.
\]
Возьмём определённый интеграл от обеих частей:
\[
    |z(t)| e^{-A(t - t^*)} - |z(t^*)| \le -\frac{B}{A} e^{-A(t - t^*)} + \frac{B}{A}.
\]
Тогда
\[
    |z(t)| \le |z(t^*)| e^{A(t - t^*)} + \frac{B}{A} \left( e^{A(t - t^*)} - 1 \right).
\]
Получили почти то, что нужно, но тут вместо $t_0$ стоит $t^*$.
Но по построению имеем, что $t - t^* \le t - t_0$ и $|z(t^*)| \le |z(t_0)|$, ибо мы выбирали $t^*$ так, что либо $z(t^*) = 0$, либо $t^* = t_0$.

\QED

\textbf{Теорема.} (Вторая о продолжении решения) Пусть $I \subset \mathbb R$ --- интервал, $f: I \times \mathbb R^n \to \mathbb R^n$, $t_0 \in I$, $x_0 \in \mathbb R^n$.
Также пусть:
\begin{itemize}
    \item Для $f$ выполняются предположения теоремы о существовании и единственности решения.
    \item Найдутся $a, b \in C(I, \mathbb R_+)$, такие что для всех $t \in I, x \in \mathbb R^n$ выполняется $|f(t, x)| \le a(t)|x| + b(t)$ --- условие ограниченного роста.
\end{itemize}
Тогда существует решение $x: I \to \mathbb R^n$ задачи Коши.
От предыдущих теорем эта отличается тем, что решение определено на всём данном интервале.

\textbf{Доказательство.} I. Возьмём числа $\alpha, \beta \in I$, такие что $\alpha < t_0 < \beta$.
Тогда найдутся $A, B > 0$, такие что $|a(t)| \le A$ и $|b(t)| \le B$ для всех $t \in [\alpha, \beta]$.
Положим $\Gamma := (\alpha, \beta) \times O(0, R + 1) \subset I \times \mathbb R^n$.
Здесь
\[
    R := \max_{\alpha \le t \le \beta} \left( |x_0|e^{A|t - t_0|} + \frac{B}{A} \left( e^{A|t - t_0|} - 1 \right) \right)
\]
Из теоремы о продолжении решения до границы следует, что существует решение $x: (\overline{\alpha}, \overline{\beta}) \to \mathbb R^n$, такое что $\forall t~x(t) \in \Gamma$ и $(\alpha, x(\overline{\alpha} + 0)), (\beta, x(\overline{\beta} - 0)) \in \partial \Gamma$.
Из леммы имеем, что $|x(t)| \le R$.
Поэтому $\overline{\alpha} = \alpha$ и $\overline{\beta} = \beta$, так как их проекция на $\mathbb R^n$ не может лежать на границе $O(0, R + 1)$, то есть проекция на $I$ лежит на границе.
Таким образом, решение $x(\cdot)$ определено на $(\alpha, \beta)$.

II. Возьмём числа $\alpha_i$, $\beta_i$, такие что $\alpha_i \to \inf(I)$, $\beta_i \to \sup(I)$, причём обе монотонны.
Тогда для всех $i$ выполнено $\alpha_i < t_0 < \beta_i$, интервалы $(\alpha_i, \beta_i)$ образуют цепочку вложений, и $I = \bigcup_{i=1}^\infty (\alpha_i, \beta_i)$.
Для всех $i$ существует решение $x^i: (\alpha_i, \beta_i) \to \mathbb R^n$.
Рассмотрим произвольный $t \in I$. Для всех пар индексов $i, j$, таких что $t \in (\alpha_i, \beta_i) \cap (\alpha_j, \beta_j)$, верно по теореме о существовании и единственности решения, что $x^i(t) = x^j(t)$.
Поэтому можно положить $x(t) := x^i(t)$, и оно будет определено однозначно вне зависимости от $i$.
Следовательно, мы определили $x(\cdot)$ во всех точках, и он является искомым решением задачи Коши.

\QED

\section{Уравнения, не разрешённые относительно производной}
Рассмотрим уравнение $f(t, x, x') = 0$.
Здесь $f: \Omega \to \mathbb R$, где $\Omega \subset \mathbb R^3$.

\textbf{Пример.} (Задача Коши, у которой много решений)
\[
    \begin{cases}
        (x')^2 - x^2 = 0 \\
        x(0) = x_0
    \end{cases}.
\]
Оно эквивалентно одному из двух уравнений $x' = x$ и $x' = -x$.
Их решениями являются $x = x_0 e^{-t}$ и $x = x_0 e^t$ --- произошла неоднозначность.

\subsection{Теорема о существовании и единственности}
\textbf{Теорема.} (О существовании и единственности решения задачи Коши для неявного уравнения) Пусть $(t_0, x_0, v_0) \in \Omega$, $f \in C^1(\Omega, \mathbb R)$.
Если $f(t_0, x_0, v_0) = 0$ и $\frac{\partial f}{\partial x'} (t_0, x_0, v_0) \ne 0$, то существует $d > 0$ и решение $x: (t_0 - d, t_0 + d) \to \mathbb R$ задачи Коши, являющееся единственным, удовлетворяющим равенству $x'(t_0) = v_0$.

\textbf{Доказательство.} Посмотрим на $f(t, x, x') = 0$, как на уравнение относительно трёх переменных и разрешим его относительно $x'$.
По теореме о неявной функции найдутся числа $r, \varepsilon > 0$ и отображение $g: O((t_0, x_0), r) \to \mathbb R$, такие что $g \in C^1$, $f(t, x, g(t, x)) \equiv 0$, $g(t_0, x_0) = v_0$ (следует из того, что $f(t_0, x_0, v_0) = 0$ и производная не равна нулю), и данное решение единственно в $\varepsilon$-окрестности точки $v_0$.
Рассмотрим новую задачу Коши:
\[
    \begin{cases}
        x' = g(t, x) \\
        x(t_0) = x_0
    \end{cases}.
\]
Теперь у нас явно выражен $x'$, поэтому можно применить теорему о существовании и единственности решения, то есть найдётся $d > 0$ и $x: (t_0 - d, t_0 + d) \to \mathbb R$, являющийся решением.
Проверим, что это решение подходит в исходную систему:
\[
    f(t, x(t), x'(t)) \equiv f(t, x(t), g(t, x(t))) \equiv 0, t \in (t_0 - d, t_0 + d).
\]
Выполнение начального условия следует из того, что $x$ удовлетворяет начальному условию в новой задаче Коши.
Проверим для $x'(t_0)$:
\[
    x'(t_0) = g(t_0, x(t_0)) = g(t_0, x_0) = v_0.
\]
Следовательно, мы нашли решение исходной задачи Коши. 

Докажем, что оно единственно.
Уменьшим $d$ так, чтобы для всех $t \in (t_0 - d, t_0 + d)$ выполнялось $(t, x(t)) \in O((t_0, x_0), \frac{r}{2})$.
Предположим противное: существует ещё одно решение $\phi: I \to \mathbb R$, такое что $\phi(t_0) = x_0$, $\phi'(t_0) = v_0$ и существует $\widehat t \in J := (t_0 - d, t_0 + d) \cap I$, такое что $\phi(\widehat t) \ne x(\widehat t)$.
Не умаляя общности, $\widehat t > t_0$.
Положим $t_1 = \inf \{t \in J: t > t_0, x(t) \ne \phi(t)\}$.
По допущению множество непусто, поэтому инфимум брать можно, теперь мы знаем, что $x(t) \equiv \phi(t)$ на отрезке $[t_0, t_1]$.
В самом начале доказательства единственности мы уменьшили $d$: засчёт этого $(t_1, \phi(t_1)) = (t_1, x(t_1)) \in O((t_0, x_0), \frac{r}{2})$, что, в силу непрерывности $\phi$, позволяет выбрать $t_3 > t_1$ так, что $(t, \phi(t)) \in O((t_0, x_0), r)$ на $[t_0, t_3]$.
Дополнительно уменьшим $t_3$ так, чтобы $\phi'(t)$ лежало в $O(v_0, \varepsilon)$ при $t \in [t_0, t_3]$.
Наконец, возьмём $t_2$ из полуинтервала $(t_1, t_3)$, по построению $\phi(t_2) \ne x(t_2)$.

\begin{figure}[ht]
    \centering
    \incfig{implicit-equation-t}{0.75\linewidth}
    \caption{Иллюстрация конструкции}
\end{figure}

Поскольку $f(t, \phi(t), \phi'(t)) \equiv 0$ на $[t_0, t_3]$, $(t, \phi(t)) \in O((t_0, x_0), r)$ и $\phi'(t) \in O(v_0, \varepsilon)$, мы можем применить единственность из теоремы о неявной функции и получить $\phi'(t) \equiv g(t, \phi(t))$ на отрезке $[t_0, t_3]$.
Кроме того, $\phi(t_0) = x_0$, поэтому $\phi$ --- это решение новой задачи Коши на $[t_0, t_3]$.
Следовательно, $\phi(t) \equiv x(t)$ на $[t_0, t_3]$, и, в частности, $\phi(t_2) = x(t_2)$ --- противоречие.

\QED

\textbf{Замечание.} Может захотеться доказать единственность проще: решение новой задачи Коши единственно, функция в теореме о неявной функции --- тоже.
Однако мы не можем просто так подставить $f(t, \phi(t), \phi'(t))$ и после этого сказать, что $\phi'(t) \equiv g(t, \phi(t))$, ибо не гарантируется, что $\phi'(t)$ попадает в $\varepsilon$-окрестность точки $v_0$.
В этом же доказательстве, мы приближаем $t_3$ к $t_1$, чтобы всё попадало, ибо производные $x(\cdot)$ и $\phi(\cdot)$ на $[t_0, t_1]$ совпадают.

\subsection{Особые решения}
\textbf{Определение.} Множество
\[
    \left\{ (t, x)~|~\exists v: (t, x, v) \in \Omega, f(t, x, v) = 0, \frac{\partial f}{\partial x'} (t, x, v) = 0 \right\}
\]
называется \textit{дискриминантной кривой}. На нём решение может не быть единственным.

\textbf{Определение.} $\widehat{x}(\cdot)$ называется \textit{особым решением} ОДУ $f(t, x, x') = 0$, если для всех $t_0$ существует решение $x(\cdot)$, такое что $\widehat{x}(t_0) = x(t_0)$, $\widehat{x}'(t_0) = x'(t_0)$ и $\widehat{x}(t) \not\equiv x(t)$ на интервале $(t_0 - d, t_0 + d)$ для всех $d$.
Они нас интересуют из-за того, что в них можно ``склеивать`` два других решения.

\textbf{Пример.} $x' = \sqrt[3]{x}$. $x(t) \equiv 0$ --- особое решение.
Пример склеивания решений был в параграфе про уравнения с разделяющимися переменными.

\textbf{Утверждение.} Если $\widehat{x}: I \to \mathbb R$ --- особое решение, то для всех $t \in I$ точка $(t, \widehat{x}(t))$ лежит на дискриминантной кривой.

\textbf{Доказательство.} Зафиксируем $\tau \in I$. Так как $\widehat x$ --- решение, $f(t, \widehat x(t), \widehat x'(t)) \equiv 0$ на $I$.
Если $\frac{\partial f}{\partial x'} (\tau, \widehat x(\tau), \widehat x'(\tau)) \ne 0$, то $\widehat x(\cdot)$ --- единственное решение задачи Коши
\[
    \begin{cases}
        f(t, x, x') = 0 \\
        x(\tau) = \widehat x(\tau)
    \end{cases},
\]
удовлетворяющее условию $x'(\tau) = \widehat x'(\tau)$ по теореме о существовании и единственности для неявного уравнения.
Но для того, чтобы оно было особым, нам нужно ещё какое-то решение, которое будет его касаться, так что случай не подходит.
Таким образом, $\frac{\partial f}{\partial x'} (\tau, \widehat x(\tau), \widehat x'(\tau)) = 0$, то есть $(\tau, \widehat x(\tau))$ лежит в дискриминантной кривой.

\QED

\textbf{Пример.} $(x')^2 - 4x^3(1-x) = 0$.
Найдём дискриминатную кривую:
\[
    \begin{cases}
        v^2 - 4x^3(1-x) = 0 \\
        2v = 0.
    \end{cases}
\]
Нетрудно заметить, что дискриминатная кривая --- это две прямые $(t, 0)$ и $(t, 1)$.
Чтобы найти особые решения, нужно решить исходное уравнение: $x(t) = \frac{1}{(t + c)^2 + 1}$, $x(t) = 0$ и $x(t) = 1$.
Особым решением является $x \equiv 1$, так как его касаются все решения $\frac{1}{(t + c)^2 + 1}$, и по этой прямой можно склеивать решения для разных $c$.

\section{Некоторые следствия теорем о существовании решений}
Мы будем рассматривать два типа уравнений:

\subsection{Уравнения старших порядков}
Дано открытое множество $\Gamma \subset \mathbb R^{k+1}$, отображение $f: \Gamma \to \mathbb R$, вектор $(t_0, x_0^0, x_0^1, \dots, x_0^{k-1}) \in \Gamma$.
Задача Коши вида
\[
    \begin{cases}
        x^{(k)} = f(t, x, x', \dots, x^{(k-1)}) \\
        x(t_0) = x_0^0 \\
        \vdots \\
        x^{(k-1)}(t_0) = x_0^{k-1}
    \end{cases} .
\]
Хотелось бы свести эту задачу к задаче с меньшим порядком производной, чтобы применить теорему о существовании и единственности.
Положим $y_i := x^{i-1}$ для $i \in [1, k]$.
Тогда система сводится к виду
\[
    \begin{cases}
        y_1' = y_2 \\
        y_2' = y_3 \\
        \vdots \\
        y_{k-1}' = y_k \\
        y_k' = f(t, y_1, y_2, \dots, y_k) \\
        y_1(t_0) = x_0^0 \\
        y_2(t_0) = x_0^1 \\
        \vdots \\
        y_k(t_0) = x_0^{k-1}
    \end{cases} .
\]
Введём отображение $F: \Gamma \to \mathbb R^k$, такое что $F(t, y) = (y_2, y_3, \dots, f(t, y))^T$.
Положим $y_0 = (x_0^0, x_0^1, \dots, x_0^{k-1})^T$, тогда эту задачу можно записать в виде
\[
    \begin{cases}
        y' = F(t, y) \\
        y(t_0) = y_0
    \end{cases}.
\]
В частности, если $x$ --- решение исходной задачи, то $y = (x, x', \dots)^T$ --- это решение новой, и, наоборот, если $y$ --- решение новой задачи, то $x = y_1$ --- решение исходной.

\textbf{Теорема.} Пусть $f$ непрерывна, для всех $(t, x_0, x_1, \dots, x_{k-1})$ существуют и являются непрерывными частные производные $\frac{\partial f_i}{\partial x^{(j)}}$.
Тогда существует $d > 0$ и решение $x: (t_0 - d, t_0 + d) \to \mathbb R$, являющееся единственным решением задачи Коши.
Следует напрямую из теоремы о существовании и единственности и замены, описанной выше.

\subsection{Линейные уравнения}
Пусть нам дана непрерывная функция $A: I \to \mathbb R^{n \times n}$, функция $b: I \to \mathbb R^n$, число $t_0 \in I$ и $x_0 \in \mathbb R^n$. 
Рассмотрим систему линейных уравнений
\[
    \begin{cases}
        x' = A(t) \cdot x + b(t) \\
        x(t_0) = x_0
    \end{cases} .
\]

\textbf{Теорема.} Если $A$ и $b$ непрерывны, то существует единственное решение $x: I \to \mathbb R^n$ данной задачи Коши.
Это не просто теорема о существовании и единственности решения задачи Коши, так как здесь решение определено на всём интервале $I$.

\textbf{Доказательство.} Положим $\widehat a(t) := \| A(t) \|$, $t \in I$ и $\widehat b(t) = |b(t)|$, $t \in I$.
Тогда
\[
    |A(t)x + b(t)| \le |A(t)x| + |b(t)| \le \|A(t) \| \cdot |x| + |b(t)| \le \widehat a(t)|x| + \widehat b(t).
\]
Применяя вторую теорему о продолжении, получаем, что решение существует на всём интервале $I$.
Докажем единственность. Пусть $\widehat x, \phi: I \to \mathbb R^n$ --- решения, покажем, что они совпадают.
Не умаляя общности, существует $t > t_0$, такое что $\widehat x(t) \ne \phi(t)$.
Положим $\tau = \inf \{t > t_0: \widehat x(t) \ne \phi(t)\}$, тогда $\widehat x(\tau) = \phi(\tau)$ (доказывается от противного, получится, что $\tau$ --- не инфимум).
Следовательно, в любой окрестности точки $\tau$ задача Коши
\[
    \begin{cases}
        x' = A(t)x + b(t) \\
        x(\tau) = \widehat x(\tau)
    \end{cases}
\]
имеет хотя бы два решения --- противоречие.

\QED

\section{Линейные однородные системы ОДУ}
Пусть нам дан интервал $I \subset \mathbb R$ и линейные пространства $C(I, \mathbb R^n)$, $C^1(I, \mathbb R^n)$ (непрерывные и непрерывно дифференцируемые функции).

\textbf{Напоминание.} Система функций $x^1, \dots, x^k \in C(I, \mathbb R^n)$ называется \textit{линейно зависимой}, если существует линейная комбинация с не всеми нулевыми коэффициентами, тождественно равная нулю.

\textbf{Упражнение.} Если $x^1, \dots, x^k$ линейно зависимы, то для всех $t \in I$ векторы $x^1(t), \dots, x^k(t)$ линейно зависимы.

\textbf{Замечание.} В обратную сторону неверно, например, $x^1(t) = 1$ и $x^2(t) = t$.

\textbf{Определение.} Пусть даны функции $x^1, \dots, x^n \in C(I, \mathbb R^n)$.
\textit{Определителем Вронского} этой системы $\omega(t) = \det(x^1(t), \dots, x^n(t))$.
Нетрудно заметить, что если векторы линейно зависимы, то $\omega(t) \equiv 0$, и обратно неверно: $x^1(t) = (1~1)^T$, $x^2(t) = (t~t)^T$.

Пусть нам даны функции $a_{j,k} \in C(I, \mathbb R)$, где $j, k \in [1, n]$.
Положим матрицу $A(t) = (a_{j,k}(t))_{j,k \in [1, n]}$.
Рассмотрим систему
\begin{equation}
    \begin{cases}
        x_1' = a_{11}(t)x_1 + \dots + a_{1n}(t)x_n \\
        \vdots \\
        x_n' = a_{n1}(t)x_1 + \dots + a_{nn}(t)x_n
    \end{cases}
\end{equation}
Или, что эквивалентно, $x' = A(t)x$.

\textbf{Определение.} Эта система называется \textit{линейной однородной системой ОДУ}.

\textbf{Замечание.} Почему эта система называется линейной? Введём линейный оператор $L: C^1(I, \mathbb R^n) \to C(I, \mathbb R^n)$, такой что $(Lx)(t) = x'(t) - A(t)x(t)$.
Заметим, что множество решений уравнений совпадает с ядром данного линейного оператора.

\textbf{Утверждение.} Пусть $x^1, \dots, x^n$ --- решения системы (1). Если существует $\tau \in I$, такое что $\omega(\tau) = 0$, то все решения $x^1, \dots, x^n$ линейно зависимы.

\textbf{Доказательство.} Если $\omega(\tau) = 0$, то $x^1(\tau), \dots, x^n(\tau)$ линейно зависимы (именно значения в этой точке), то есть существуют $\lambda_1, \dots, \lambda_n$, такие что $\sum_{j=1}^{n} \lambda_j x^j(\tau) = 0$.
Положим $x := \sum_{j=1}^{n} \lambda_j x^j$.
По построению $x(\tau) = 0$, то есть $x$ является решением задачи Коши
\[
    \begin{cases}
        y' = A(t)y \\
        y(\tau) = 0.
    \end{cases}
\]
По теореме о существовании и единственности решения для линейных уравнений у этой задачи есть очевидное и уникальное решение --- $y = 0$ на всём интервале $I$.
Следовательно, $x(t) \equiv 0$, то есть $\sum_{j=1}^{n} \lambda_jx^j(t) \equiv 0$, то есть $x^1, \dots, x^n$ линейно зависимы.

\QED

\textbf{Замечание.} Смысл утверждения состоит в том, что ранее мы сказали, что если функции линейно зависимы, то определитель Вронского является тождественным нулём, но обратное не всегда верно; однако если функции являются решением системы (1), то это уже выполняется в обе стороны.

\textbf{Определение.} \textit{Фундаментальной системой решений} называется упорядоченный набор из $n$ линейно независимых решений системы (1).

\textbf{Утверждение.} ФСР существует.

\textbf{Доказательство.} Зафиксируем $t_0$ и рассмотрим задачу Коши
\[
    \begin{cases}
        x' = A(t) \cdot x \\
        x(t_0) = e_j
    \end{cases} ,
\]
где $e_j$ --- базисный вектор. По теореме о существовании у неё существует решение $x^j \in C^1(I, \mathbb R^n)$ при любом $j \in [1, n]$.
Покажем, что все $x^j$ линейно независимы. Действительно, $x^j(t_0) = e_j$, то есть все $x^j(t_0)$ линейно независимы, поэтому можно доказать от противного, что и сами решения линейно независимы.

\QED

\textbf{Определение.} \textit{Общим решением} называется множество всех решений системы (1).

\textbf{Теорема.} Если $x^1, \dots, x^n$ --- ФСР, то общее решение --- это линейная оболочка $x^1, \dots, x^n$.

\textbf{Доказательство.} Докажем два вложения. $\supset$: очевидно. $\subset$. Пусть $\widehat x(\cdot)$ --- решение системы (1). 
Возьмём произвольное $t_0 \in I$, тогда, так как $x^j$ образуют ФСР, $x^j(t_0)$ линейно независимы по утверждению выше.
Следовательно, существует представление $\widehat x(t_0) = \sum_{j=1}^{n} \lambda_jx^j(t_0)$.
Рассмотрим задачу Коши
\[
    \begin{cases}
        x' = A(t) \cdot x \\
        x(t_0) = \widehat x(t_0)
    \end{cases} .
\]
У неё есть два решения --- $\widehat x(\cdot)$ и $\sum_{j=1}^{n} \lambda_jx^j(\cdot)$, а по теореме о существовании и единственности для линейного уравнения на интервале $I$ решение только одно.
Следовательно, они совпадают, что и доказывает включение.

\QED

\textbf{Определение.} Пусть $x^1, \dots, x^n \in C^1(I, \mathbb R^n)$ --- ФСР.
Тогда $X(t) = (x^1(t), \dots, x^n(t))$ для $t \in I$ называется \textit{фундаментальной матрицей решений} (ФМР).

\textbf{Замечание.} Общее решение системы (1) можно переписать через ФМР: $\{X(\cdot) \cdot c: c \in \mathbb R^n\}$.

\textbf{Утверждение.} $X'(t) \equiv A(t) X(t)$, $t \in I$. Доказывается прямой проверкой.

\textbf{Теорема.} (Об описании множества всех ФМР) Пусть $X(\cdot)$ --- фундаментальная матрица решений.
Тогда множество всех ФМР системы (1) записывается в виде $\{X(\cdot) C: C \in \mathbb R^{n \times n}, \det(C) \ne 0\}$.

\textbf{Доказательство.} $\supset$. Возьмём такую матрицу $C$. Тогда
\[
    X(t) C = \left(\sum_{j=1}^{n} C_{j1} x^j(t)~\dots~\sum_{j=1}^{n} C_{jn}x^j(t) \right).
\]
Каждый столбец является линейной комбинацией столбцов $X$, то есть решением системы.
Более того, $\det(XC) = \det(X) \cdot \det(C) \ne 0$, поэтому столбцы линейно независимы, то есть образуют ФМР.

$\subset$. Пусть $Y(\cdot)$ --- фундаментальная матрица решений, зафиксируем произвольное $t_0 \in I$ и положим $C = X^{-1}(t_0) \cdot Y(t_0)$.
Рассмотрим матрицу $X(\cdot) C$ --- она является ФМР по первому пункту и совпадает с $Y$ в точке $t_0$.
Пусть $e_j$ --- базисный вектор.
Рассмотрим задачу Коши
\[
    \begin{cases}
        x' = Ax \\
        x(t_0) = Y e_j (t_0)
    \end{cases} .
\]
У неё есть два решения: $x = Y e_j$ и $x = XC e_j$, следовательно, они совпадают на интервале $I$.
Таким образом, все столбцы матриц $Y$ и $XC$ совпадают.

\QED

\textbf{Теорема.} (Формула Лиувилля-Остроградского) Пусть $x^1, \dots, x^n$ --- ФСР, $\omega(\cdot)$ --- её определитель Вронского, тогда для всех $t_0 \in I$ верно
\[
    \omega(t) \equiv \omega(t_0) \exp \left( \int_{t_0}^{t} \tr(A(s)) ds \right).
\]

\textbf{Лемма.} Пусть
\[
    \omega_j(t) = \det
    \begin{pmatrix}
        x_1^1(t) & \dots & x_1^n(t) \\
        \vdots \\
        (x^1_j)'(t) & \dots & (x^n_j)'(t) \\
        \vdots \\
        x_n^1(t) & \dots & x_n^n(t)
    \end{pmatrix} .
\]
(производные только в $j$-ой строке). Тогда $\omega'(t) = \sum_{j=1}^{n} \omega_j(t)$ для всех $t \in I$.

\textbf{Доказательство.} Вспомним, что определитель --- это сумма произведений по всех перестановкам.
\[
    \omega'(t) = \frac{d}{dt} \left( \sum_{\pi} \sigma(\pi) x_1^{\pi(1)}(t) \dots x_n^{\pi(n)}(t) \right) = 
\]
Теперь продифференцируем:
\[
    = \sum_\pi \sigma(\pi) \left( (x_1^{\pi(1)})' x_2^{\pi(2)} \dots x_n^{\pi(n)} + \dots + x_1^{\pi(1)} \dots (x_n^{\pi(n)})' \right).
\]
Разобъём на $n$ сумм, где в $i$-ой сумме берётся производная у $x_i$, и получим искомое.

\QED

\textbf{Доказательство теоремы.} По лемме
\[
    \omega'(t) = \sum_{j=1}^{n} \det
    \begin{pmatrix}
        x_1^1(t) & \dots & x_1^n(t) \\
        \vdots & \vdots & \vdots \\
        (x_j^1)'(t) & \dots & (x_j^1)'(t) \\
        \vdots & \vdots & \vdots \\
        x_n^1(t) & \dots & x_n^b(t)
    \end{pmatrix} .
\]
Теперь вспомним, что каждый столбец является решением системы, поэтому производную по тождеству $x_i' = Ax_i$ можно переписать в виде
\[
    \sum_{j=1}^{n} \det
    \begin{pmatrix}
        x_1^1(t) & \dots & x_1^n(t) \\
        \vdots & \vdots & \vdots \\
        \sum_{k=1}^{n} a_{jk}(t) x_k^1 & \dots & \sum_{k=1}^{n} a_{jk}(t) x_k^n(t) \\
        \vdots & \vdots & \vdots \\
        x_n^1(t) & \dots & x_n^n(t)
    \end{pmatrix} .
\]
Теперь умножим $i$-ую строку ($i \ne j$) на $a_{ji}$ и вычтем из $j$-ой строки.
Тогда в $j$-ой строке в сумме останется лишь одно слагаемое --- $a_{jj}(t) \cdot x_j^i$, где $i$ --- номер столбца.
Остаётся по линейности определителя вынести $a_{jj}$ из $j$-ой строки, и в матрице останется просто $\omega(t)$, то есть
\[
    \omega'(t) = \sum_{j=1}^{n} a_{jj}(t) \omega(t) \equiv \omega(t) \tr(A(t)).
\]
Теперь посмотрим на это, как на дифференциальное уравнение. Это уравнение с разделяющими переменными, его можно решить и получить
\[
    \omega(t) = C \exp \left( \int_{t_0}^{t} \tr(A(s)) ds \right).
\]
Теперь подставим $t = t_0$ и получим, что $C = \omega(t_0)$.

\QED

\textbf{Замечание.} Звучит, как что-то странное и нигде не нужное, но на практике это помогает, когда мы смогли угадать одно решение системы и хотим найти остальные.

\section{Линейные неоднородные системы ОДУ.\\ Линейные ОДУ высших порядков}
\subsection{Линейные неоднородные системы ОДУ}
\setcounter{equation}{0}
Пусть нам даны $n \in \mathbb N$, интервал $I \subset \mathbb R$ и функции $A \in C(I, \mathbb R^{n \times n})$, $b \in C(I, \mathbb R^n)$.
Мы уже умеем решать систему вида
\begin{equation}
    x' = A(t)x
\end{equation}
Нас интересуют решения системы
\begin{equation}
    x' = A(t)x + b(t).
\end{equation}

\textbf{Теорема.} Пусть $\widehat x(\cdot)$ --- частное решение системы (2).
Тогда общее решение системы (2) --- множество всех функций вида $x(\cdot) + \widehat x(\cdot)$, где $x(\cdot)$ --- решение системы (1).
Очевидно, так как это линейное уравнение.

\subsubsection{Метод вариации постоянных}
Позволяет находить частное решение системы (2).

Пусть $X(\cdot) = (x_1(\cdot), \dots, x_n(\cdot))$ --- фундаментальная матрица решений (1), $t_0 \in I$.
Тогда $x(t) = X(t) C$ --- общее решение (1).
Найдём частное решение (2) в виде
\[
    x(t) + X(t) C(t), t \in I.
\]
Продифференцируем:
\[
    X'(t) C(t) + X(t) C'(t) = A(t) X(t) C(t) + b(t).
\]
Так как $A(t) X(t) = X'(t)$, можно упростить:
\[
    X(t) C'(t) = b(t).
\]
Тогда
\[
    C'(t) = X^{-1}(t) b(t),
\]
то есть
\[
    C(t) = \int_{t_0}^{t} X^{-1}(s) b(s) ds.
\]
Таким образом, частное решение записывается в виде
\[
    \widehat x(t) = X(t) \int_{t_0}^{t} X^{-1}(s) b(s) ds.
\]
И общее решение --- это
\[
    x(t) = X(t) C + X(t) \int_{t_0}^{t} X^{-1}(s) b(s) ds.
\]
Корректность следует из того, что все переходы верны в обе стороны.

\subsection{Линейные однородные ОДУ}
Пусть даны $n \in \mathbb N$, интервал $I \subset \mathbb R$ и функции $b, a_0, a_1, \dots, a_n \in C(I, \mathbb R (\mathbb C))$ --- можно и для вещественных, и для комплексных функций.
Дополнительное ограничение: $a_0$ не обращается в ноль.
Нас интересует уравнение
\begin{equation}
    \sum_{j=0}^{n} a_{n-j}(t) x^{(j)} = 0 \sim a_0(t) x^{(n)} + \dots + a_{n-1}(t) x' + a_n(t) x = 0.
\end{equation}
И уравнение
\begin{equation}
    \sum_{j=0}^{n} a_{n-j}(t) x^{(j)} = b(t) \sim a_0(t) x^{(n)} + \dots + a_{n-1}(t) x' + a_n(t) x = b(t).
\end{equation}
(В обоих случаях написаны две эквивалентные формы)

Сделаем замену: 
\begin{equation}
    y^1 = x, y^2 = x', \dots, y^n = x^{(n-1)}.
\end{equation}
Тогда уравнение (3) записывается в виде
\begin{equation}
    \begin{cases}
        (y^1)' = y^2 \\
        \vdots \\
        (y^{n-1})' = y^n \\
        (y^n)' = -\frac{a_1(t)}{a_0(t)}y^n - \dots - \frac{a_n(t)}{a_0(t)} y^1
    \end{cases} .
\end{equation}
Сделаем то же самое для уравнения (4):
\begin{equation}
    \begin{cases}
        (y^1)' = y^2 \\
        \vdots \\
        (y^{n-1})' = y^n \\
        (y^n)' = -\frac{a_1(t)}{a_0(t)}y^n - \dots - \frac{a_n(t)}{a_0(t)} y^1 + \frac{b(t)}{a_0(t)}
    \end{cases} .
\end{equation}
Эквивалентности $(3) \sim (6)$ и $(4) \sim (7)$ доказываются прямой проверкой.

\textbf{Лемма.} Пусть $x_1(\cdot), \dots, x_k(\cdot)$ --- решения уравнения (3), а $y_1(\cdot), \dots, y_k(\cdot)$ --- соответствующие после замены решения системы (6).
Система функций $x_1, \dots, x_k$ линейно зависима тогда и только тогда, когда система $y_1, \dots, y_k$ линейно зависима.

\textbf{Доказательство.} Из определения линейной зависимости существует ненулевой $\lambda \in \mathbb R^k (\mathbb C^k)$, такой что $\sum_{j=1}^{n} \lambda_j x_j (t) \equiv 0$.
Продифференцируем сумму $n$ раз:
\[
    \exists \lambda \in \mathbb R^k (\mathbb C^k):
    \begin{cases}
        \sum_{j=1}^{n} \lambda_j x_j(t) \equiv 0 \\
        \sum_{j=1}^{n} \lambda_j x_j'(t) \equiv 0 \\
        \sum_{j=1}^{n} \lambda_j x_j''(t) \equiv 0 \\
        \vdots
    \end{cases}
\]
Подставим $y_i$ и просуммируем покоординатно (то есть $y_j = (y_j^1, \dots, y_j^n)$:
\[
    \iff \exists \lambda \in \mathbb R^k (\mathbb C^k):
    \sum_{j=1}^{n} \lambda_j y_j(t) \equiv 0
\]
Первое утверждение эквивалентно линейной зависимости $x_1, \dots, x_k$, второе --- $y_1, \dots, y_k$.

\QED

\textbf{Теорема.} Существуют линейно независимые решения $x_j(\cdot)$, где $j \in [1, n]$, уравнения (3).
Более того, решений ровно $n$, то есть его общее решение будет имеет вид
\[
    x(t) = c_1x_1(t) + \dots + c_nx_n(t), t \in I, c_j \in \mathbb R (\mathbb C).
\]
Следует из эквивалентности системе (6), для которой аналогичные утверждения были доказаны ранее.

\QED

\textbf{Замечание.} Для комплексных чисел аналогичной утверждение не было доказано, но в этом случае на систему из $n$ комплексных уравнений можно смотреть, как на систему из $2n$ вещественных уравнений.

\textbf{Определение.} Пусть $x_1, \dots, x_n \in C^{n-1}(I, \mathbb R(\mathbb C))$.
\textit{Определителем Вронского} этой системы функций называется
\[
    \omega(t) =
    \begin{vmatrix}
        x_1(t) & \dots & x_n(t) \\
        x_1'(t) & \dots & x_n'(t) \\
        \vdots & \ddots & \vdots \\
        x_1^{(n-1)}(t) & \dots & x_n^{(n-1)}(t)
    \end{vmatrix}
\]
Нетрудно проверить, что для линейно зависимых функций определитель Вронского равен нулю, но в обратную сторону это неверно: $x_1(t) = t^2$, $x_2(t) = t|t|$.
(Здесь небольшое дежавю, но ранее мы определяли его для более узкого случая).

\textbf{Утверждение.} (О связи определителя Вронского для функций и для вектор-функций)
Пусть $x_1, \dots, x_n$ --- решение уравнения (3), $y_1, \dots, y_n$ --- решение системы (6), связанное с $x_1, \dots, x_n$ заменой (5).
Тогда $\omega(t)$ совпадает с определителем Вронского вектор-функций $y_1, \dots, y_n$.
Доказывается прямой проверкой.

\textbf{Утверждение.} Пусть $x_1, \dots, x_n$ --- решение уравнения (3).
Тогда если существует $\tau$, такое что $\omega(\tau) = 0$, то $\omega(t) \equiv 0$ и функции линейно зависимы.

Доказывается переходом к вектор-функциям $y_i$: возьмём $\tau$, получим линейную зависимость функций $y_i$ в какой-то точке, значит, они линейно зависимы, так что $\omega(t) \equiv 0$ и для функций $y_i$, и для функций $x_i$ по утверждению выше.

\textbf{Утверждение.} (Формула Лиувилля-Остроградского)
\[
    \omega(t) \equiv \omega(t_0) \exp \left( -\int_{t_0}^{t} \frac{a_1(s)}{a_0(s)}ds \right).
\]

\textbf{Доказательство.} Найдём матрицу $A$ для вектор-функций $y_i$:
\[
    A(t) =
    \begin{pmatrix}
        0 & 1 & 0 & \dots & 0 \\
        0 & 0 & 1 & \dots & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        -\frac{a_n}{a_0} & -\frac{a_{n-1}}{a_0} & -\frac{a_{n-2}}{a_0} & \dots & -\frac{a_1}{a_0}
    \end{pmatrix}
\]
Соответственно её след равен $-\frac{a_1}{a_0}$.

\QED

\textbf{Теорема.} (О решении уравнения (4)) $x_{\text{общ.}(4)} = x_{\text{част.}(4)} + x_{\text{общ.}(3)}$.
Частное решение уравнения (4) можно найти методом вариации постоянной: пусть $x_1, \dots, x_n$ --- линейно независимые решения уравнения (3).
Будем искать частное решение (4) в виде
\[
    x(t) = \sum_{j=1}^{n} C_j(t) x_j(t).
\]
Возьмём $C_j(\cdot)$, которые получаются при применении метода вариации постоянной для системы (7).
Заметим, что они подойдут и для системы (4), делается прямой проверкой.
Как доказывалось в разделе про линейные системы, $C_j(\cdot)$ обладают следующим свойством: если мы ищем частное решение системы $x' = A(t) x + b(t)$, то $X(t) C' = b(t)$, где $X(\cdot)$ --- фундаментальная матрица решений.
Подставим в эту формулу систему (7):
\[
    \begin{pmatrix}
        x_1(t) & \dots & x_n(t) \\
        x_1'(t) & \dots & x_n'(t) \\
        \vdots & \ddots & \vdots \\
        x_1^{(n-1)}(t) & \dots & x_n^{(n-1)}(t)
    \end{pmatrix}
    \begin{pmatrix}
        C_1'(t) \\
        C_2'(t) \\
        \vdots \\
        C_n'(t)
    \end{pmatrix}
    =
    \begin{pmatrix}
        0 \\
        0 \\
        \vdots \\
        \frac{b(t)}{a_0(t)}
    \end{pmatrix} .
\]
Остаётся выразить $C(t)$, решать такие системы мы умеем.

\textbf{Пример.} $x'' - 3x' + 2x = e^{-t}$.
Здесь $n = 2$, $a_0(t) \equiv 1$, $a_1(t) \equiv -3$, $a_2(t) \equiv 2$, $b(t) \equiv e^{-1}$ --- линейное ОДУ.
Будем искать решение вида $x(t) = e^{\lambda t}$.
Тогда однородное уравнение переписывается в виде $(\lambda^2 - 3\lambda + 2) e^{\lambda t} = 0$, то есть решение --- $x(t) = c_1e^t + c_2 e^{2t}$.
Найдём частное решение:
\[
    \begin{pmatrix}
        e^t & e^{2t} \\
        e^t & 2e^{2t}
    \end{pmatrix}
    \begin{pmatrix}
        c_1' \\
        c_2'
    \end{pmatrix}
    =
    \begin{pmatrix}
        0 \\
        e^{-t}
    \end{pmatrix}
\]
Умножим на обратную матрицу:
\[
    \begin{pmatrix}
        c_1' \\
        c_2'
    \end{pmatrix}
    =
    \frac{1}{e^{3t}}
    \begin{pmatrix}
        2e^{2t} & -e^{2t} \\
        -e^t & e^t
    \end{pmatrix}
    \begin{pmatrix}
        0 \\
        e^{-t}
    \end{pmatrix}
\]
То есть $c_1' = -e^{-2t}$ и $c_2' = e^{-3t}$.
Интегрируя, получаем $c_1 = \frac{1}{2} e^{-2t}$ и $c_2 = -\frac{1}{3} e^{-3t}$.
Тогда частным решением будет
\[
    x(t) = \frac{1}{2} e^{-2t} e^t - \frac{1}{3} e^{-3t}e^{2t} = \frac{1}{6} e^{-t}.
\]
Общее решение --- сумма общего однородного и частного.

\setcounter{equation}{0}
\section{Линейные ОДУ с постоянными коэффициентами}
\subsection{Линейные однородные ОДУ с постоянными коэффициентами}
Пусть $n \in N$, $a_0, \dots, a_n \in \mathbb C$, причём $a_0 \ne 0$.
Рассмотрим уравнение 
\begin{equation}
    a_0 x^{(n)} + \dots + a_n x =0.
\end{equation}
Сделаем замену $x(t) = e^{\lambda t}$, тогда уравнение приводится к виду $M(\lambda) = a_0 \lambda^n + \dots + a_n = 0$.
Тогда, если $\lambda_1, \dots, \lambda_n$ --- корни этого уравнения, решением задачи будут функции $x(t) = e^{\lambda_i t}$ и любые их линейные комбинации.

Положим $Lx(t) = \sum_{j=0}^{n} a_{n-j} x^{(j)} (t)$ --- линейный оператор, подставляющий $x$ в уравнение (1).

\textbf{Лемма.} Пусть $\gamma \in \mathbb C$ --- корень $M$ кратности $k \in \mathbb N \cup \{0\}$, $s$ --- неотрицательное целое число.
Тогда
\[
    L(t^s e^{\gamma t}) =
    \begin{cases}
        0, & s \le k - 1 \\
        P(t) e^{\gamma t}, & s \ge k
    \end{cases} ,
\]
где $P$ --- какой-то многочлен степени $s - k$.

\textbf{Доказательство.}
\[
    L(t^s e^{\gamma t}) \equiv \sum_{j=0}^{n} a_{n-j} \frac{\partial^j}{\partial t^j} (t^s e^{\gamma t}) \equiv \sum_{j=0}^{n} a_{n-j} \frac{\partial^j}{\partial t^j} \left( \frac{\partial^s}{\partial \gamma^s} e^{\gamma t} \right) \equiv
\]
(Поменяем частные производные местами)
\[
    \equiv \sum_{j=0}^{n} a_{n-j} \frac{\partial^s}{\partial \gamma^s} (\gamma^j e^{\gamma t}) \equiv \frac{\partial^s}{\partial \gamma^s} (e^{\gamma t} M(\gamma)) \equiv \sum_{j=0}^{s} C_s^j M^{(j)}(\gamma) t^{s-j} e^{\gamma t} \equiv
\]
(так как $\gamma$ --- корень кратности $k$)
\[
    \equiv \sum_{j=k}^{s} C_s^j M^{(j)}(\gamma) t^{s-j} e^{\gamma t}.
\]
Получили искомый многочлен.

\QED

\textbf{Теорема.} Пусть $\lambda_1, \dots, \lambda_m$ --- это попарно различные корни кратностей $k_1, \dots, k_m$ соответственно.
Тогда $t^s e^{\lambda_j t}$ ($s \in [0, k_j), j \in [1, m]$) является фундаментальной системой решений.

\textbf{Доказательство.} Очевидно, что эти функции являются решениями (применим оператор $L$ и по лемме получаем 0).
Докажем, что они линейно независимы.
Пусть $\sum c_{j,s} \cdot t^s e^{\lambda_j t} \equiv 0$.
Тогда существуют многочлены, такие что
\[
    p_1(t) e^{\lambda_1 t} + \dots + p_m(t) e^{\lambda_m t} \equiv 0.
\]
Разделим:
\[
    p_1(t) + \dots + p_m(t) e^{(\lambda_m - \lambda_1)t} \equiv 0.
\]
Продифференцируем достаточное число раз, чтобы $p_1(t)$ обнулился. Получаем новые многочлены
\[
    \widehat p_2(t) e^{\lambda_2 - \lambda_1)t} + \dots + \widehat p_m e^{(\lambda_m - \lambda_1)t} \equiv 0.
\]
Нетрудно доказать, что $\deg(p_i) = \deg(\widehat p_i)$, поэтому эту операцию можно дальше продолжать, пока не получим
\[
    \widehat p_m(t) e^{(\lambda_m - \lambda_{m-1})t} \equiv 0.
\]
Получаем, что многочлен $\widehat p_m$ нулевой, то есть у него степень $-\infty$, то есть и у исходного многочлена $p_m$ была такая степень.
Повторяя эту процедуру для остальных многочленов, получаем, что все многочлены нулевые.
Следовательно, все коэффициенты линейной комбинации нулевые.

\QED

\subsection{Линейные неоднородные ОДУ с постоянными коэффициентами}
Всё то же самое, но теперь
\begin{equation}
    a_0 x^{(n)} + \dots + a_{n-1} x' + a_n x = b(t).
\end{equation}
Если $b(\cdot)$ --- многочлен, то достаточно найти частное решение для случая, когда $b(\cdot)$ является мономом, а это делается методом неопределённых коэффициентов.
Рассмотрим случай, когда $b(t) = p(t) e^{\gamma t}$ --- квазимногочлен.

\textbf{Теорема.} Существует решение $x(\cdot)$ уравнения (2) вида $x(t) = t^k q(t) e^{\gamma t}$, где $k$ --- кратность корня $\gamma$ многочлена $M$, а $q$ --- какой-то многочлен, степень которого не превосходит степень $p$.

\textbf{Доказательство.} Пусть $m = \deg(p)$, $p(t) = q_0 t^m + \dots + q_{m-1} t + q_m$.
Положим $x_1(t) = a t^{m+k} e^{\gamma t}$ (число $a$ определим позже).
Тогда по лемме
\[
    L x_1(t) = a(\Theta t^m + r(t)) e^{\gamma t}, \deg(r) < m.
\]
Положим $a = \frac{p_0}{\Theta}$. Тогда $Lx_1(t) = (p_0 t^m + \widehat r(t)) e^{\gamma t}$.
Докажем теорему индукцией по $m$. При $m = 0$ имеем $x = x_1$ --- искомый многочлен.
Пусть теорема верна для многочленов степени меньше $m$.
Будем искать решение в виде $x = x_1 + x_2$, где $x_2$ --- квазимногочлен, то есть мы хотим, чтобы $L(x_1 + x_2)(t) \equiv p(t) e^{\gamma t}$.
Распишем $x_1$ и $x_2$:
\[
    (p_0 t^m + \widehat r(t)) e^{\gamma t} + Lx_2(t) = (p_0 t^m + p_1 t^{m-1} + \dots + p_m) e^{\gamma t}.
\]
Сократим $p_0 t^m e^{\gamma t}$:
\[
    Lx_2(t) \equiv (-\widehat r(t) + p_1 t^{m-1} + \dots + p_m) e^{\gamma t}.
\]
Справа имеем многочлен степени менее $m$, поэтому можно применить предположение индукции:
\[
    \exists q_2: L(t^k q_2(t) e^{\gamma t}) \equiv (-\widehat r(t) + p_1 t^{m-1} + \dots) e^{\gamma t}, \deg(q_2) \le m - 1.
\]
Таким образом,
\[
    x(t) = \frac{p}{\Theta} t^{m+k} e^{\gamma t} + t^k q_2(t) e^{\gamma t}
\]
является решением уравнения (2).

\QED

\textbf{Пример.} $x'' - x = t^3 e^t$.
Характеристический многочлен --- $\lambda^2 - 1 = 0$. Возьмём корень $\gamma = 1$ кратности 1.
Тогда решение имеет вид
\[
    x(t) = t^1 (q_0t^3 + q_1t^2 + q_2 t + q_3) e^t.
\]
Теперь это можно поставить в исходное уравнение и получить систему уравнений, из которой получается решение.

\subsection{Вещественные решения}
Теперь коэффициенты вещественные, уравнение однородное и мы хотим найти вещественные решения.
Тогда характеристический многочлен имеет решения трёх типов: вещественные и пары сопряжённых комплексных.
Рассмотрим пары комплексных $\lambda = \alpha \pm i \beta$.
Тогда $t^s e^{\lambda t}$ и $t^s e^{\overline{\lambda} t}$ --- решения (1).
В силу линейности решениями также являются
\[
    \frac{t^s e^{\lambda t} + t^s e^{\overline \lambda t}}{2},~~
    \frac{t^s e^{\lambda t} - t^s e^{\overline \lambda t}}{2i}.
\]
Складывая эти решения, получаем по определению синуса и косинуса решения вида $t^s e^{\alpha t} \cos(\beta t)$ и $t^s e^{\alpha t} \sin(\beta t)$ --- вещественные.

\textbf{Теорема.} Вместе с корнями $\lambda \in \mathbb R$ многочлена $M$ эти решения образуют фундаментальную систему решений.

\textbf{Доказательство.} Пусть $\lambda_j = \alpha_j + i \beta_j$, $\overline{\lambda_j} = \alpha_j - i \beta_j$ для $j \in [1, l]$ и $\lambda_j \in \mathbb R$ для $j \in [2l + 1, n]$ --- все решения в комплексных числах.
Положим $z_j^s(t) = t^s e^{\lambda_j t}$, $z_{j+l}^s(t) = t^s e^{\overline{\lambda_j} t}$, $z_j^s(t) = t^s e^{\lambda_j t}$ --- соответствующие этим $\lambda$ решения.
Докажем линейную независимость, пусть
\[
    \sum_s \left(\sum_{j=1}^{l} c_j^s \frac{z_j^s + z_{j+l}^s}{2} + \sum_{j=1}^{l} c_{j+l}^s \frac{z_j^s - z_{j+l}^s}{2i} + \sum_{j=2l+1}^{m} c_j^s z_j \right) \equiv 0.
\]
Приведём подобные слагаемые:
\[
    \left( \frac{c_1^s}{2} + \frac{c_{l+1}^s}{2i} \right) z_1^s + \left( \frac{c_1^s}{2} - \frac{c_{l+1}^s}{2i} \right) z_{l+1}^s + \dots + \sum_{j=2l+1}^{m} c_j^s z_j^s \equiv 0.
\]
В силу равенства нулю все коэффициенты равны нулю.
Из этих слагаемых видим, что $c_1^s = c_{l+1}^s = c_{2l+1}^s = 0$.
Аналогично для остальных.

\QED

Случай неоднородных уравнений: выражение в правой части можно привести к виду $(P_1(t) \cos(\mu t) + B(t) \sin(\mu t)) e^{\eta t}$.
Решение тогда ищется в виде
\[
    x(t) = t^k (Q_1(t) \cos(\mu t) + Q_2(t) \sin(\mu t)) e^{\eta t},
\]
где $k$ --- кратность корня $\gamma = \eta + i \mu$.

\setcounter{equation}{0}
\section{Системы линейных ОДУ с постоянными коэффициентами}
\subsection{Комплексные однородные системы}
Пусть нам даны $n \in \mathbb N$ и матрица $A \in \mathbb C^{n \times n}$.
Рассмотрим систему
\begin{equation}
    x' = Ax.
\end{equation}

Как мы знаем из алгема, матрицу $A$ можно привести к жордановой нормальной форме, то есть найдутся матрицы $B, C \in \mathbb C^{n \times n}$, причём $\det(C) \ne 0$, такие что $B = C^{-1} A C$, и матрица $B$ имеет вид
\[
    \begin{pmatrix}
        K_1 & 0 & 0 \\
        \vdots & \ddots & \vdots \\
        0 & \dots & K_s
    \end{pmatrix},
\]
где $K_j$ --- жорданова клетка с собственным значением $\lambda_j$ и собственным вектором $h_j$.

\textbf{Определение.} $h_{j,1}, \dots, h_{j,k_j}$ называется \textit{серией} с собственным значением $\lambda_j$, если $Ah_{j,1} = \lambda_j h_{j,1}$, $Ah_{j,2} = \lambda_j h_{j,2} + h_{j,1}$, и так далее, $Ah_{j,k_j} = \lambda h_{j,k_j} + h_{j,k_j - 1}$.
У Штепина это, кажется, называлось жордановой диаграммой.

Научимся их находить проще.
Пусть $e_1, \dots, e_n$ --- базис, тогда, просто подставляя в определение $B$, получаем $Be_1 = \lambda_1 e_1$, $Be_2 = \lambda_1 e_2 + e_1$, и так далее, $Be_{k_1} = \lambda_1 e_{k_1} + e_{k_1 - 1}$.
Теперь выразим $B$ через $A$:
\[
    C^{-1} A C e_1 = \lambda_1 e_1 \Rightarrow A(Ce_1) = \lambda_1 (Ce_1).
\]
Аналогично $A(Ce_2) = \lambda_1 (Ce_2) + Ce_1$.

Таким образом, зная матрицу $C$, мы можем вычислить серию довольно нехитрым образом: $h_{1,i} = Ce_i$, аналогично для остальных клеток.

На этом воспоминания из алгема заканчиваются, вернёмся к диффурам.
Положим
\[
    \omega_{j,r}(t) := \frac{t^{r-1}}{(r-1)!} h_{j,1} + \frac{t^{r-2}}{(r-2)!} h_{j,2} + \dots + h_{j,r},
\]
где $j = \overline{1,s}$ и $r = \overline{1,k_j}$.
Заметим, что всего функций будет $n$ из простых комбинаторных соображений.

\textbf{Теорема.} Набор функций $\omega_{j,r}(t) e^{\lambda_j t}$ является фундаментальной системой решений.
Так, мы научились явно строить решение системы.

\textbf{Доказательство.} Проверим, что все $n$ функций действительно являются решениями.
Для этого нам понадобится два тождества.
Первое:
\[
    \omega_{j,r}'(t) \equiv \omega_{j,r-1}(t).
\]
(здесь мы считаем, что $\omega_{j,0}(t) \equiv 0$)

Второе:
\[
    A \omega_{j,r}(t) \equiv \lambda_j \omega_{j,r}(t) + \omega_{j,r-1}(t).
\]
Следует из соотношений на $h_{j,\cdot}$, как на серию.

Теперь подставим в систему:
\[
    \frac{d}{dt} \left( \omega_{j,r}(t) e^{\lambda_j t} \right) \equiv \left( \omega_{j,r}'(t) + \lambda_j \omega_{j,r} \right) e^{\lambda_j t} \equiv (\omega_{j,r-1}(t) + \lambda_j \omega_{j,r}) e^{\lambda_j t} \equiv
\]
\[
    \equiv A\omega_{j,r}(t) \cdot e^{\lambda_j t} \equiv A (\omega_{j,r}(t) e^{\lambda_j t}).
\]
Следовательно, решением являются.

Проверим линейную независимость.
Заметим, что $\omega_{j,r}(t) e^{\lambda_j t} |_{t=0} = h_{j,r}$.
Из простых соображений все $h_{j,r}$ линейно независимы, откуда и $\omega_{j,r}(t) e^{\lambda_j t}$ линейно независимы.

\QED

\subsection{Вещественные однородные системы}
Пусть нам даны $n \in \mathbb N$ и $A \in \mathbb R^{n \times n}$, система та же.
Теперь нас напрягают комплексные собственные значения.

Пусть $\lambda_1, \dots, \lambda_l \in \mathbb C$, а $\lambda_{l+1}, \dots, \lambda_{2l}$ --- сопряжённые к ним соответственно, и оставшиеся $\lambda_{2l+1}, \dots, \lambda_s \in \mathbb R$.
Тогда серию можно построить так, что $h_{l+1, r} = \overline h_{1,r}, \dots, h_{2l,r} = \overline h_{l,r}$.

Определение $\omega_{j,r}$ остаётся тем же, но теперь для $j = \overline{1,l}$ мы рассмотрим функции
\[
    \frac{1}{2} \left( \omega_{j,r}(t) e^{\lambda_j t} + \overline{\omega_{j,r}(t) e^{\lambda_j t}} \right)
\]
и
\[
    \frac{1}{2i} \left( \omega_{j,r}(t) e^{\lambda_j t} - \overline{\omega_{j,r}(t) e^{\lambda_j t}} \right).
\]
Нетрудно заметить, что они вещественные, являются решениями и линейно независимы (это доказывалось в предыдущем параграфе).
Остаются вещественные собственные значения, но для них всё просто: $\omega_{j,r}(t) e^{\lambda_j t}$ подойдёт.

Перепишем решения для $j = \overline{1,l}$ в более удобном виде.
Пусть $\lambda_j = \alpha_j + i\beta_j$, $h_{j,r} = a_{j,r} + ib_{j,r}$ --- алгебраическая форма записи комплексного числа.
Тогда первая группа решений будет иметь вид
\[
    Re \left( \omega_{j,r}(t) e^{\lambda_j t} \right) = \left( \frac{t^{r-1}}{(r-1)!} a_{j,1} + \dots + a_{j,r} \right) \cos(\beta_j t) e^{\alpha_j t} -
\]
\[
    - \left( \frac{t^{r-1}}{(r-1)!} b_{j,1} + \dots + b_{j,r} \right) \sin(\beta_j t) e^{\alpha_j t}
\]
--- произведение действительных частей минус произведение мнимых.

Вторая ---
\[
    \left( \frac{t^{r-1}}{(r-1)!} b_{j,1} + \dots + b_{j,r} \right) \cos(\beta_j t) e^{\alpha_j t} + \left( \frac{t^{r-1}}{(r-1)!} a_{j,1} + \dots + a_{j,r} \right) \sin(\beta_j t) e^{\alpha_j t}.
\]

\subsection{Компексные неоднородные системы}
Как и ранее, у нас есть $n \in \mathbb N$, $A \in \mathbb C^{n \times n}$, многочлен $p: \mathbb R \to \mathbb C^n$ и число $\gamma \in \mathbb C$.
Рассмотрим систему
\begin{equation}
    x' = Ax + p(t) e^{\gamma t}.
\end{equation}

\textbf{Определение.} $p(t) e^{\gamma t}$ будем называть \textit{правой частью специального вида}.
В этом случае мы умеем находить частное решение даже без метода вариации постоянной.

Перепишем систему в более удобном виде
\begin{equation}
    z' = Bz + \tilde p(t) e^{\gamma t},
\end{equation}
где $B = C^{-1} A C$ и $\tilde p(t) = C^{-1} p(t)$.

Какая связь между системами (2) и (3)?
Пусть $z(\cdot)$ --- решение (3), положим $x(\cdot) := C z(\cdot)$, тогда $z = C^{-1} x$.
Подставим в систему (2): $C^{-1} x' = BC^{-1} x + \tilde p e^{\gamma t}$, откуда $x' = CBC^{-1} x + C \tilde p e^{\gamma t}$, то есть $x' = Ax + p e^{\gamma t}$.
Иными словами, системы эквивалентны, засим будем решать систему (3), ибо она проще из жордановых соображений.
В частности, нас только интересует какое-то одно частное решение, ибо остальные получаются по линейности.

Пусть
\[
    B =
    \begin{pmatrix}
        K_1 & 0 & 0 \\
        \vdots & \ddots & \vdots \\
        0 & \dots & K_s
    \end{pmatrix},
\]
$k_i$ --- размер клетки $K_i$, $\lambda_i$ --- собственные значения.
Напишем систему (3) построчно:
\[
    \begin{cases}
        z_1' = \lambda_1 z_1 + z_2 + \tilde p_1 e^{\gamma t} \\
        \vdots \\
        z_{k_1-1}' = \lambda_1 z_{k_1 - 1} + z_{k_1} + \tilde p_{k_1 - 1} e^{\gamma t} \\
        z_{k_1}' = \lambda_1 z_{k_1} + \tilde p_{k_1} e^{\gamma t} \\
        \vdots
    \end{cases}
\]
Проанализируем эти уравнения.
Если $\gamma \ne \lambda_1$, то решение $k_1$-ой строчки можно написать в виде $z_{k_1}(t) = q_{k_1} e^{\gamma t}$, где $\deg(q_{k_1}) \le \deg(p)$, по теореме для линейных однородных уравнений.
Аналогично $z_{k_1 - 1}(t) = q_{k_1-1}(t) e^{\gamma t}$, где $\deg(q_{k_1-1}) \le \deg(p)$.
Такую штуку можно провернуть для всех уравнений, что даёт нам искомое решение.

Если же $\gamma = \lambda_1$, то $k_1$-ое уравнение решается, как $z_{k_1}(t) = t \cdot q_{k_1}(t) e^{\gamma t}$ для $\deg(q_{k_1}) \le \deg(p)$.
Загоним $t$ в $q_{k+1}$, тогда у нас получится всё, как в предыдущем случае, но теперь $\deg(q_{k_1}) \le 1 + \deg(p)$, $\deg(q_{k_1-1}) \le 2 + \deg(p)$ и так далее.
По итогу,

\textbf{Теорема.} Если $\gamma \ne \lambda_j$ для всех $j$, то существует частное решение системы (2), имеющее вид $x(t) = P(t) e^{\gamma t}$, где $\deg(P) \le \deg(p)$.

А если же $\gamma = \lambda_j$, то существует частное решение системы (2), имеющее вид $x(t) = P(t) e^{\gamma t}$, где $\deg(P) \le \deg(p) + \max\{k_l\}$, где максимум берётся по таким $l$, что $\lambda_l = \lambda_j$, ибо в таких клетках как раз реализуется второй случай и оценка на степень ухудшается.

Таким образом, для нахождения частного решения можно просто применить метод неопределённых коэффициентов.

\setcounter{equation}{0}
\subsection{Вещественные неоднородные системы с правой частью специального вида}
Пусть дана $A \in \mathbb R^{n \times n}$, числа $\alpha, \beta \in \mathbb R$ и многочлены $U, V: \mathbb R \to \mathbb R^n$.
Рассмотрим систему
\begin{equation}
    x' = Ax + e^{\alpha t}(V(t) \cos(\beta t) + V(t) \sin(\beta t))
\end{equation}
и систему
\begin{equation}
    z' = Az + (U(t) - i V(t)) e^{(\alpha + i \beta) t}.
\end{equation}

У второй системы, как известно, есть решение вида
\[
    z(t) = (F(t) + i G(t)) e^{(\alpha + i\beta)t},
\]
где $F$ и $G$ --- многочлены, степени которых не превосходят $\max(\deg(U), \deg(V)) + k$, где $k$ --- размер наибольшей жордановой клетки, содержащей собственное значение $\lambda = \alpha + i\beta$, в жордановой форме $A$.

Решение первой системы тогда записывается в виде
\[
    x(t) = Re(z(t)) = e^{\alpha t} (F(t) \cos(\beta t) - G(t) \sin(\beta t)).
\]
Проверим это: при подстановке $Re(z(t))$ во вторую систему мы получим в точности систему (1).

\section{Матричная экспонента}
\subsection{Определение}
Пусть дана матрица $A \in \mathbb R^{n \times n}$ (можно и в $\mathbb C^{n \times n}$).
Тогда
\[
    e^A := \sum_{j=0}^{\infty} \frac{A^j}{j!},
\]
где $0! = 1$ и $A^0 = E$.

\textbf{Лемма.} Для любого $r > 0$ ряд $\sum_{j=0}^{\infty} \frac{t^j A^j}{j!}$ сходится абсолютно и равномерно при $|t| \le r$.

\textbf{Доказательство.} Оценим норму одного слагаемого:
\[
    \left\| \frac{t^j A^j}{j!} \right\| \le \frac{t^j}{j!} \| A \|^j,
\]
так как норма произведение не превосходит произведение норм.
Ряд слагаемых справа сходится абсолютно и равномерно, как экспонента, поэтому и слева то же самое.
Более того, норма суммы не превосходит сумму норм, поэтому
\[
    \left\| \sum_{j=0}^{\infty} \frac{t^j A^j}{j!} \right\| \le 
    \sum_{j=0}^{\infty} \left\| \frac{t^j A^j}{j!} \right\| \le 
    e^{t\|A\|}.
\]

\QED

\subsection{Свойства}
\textbf{Свойство 1.} Пусть матрицы $A$ и $B$ коммутируют, тогда $e^{A + B} = e^A e^B$.

\textbf{Доказательство.}
Вычислим левую и правую части:
\[
    e^{A+B} = \sum_{j=0}^{\infty} \frac{(A+B)^j}{j!} = \sum_{j,k=0}^{\infty} \Theta_{j,k} A^j B^k.
\]
\[
    e^A e^B = \sum_{j=0}^{\infty} \frac{A^j}{j!} \sum_{k=0}^{\infty} \frac{B^k}{k!} = \sum_{j,k=0}^{\infty} \mu_{j,k} A^j B^k.
\]
Можно вычислить значения $\Theta_{j,k}$ и $\mu_{j,k}$, но лучше заметим, что они не зависят от $n$, поэтому их можно вычислить при $n = 1$ и подставить в общую формулу.
Так вот, при $n = 1$ получаем, что $A$ и $B$ --- это числа, и свойство заведомо верно.
Следовательно, $\Theta_{j,k} = \mu_{j,k}$ при $n = 1$ и при всех $n$.

\QED

\textbf{Замечание.} Если $AB \ne BA$, то свойство не обязательно верно.
Возьмём 
\[
    A =
    \begin{pmatrix}
        0 & 1 \\
        0 & 0
    \end{pmatrix}
    ,
    B =
    \begin{pmatrix}
        0 & 0 \\
        1 & 0
    \end{pmatrix}
\]
Тогда $A^2 = B^2 = 0$ и
\[
    A + B =
    \begin{pmatrix}
        0 & 1 \\
        1 & 0
    \end{pmatrix},
    AB =
    \begin{pmatrix}
        1 & 0 \\
        0 & 0
    \end{pmatrix},
    BA =
    \begin{pmatrix}
        0 & 0 \\
        0 & 1
    \end{pmatrix}
\]
То есть $e^A e^B \ne e^B e^A$, но $e^{A + B} = e^{B + A}$.

\textbf{Свойство 2.} Функция $t \to e^{tA}$ дифференцируема и $\frac{d}{dt} e^{tA} = A e^{tA}$.

\textbf{Доказательство.}
Продифференцируем ряд $e^{tA} = \sum_{j=0}^{\infty} \frac{t^j A^j}{j!}$:
\[
    \frac{d}{dt} \left( \frac{t^j A^j}{j!} \right) \equiv \frac{t^{j-1} A^j}{(j-1)!}.
\]
Оценим норму одного слагаемого производной:
\[
    \left\| \frac{t^{j-1} A^j}{(j - 1)!} \right\| \le \frac{|t|^{j-1}}{(j-1)!} \|A\|^j.
\]
Этот ряд сходится абсолютно и равномерно.
Следовательно, функция дифференцируема и её можно дифференцировать почленно.
\[
    \frac{d}{dt} \left( e^{tA} \right) \equiv \sum_{j=1}^{\infty} \frac{t^{j-1} A^j}{(j-1)!} \equiv A \sum_{j=1}^{\infty} \frac{t^{j-1} A^{j-1}}{(j-1)!} \equiv A e^{tA}.
\]

\QED

\textbf{Замечание.} Матрицу $A$ можно было вынести и справа и получить $e^{tA} A$.

\textbf{Свойство 3.} $X(t) = e^{tA}$ является решением задачи Коши $X' = AX$, $X(0) = E$.
Первое следует из свойства 2, второе --- из определения.
В частности, $X(t)$ является фундаментальной системой решений системы $x
 = Ax$.

\textbf{Свойство 4.} $\det(e^{tA}) \equiv e^{t \cdot \tr(A)}$.
Следует из теоремы Лиувилля-Остроградского для уравнения $x' = Ax$: берём $t_0 = 0$, тогда матрица $A(s)$ не зависит от $s$ и интеграл тривиален.

\textbf{Свойство 5.} Пусть
\[
    K = 
    \begin{pmatrix}
        \lambda & 1 & 0 & \dots & 0 \\
        0 & \lambda & 1 & \dots & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \dots & \lambda
    \end{pmatrix},
    F = 
    \begin{pmatrix}
        0 & 1 & 0 & \dots & 0 \\
        0 & 0 & 1 & \dots & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \dots & 0
    \end{pmatrix}
\]
(то есть $K = \lambda E + F$ --- жорданова клетка, матрицы размера $m \times m$).
Тогда
\[
    e^{tF} = \sum_{j=0}^{m-1} \frac{t^j F^j}{j!} =
    \begin{pmatrix}
        1 & \frac{t}{1!} & \frac{t^2}{2!} & \dots & \frac{t^{m-1}}{(m-1)!} \\
        0 & 1 & \frac{t}{1!} & \dots & \frac{t^{m-2}}{(m-2)!} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \dots & 1
    \end{pmatrix}
\]
Также
\[
    e^{t \lambda E} \equiv e^{t \lambda} E.
\]
И по итогу
\[
    e^{tK} = e^{t \lambda E + tF} \equiv e^{t \lambda E} e^{tF} \equiv e^{t\lambda} e^{tF}.
\]

\textbf{Свойство 6.} Пусть
\[
    B =
    \begin{pmatrix}
        K_1 & 0 & \dots & 0 \\
        0 & K_2 & \dots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \dots & K_s
    \end{pmatrix} ,
\]
где $K_i$ --- жордановы клетки. Тогда
\[
    e^{tB} =
    \begin{pmatrix}
        e^{tK_1} & 0 & \dots & 0 \\
        0 & e^{tK_2} & \dots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \dots & e^{tK_s}
    \end{pmatrix}.
\]
Очевидно, так как мы возводим блочную матрицу в степень понятным образом.

\textbf{Свойство 7.} Если $B, C \in \mathbb C^{n \times n}$ и $\det(C) \ne 0$, причём $B = C^{-1} A C$, то $e^{tA} = C e^{tB} C^{-1}$.

\textbf{Доказательство.} Распишем $e^{tB}$ по определению, в каждом множителе $C^{-1}$ и $C$ посередине сократятся, а вхождения по краям можно вынести и получить искомое.

\QED

\section{Теорема Штурма}
\setcounter{equation}{0}
Пусть $I \subset \mathbb R$ --- интервал, $a \in C^1(I, \mathbb R)$, $b \in C(I, \mathbb R)$ --- функции.
Рассмотрим уравнение
\begin{equation}
    x'' + a(t)x' + b(t)x = 0.
\end{equation}
Правильной заменой его можно свести к уравнению
\begin{equation}
    y'' + q(t)y = 0.
\end{equation}
Найдём такую замену: пусть $x(t) = u(t) y(t)$, тогда $x' = u'y + uy'$ и $x'' = u''y + 2u'y' + uy''$.
Тогда уравнение (1) записывается в виде
\[
    u''y + 2u'y' + au'y + auy' + buy = 0.
\]
Теперь будем искать решение, удовлетворяющее уравнению $2u' + au = 0$, тогда $u' = -\frac{1}{2} au$ и 
\[
    u'' = -\frac{1}{2} a'u - \frac{1}{2} au' = \left( -\frac{1}{2} a' + \frac{1}{4} a^2 \right) u.
\]
Подставим в исходное:
\[
    uy'' + \left( \left( -\frac{1}{2} a' + \frac{1}{4} a^2 \right) u - \frac{1}{2} a^2 u + bu \right) y = 0.
\]
Если $2u' + au = 0$, то
\[
    u(t) = \exp \left( -\frac{1}{2} \int_{t_0}^{t} a(s) ds \right),
\]
поэтому она всегда положительна, и на неё можно разделить:
\[
    y'' + \left(-\frac{1}{2} a' - \frac{1}{4} a^2 + b \right) y = 0.
\]
Теперь будем рассматривать только уравнение (2).

\subsection{Свойства}
\textbf{Свойство 1.} Если $y(\cdot)$ --- нетривиальное решение, то все нули функции $y(\cdot)$ являются простыми, то есть если $y(t_0) = 0$, то $y'(t_0) \ne 0$.

\textbf{Доказательство.} Рассмотрим задачу Коши
\[
    \begin{cases}
        y'' + q(t) y = 0 \\
        y(t_0) = 0 \\
        y'(t_0) = 0
    \end{cases}
\]
У неё есть ровно одно решение $y(t) \equiv 0$ --- тривиальное.

\QED

\textbf{Определение.} $\widehat t$ называется \textit{точкой прикосновения множества} $T \subset \mathbb R$, если для любого $\varepsilon > 0$ множество $T \cap O_\varepsilon(\widehat t)$ непусто.
В отличие от предельных точек здесь шар не проколотый.

\textbf{Свойство 2.} Если $y(\cdot)$ --- нетривиальное решение, то множество нулей $\{t \in I: y(t) = 0\}$ не имеет предельных точек, то есть нет сходящейся последовательности нулей.

\textbf{Доказательство.} От противного: существует последовательность $\{t_j\} \subset I$ и $t \in I$, такие что $t_j \to t$, все $y(t_j) = 0$ и $y(t) = 0$.
Тогда между соседними $t_j$ производная $y'$ обращается в ноль по теореме Ролля, так как значения на концах равны.
Отсюда $y'(t) = 0$ --- противоречие со свойством 1.

\QED

\subsection{Теорема Штурма}
Пусть даны $q, Q \in C(I, \mathbb R)$, числа $t_1, t_2 \in I$, такие что $t_1 < t_2$ и уравнение
\begin{equation}
    z'' + Q(t) z = 0.
\end{equation}
Пусть также:
\begin{enumerate}
    \item $q(t) \le Q(t)$ на $I$.
    \item $y$ --- нетривиальное решение (2).
    \item $y(t_1) = y(t_2) = 0$.
    \item $y(t) \ne 0$ на $(t_1, t_2)$ (по свойству 2 это натуральное предположение).
    \item $z$ --- нетривиальное решение (3).
\end{enumerate}
Тогда либо
\begin{itemize}
    \item Существует $t_0 \in (t_1, t_2)$, такое что $z(t_0) = 0$.
    \item $z(t_1) = z(t_2) = 0$.
\end{itemize}

\textbf{Доказательство.} Пусть для определённости $y(t) > 0$ на $(t_1, t_2)$ (так как она непрерывна и не равна нулю на интервале, так считать можно).
Тогда $y'(t_1) \ge 0$ и $y'(t_2) \le 0$.
Более того, по свойству 1 $y'(t_1) > 0$ и $y'(t_2) < 0$.
Умножим (2) на $z$, (3) --- на $y$ и вычтем:
\[
    y''(t) z(t) - z''(t) y(t) \equiv (Q(t) - q(t)) y(t) z(t).
\]
Заметим, что в левой части производная:
\[
    \frac{d}{dt} \left( y'(t) z(t) - y(t) z'(t) \right) \equiv (Q(t) - q(t)) y(t) z(t).
\]
Проинтегрируем по $t$ от $t_1$ до $t_2$:
\[
    y'(t_2) z(t_2) - y(t_2) z'(t_2) - y'(t_1) z(t_1) + y(t_1) z'(t_1) = \int_{t_1}^{t_2} (Q(t) - q(t)) y(t) z(t) dt.
\]
Вспомним, что $t_1$ и $t_2$ --- нули функции $y$:
\begin{equation}
    y'(t_2) z(t_2) - y'(t_1) z(t_1) = \int_{t_1}^{t_2} (Q(t) - q(t)) y(t) z(t) dt.
\end{equation}

Предположим, что $z(t) > 0$ на $(t_1, t_2)$ (при $z(t) < 0$ аналогично).
Тогда есть 4 случая: $z(t) > 0$ на $[t_1, t_2]$, $z(t) > 0$ на $[t_1, t_2)$ и $z(t_2) = 0$ и наоборот, $z(t_1) = z(t_2) = 0$.
В первых трёх случаях правая часть уравнения (4) неотрицательна, а левая --- отрицательна, так как мы вычитаем из неположительного неотрицательное, и хотя бы одно из них отлично от нуля, противоречие.

Таким образом, мы предположили отрицание первого следствия и получили второе следствие, то есть теорема доказана.

\QED

\textbf{Пример.}
\[
    y'' + \frac{1}{4(1 + t^2)}y = 0, t > 0.
\]
Тогда его решение $y(\cdot)$ имеет не более двух нулей.
Рассмотрим уравнение
\[
    z'' + \frac{1}{4t^2} z = 0.
\]
Его решением является $z(t) = \sqrt t (c_1 \ln(t) + c_2)$ --- не более одного нуля. Положим
\[
    q(t) := \frac{1}{4(1 + t)^2}, Q(t) := \frac{1}{4t^2}.
\]
По теореме Штурма и перебору случаев получаем искомое.

\textbf{Пример 2.}
\[
    z'' + (1 + t^2) z = 0, t \in [1, 5].
\]
Любое нетривиальное решение имеет хотя бы один нуль.
Рассмотрим уравнение
\[
    y'' + 1 \cdot y = 0.
\]
Его решением является $y(t) = c_1 \sin(t + c_2)$.
У него либо 1, либо 2 нуля в зависимости от $c_2$. Рассмотрим решения с двумя.
По теореме Штурма между этими двумя нулями есть нуль функции $z(t)$.

\textbf{Следствие 1.} Пусть $q(t) \le 0$. Тогда для любого нетривиального решения $y(\cdot)$ мощность множества его нулей не превосходит 1.

\textbf{Доказательство.} Положим $Q(t) \equiv 0$. Тогда уравнение (3) имеет вид $z'' = 0$, и у него есть нетривиальное решение $z(t) \equiv 1$.
Если у решения $y(\cdot)$ хотя бы 2 нуля, то по теореме Штурма у $z(t) \equiv 1$ есть хотя бы один нуль.

\QED

\textbf{Следствие 2.} Пусть $y_1, y_2$ --- линейно независимые решения (2), $t_1$ и $t_2$ --- последовательные нули $y_1(t)$.
Тогда существует единственная точка $t \in (t_1, t_2)$, такая что $y_2(t) = 0$.
Иными словами, их нули чередуются (если два нуля совпали, то решения линейно зависимы).

\textbf{Доказательство.} Положим $Q(t) = q(t)$, тогда по теореме Штурма существует $t \in [t_1, t_2]$, такое что $y_2(t) = 0$.
Если $t = t_1$, то определитель Вронского
\[
    \omega(t) = 
    \begin{vmatrix}
        y_1(t) & y_2(t) \\
        y_1'(t) & y_2'(t)
    \end{vmatrix}
\]
равен нулю, так как верхняя строчка равна нулю, что противоречит линейной независимости $y_1$ и $y_2$.
При $t = t_2$ то же самое, поэтому все нули лежат в интервале.
Докажем единственность.
Пусть существуют $\tau_1, \tau_2 \in (t_1, t_2)$, такие что $\tau_1 < \tau_2$ и $y_2(\tau_1) = y_2(\tau_2) = 0$.
Тогда по теореме Штурма существует нуль решения $y_1$ в интервале $(\tau_1, \tau_2)$ --- противоречие.

\QED

\textbf{Следствие 3.} Пусть существует нетривиальное решение $\widehat y$ уравнения (2), такое что множество его нулей бесконечно.
Тогда для любого решения $y$ этого уравнения множество его нулей тоже бесконечно.
Очевидно из следствия 2: либо линейно зависимы, тогда множества нулей совпадают, либо независимы, тогда бесконечно между нулями $\widehat y$.

\section{Зависимость решения задачи Коши от параметра}
\textbf{Теорема 1.} Пусть $\Omega \subset \mathbb R \times \mathbb R^n$ открыто, $f: \Omega \to \mathbb R^n$ непрерывно, $\frac{\partial f}{\partial x}$ существует и непрерывна, $(t_0, x_0) \in \Omega$ --- произвольные.
Рассмотрим задачу Коши
\[
    \begin{cases}
        x' = f(t, x) \\
        x(t_0) = x_0
    \end{cases} .
\]
Пусть $\phi: I \to \mathbb R^n$ --- её решение, $[\alpha, \beta]$ --- отрезок, такой что $t_0 \in [\alpha, \beta] \subset I$, $\rho > 0$ --- число, такое что $V = \{(t, x): |x - \phi(t)| \le \rho, t \in [\alpha, \beta]\} \subset \Omega$.

Тогда $\forall \varepsilon > 0~\exists \delta > 0: \forall g: \Omega \to \mathbb R^n$ с теми же ограничениями, что и на $f$, таких что $|f - g| \le \delta$ на $V$, а также для любого $y_0 \in \mathbb R^n$, такого что $|y_0 - x_0| \le \delta$, решение $y(\cdot)$ задачи Коши
\[
    \begin{cases}
        y' = g(t, y) \\
        y(t_0) = y_0
    \end{cases}
\]
существует на $[\alpha, \beta]$ и $|\phi(t) - y(t)| < \varepsilon$ на $[\alpha, \beta]$.

Смысл всего этого текста: пусть у нас есть задача Коши с функцией $f$ и начальным условием $(t_0, x_0)$.
Если мы взяли функцию $g$, близкую к $f$, и начальное условие $(t_0, y_0)$, близкое к $(t_0, x_0)$, то и решения двух задач будут близки друг к другу.

\textbf{Доказательство.} Так как $V$ является компактом, на нём функция $f$ ограничена числом $m$ и липшицева с константой $k$.
Пусть $J \subset [\alpha, \beta]$ --- отрезок, такой что $t_0 \in J$ и при $t \in J$ $(t, y(t)) \in V$.
Тогда
\[
    |y'(t) - \phi'(t)| = |g(t, y(t)) - f(t, \phi(t))| \le
\]
\[
    \le |g(t, y(t)) - f(t, y(t))| + |f(t, y(t)) - f(t, \phi(t))| \le
\]
\[
    \le \delta +k|\phi(t) - y(t)|, \forall t \in J.
\]
Положим $r = \beta - \alpha$, тогда по лемме о дифференциальном неравенстве $|\phi(t) - y(t)| \le \delta e^{kr} + \frac{\delta}{k} (e^{kr} - 1)$.
Зафиксируем $\varepsilon > 0$ (будем считать, что $\varepsilon < \rho$).
Выберем $\delta$ так, чтобы правая часть неравенства была меньше $\varepsilon$.
По теореме о продолжении решения решение $y(\cdot)$ определено на всём отрезке $[\alpha, \beta]$, так как мы сделали разность с $\phi(\cdot)$ достаточно малой, то есть продолжение $y(\cdot)$ не может дойти до другой границы $V$, ибо в этом случае $|y - \phi| \ge \rho \ge \varepsilon$.
Оценка на модуль разности решений теперь следует из выбора $\delta$.

\QED

\textbf{Теорема 2.} Пусть $\Omega \subset \mathbb R \times \mathbb R^n$ открыто, $M \subset \mathbb R^m$, $M \ne \varnothing$, $f: \Omega \times M \to \mathbb R^n$ непрерывно, $a: M \to \mathbb R^n$ непрерывно, $\frac{\partial f}{\partial x}$ существует и непрерывна, $\phi(\cdot, \mu)$ --- непродолжаемое решение задачи Коши
\[
    \begin{cases}
        x' = f(t, x, \mu) \\
        x(t_0) = a(\mu)
    \end{cases},
\]
$D := \{(t, \mu): \text{$\phi(t, \mu)$ определено}\}$.
Тогда отображние $\phi(\cdot, \cdot)$ непрерывно, а множество $D$ открыто.

\textbf{Доказательство.} Зафиксируем $(\widehat t, \widehat \mu) \in D$.
Возьмём отрезок $[\alpha, \beta]$, такой что $t_0 \in (\alpha, \beta)$ и $\widehat t \in (\alpha, \beta)$.
Зафиксируем $\rho > 0$, такое что
\[
    V := \{(t, x): t \in [\alpha, \beta], |\phi(t, \widehat \mu) - x| \le \rho\} \subset \Omega.
\]
Тогда по теореме 1 $\forall \varepsilon > 0~\exists \delta(\varepsilon) > 0$.
Пусть $O(\widehat \mu) \subset M$ --- такая окрестность $\widehat \mu$, что
\[
    \begin{cases}
        |f(t, x, \mu) - f(t, x, \widehat \mu)| \le \delta, \forall (t, x) \in V \\
        |a(\mu) - a(\widehat \mu)| \le \delta
    \end{cases}.
\]
Почему так можно выбрать: второе условие очевидно в силу непрерывности.
Докажем первое, от противного: для любого $j \in \mathbb N$ найдётся $(t_j, x_j, \mu_j)$, такие что $(t_j, x_j) \in V$, $|\mu_j - \widehat \mu| < \frac{1}{j}$, но при этом $|f(t_j, x_j, \mu_j) - f(t_j, x_j, \widehat \mu)| > \delta$.
Заметим, что $(t_j, x_j)$ ограничена, так как $V$ является компактом, поэтому по теореме Больцано-Вейерштрасса мы можем перейти к сходящейся подпоследовательности.
Более того, опять же в силу компактности $V$, её предел $(t, x)$ будет лежать в $V$.
Вместе с тем, что $\mu_j \to \widehat \mu$, получаем в пределе, что $|f(t, x, \widehat \mu) - f(t, x, \widehat \mu)| \ge \delta$ --- противоречие.

Тогда по теореме 1 для всех $\mu \in O(\widehat \mu)$ имеем, что $\phi(\cdot, \mu)$ определено на $[\alpha, \beta]$ и $|\phi(t, \mu) - \phi(t, \widehat \mu)| < \varepsilon$ на $[\alpha, \beta]$.
Иными словами, $\phi$ определено на $(\alpha, \beta) \times O(\widehat \mu)$, что даёт открытость $D$.
Так как при фиксированном $\mu$ функция $\phi(\cdot, \mu)$ является решением задачи Коши, она непрерывна по $t$.
Следовательно, существует окрестность $T \ni \widehat t$, такая что $|\phi(t, \widehat \mu) - \phi(\widehat t, \widehat \mu)| < \varepsilon$.
Тогда для всех $(t, \mu) \in T \times O(\widehat \mu)$ имеем
\[
    |\phi(t, \mu) - \phi(\widehat t, \widehat \mu)| \le |\phi(t, \mu) - \phi(t, \widehat \mu)| + |\phi(t, \mu) - \phi(\widehat t, \widehat \mu)| < \varepsilon + \varepsilon.
\]
Таким образом, $\phi(\cdot, \cdot)$ непрерывна.

\QED

\textbf{Теорема 3.} (б/д) Дополнительно к условиям теоремы 2 предположим, что $M \subset \mathbb R$ открыто, $\frac{\partial f}{\partial \mu}$ и $a'$ существуют и непрерывны.
Тогда $\phi$ непрерывно дифференцируема, существуют и непрерывны производные $\frac{\partial^2 \phi}{\partial t \partial \mu}$, $\frac{\partial^2 \phi}{\partial \mu \partial t}$, и $\frac{\partial^2 \phi}{\partial \mu \partial t} \equiv \frac{\partial^2 \phi}{\partial t \partial \mu}$.
Более того, $\frac{\partial \phi}{\partial \mu}(t, \widehat \mu)$ является решением задачи Коши
\[
    \begin{cases}
        v' = \frac{\partial f}{\partial x} (t, \phi(t, \widehat \mu))v + \frac{\partial f}{\partial \mu} (t, \phi(t, \widehat \mu), \widehat \mu) \\
        v(t_0) = a'(\widehat \mu)
    \end{cases}.
\]

\section{Линейные уравнения, уравнение Эйлера}
Эта тема должна была быть раньше, но Жуковский забыл про неё.

Пусть $a_0, a_1, \dots, a_n \in \mathbb R$, $b: \mathbb R \to \mathbb R$ непрерывна.
Рассмотрим \textit{уравнение Эйлера}
\[
    a_0t^n x^{(n)} + \dots + a_{n-1}t x' + a_nx = b(t).
\]

Правильной заменой его можно свести к уравнению с постоянными коэффициентами.
Решим его при $t > 0$, сделав замену $t = e^s$, $y(s) = x(e^s)$.
Получим $y' = x' e^s$, $y'' = x'' e^{2s} + x' + e^s = x'' e^{2s} + y'$ и так далее.
Теперь можно выразить $x' = e^{-s} y'$, $x'' = (y'' - y)e^{-2s}$ и так далее.
Можно доказать, что $k$-ая производная --- это линейная комбинация производных $y$, умноженная на $e^{-ks}$.

Дальше для $n = 2$: $a_0t^2 x'' + a_1 t x' + a_2 x = b(t)$.
После подстановки получим
\[
    a_0 e^{2s}(y'' - y') e^{-2s} + a_1 e^s y' e^{-s} + a_2y = b(e^s).
\]
Потом нужно отдельно решить для $t < 0$, после чего не гарантируется, что решения получится склеить, например, если $x(t) = \frac{1}{t}$: решение не может содержать точку $t = 0$.

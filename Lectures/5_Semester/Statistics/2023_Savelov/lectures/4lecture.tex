\begin{lemma} (О наследовании асимптотической нормальности)
	Если $\wh{\theta}_n$ --- асимптотически нормальная оценка $\theta \in \Theta \subseteq \R$ с асимптотической дисперсией $\sigma^2(\theta)$, а также $\tau \colon \R \to \R$ --- дифференцируемая на $\Theta$ функция, тогда $\tau(\wh{\theta}_n)$ --- асимптотически нормальная оценка $\tau(\theta)$ с асимптотической дисперсией $\sigma^2(\theta)(\tau'(\theta))^2$
\end{lemma}

\begin{proof}
	Зафиксируем $\theta \in \Theta$. Из условия мы знаем, что
	\[
		\xi_n := \sqrt{n}(\wh{\theta}_n - \theta) \xrightarrow[n \to \infty]{d_\theta} \xi \sim N(0, \sigma^2(\theta))
	\]
	Воспользуемся дельта-методом для $\tau$. Тогда:
	\[
		\frac{\tau(\theta + \xi_nb_n) - \tau(\theta)}{b_n} \xrightarrow[n \to \infty]{d_\theta} \tau'(\theta) \cdot \xi \sim N(0, \sigma^2(\theta)(\tau'(\theta))^2)
	\]
	Несложно понять, что нам нужно взять $b_n = \frac{1}{\sqrt{n}}$, чтобы метод дал нужную сходимость.
\end{proof}

\begin{proposition} (Многомерный случай леммы о наследовании асимптотической нормальности)
	Если $\wh{\theta}_n$ --- асимптотически нормальная оценка $\theta \in \Theta \subseteq \R^k$ с асимптотической матрицей ковариаций $\Sigma(\theta)$, а также $\tau \colon \R^k \to \R^s$ --- дифференцируемая на $\Theta$ функция, тогда $\tau(\wh{\theta}_n)$ --- асимптотически нормальная оценка $\tau(\theta)$ с асимптотической матрицей ковариаций $J_\tau \Sigma J_\tau^T$, где $J_\tau = J_\tau(\theta)$ --- матрица Якоби для $\tau$
\end{proposition}

\begin{proof}
	\textcolor{red}{Аналогично одномерному случаю, просто применяем многомерный дельта-метод}
\end{proof}

\subsection{Методы нахождения оценок}

\begin{note}
	Далее, как обычно, $(\cX, \B(\cX), \cP)$ --- параметрическая модель, причём $\cP = \{P_\theta \colon \theta \in \Theta\}$.
\end{note}

\subsubsection*{Метод подстановки (ОМП)}

\begin{definition}
	Пусть $G$ --- функционал из множества мер над $(\cX, \B(\cX))$ в множество параметров $\Theta$, а также выполнено условие:
	\[
		\forall \theta \in \Theta\ \ \theta = G(P_\theta)
	\]
	Тогда, \textit{оценкой по методу подстановки (ОМП)} называется оценка $\theta_n^*(X) = G(P_n^*)$, где $P_n^*$ --- эмпирическое распределение, построенное по выборке $\{X_k\}_{k = 1}^n$
\end{definition}

\begin{example}
	Рассмотрим $\cP = \{Bern(\theta)\}$, $G(P) = \int_\R xdP(x)$. В силу определения распределения Бернулли, $\theta = G(P_\theta)$. При этом для эмпирического распределения имеем такую формулу:
	\[
		G(P_n^*) = \int_\R xdP_n^*(x) = \frac{\sum_{k = 1}^n X_k}{n} = \ol{X}
	\]
\end{example}

\begin{anote}
	Говоря простым языком, метод ОМП заключается в нахождении несмещённого функционала для $P_\theta$, а затем мы просто пихаем в него эмпирическое распределение $P_n^*$.
\end{anote}

\subsubsection*{Метод моментов (ОММ)}

\begin{note}
	Пусть $\cX = \R^m$, $\Theta \subseteq \R^k$. Рассмотрим выборку $\{X_i\}_{i = 1}^n$ из распределения $P \in \cP$, а также борелевские функции $\forall i \in \range{1}{k}\ g_i \colon \R^m \to \R$ (они ещё называются \textit{пробными функциями}). Тогда далее мы принимаем следующие обозначения:
	\begin{itemize}
		\item $m_i(\theta) := \E_\theta g_i(X_1)$
		
		\item $m(\theta) := (m_1(\theta), \ldots, m_k(\theta))^T$
		
		\item $\ol{g}(X) := (\ol{g_1(X)}, \ldots \ol{g_k(X)})^T$
	\end{itemize}
	При этом мы требуем, что $m_i(\theta)$ конечны.
\end{note}

\begin{definition}
	Если существует и единственно решение системы уравнений относительно $\theta$:
	\[
		\System{
			&{m_1(\theta) = \ol{g_1(X)}}
			\\
			&{\vdots}
			\\
			&{m_k(\theta) = \ol{g_k(X)}}
		}
		\Lra m(\theta) = \ol{g}(X)
	\]
	то значение $\theta^* = m^{-1}(\ol{g}(X))$ (где $m^{-1}$ понимается покоординатно) называется \textit{оценкой по методу моментов (ОММ)}
\end{definition}

\begin{anote}
	Метод довольно интуитивен после того, как мы обсудили соотношение между $m_i(\theta)$ и $\ol{g_i(X)}$ выше.
\end{anote}

\begin{definition}
	\textit{Стандартными пробными функциями} называются $g_i(x) = x^i$.
\end{definition}

\begin{theorem} (о сильно состоятельности ОММ)
	Пусть выполнены следующие условия на $m \colon \Theta \to m(\Theta)$:
	\begin{itemize}
		\item $m$ --- биекция
		
		\item $m^{-1}$ можно доопределить до функции, заданной на $\R^k$
		
		\item Доопределённая $m^{-1}$ непрерывна в каждой точке $m(\Theta)$
	\end{itemize}
	Тогда оценка по методу моментов $\theta_n^*$ является сильно состоятельной оценкой параметра $\theta$.
\end{theorem}

\begin{proof}
	Зафиксируем $\theta \in \Theta$. По УЗБЧ мы имеем сходимость $\ol{g}(X) \to^{P_\theta\text{ п.н.}} m(\theta)$, а из этого и условия теоремы, по теореме о наследовании сходимости, можем получить такую сходимость:
	\[
		\theta_n^* = m^{-1}(\ol{g}(X)) \xrightarrow{P_\theta\text{ п.н.}} m^{-1}(m(\theta)) = \theta
	\]
\end{proof}

\begin{note}
	Несколько поясним, за что отвечает каждое требование к $m$ в теореме:
	\begin{itemize}
		\item Биективность позволяет заявить, что $m^{-1}(m(\theta)) = \theta$
		
		\item Доопределение нужно из-за $m^{-1}(\ol{g}(X))$
		
		\item Непрерывность на $m(\Theta)$ требуется, чтобы мы могли воспользоваться теоремой о наследовании сходимости
	\end{itemize}
\end{note}

\begin{theorem} (о асимптотической нормальности ОММ)
	Пусть выполнены следующие условия на $m \colon \Theta \to m(\Theta)$:
	\begin{itemize}
		\item $m$ --- биекция
		
		\item $m^{-1}$ можно доопределить до функции, заданной на $\R^k$
		
		\item Доопределённая $m^{-1}$ дифференцируема в каждой точке $m(\Theta)$
		
		\item $\forall i \in \range{1}{k}\ \ \E_\theta g_i^2(X_1) < \infty$
	\end{itemize}
	Тогда оценка по методу моментов $\theta_n^*$ является асимптотически нормальной оценкой параметра $\theta$.
\end{theorem}


\begin{proof}
	По Центральной Предельной Теореме:
	\[
		\sqrt{n}(\ol{g}(X) - m(\theta)) \xrightarrow{d_\theta} N(0, \Sigma)
	\]
	где $\Sigma$ --- матрица ковариаций для вектора $\ol{g}(X)$. Осталось применить многомерную лемму о наследовании асимптотической сходимости с функцией $m^{-1}$. Тогда $m^{-1}(\ol{g}(X)) = \theta_n^*$ является асимптотически нормальной оценкой $m^{-1}(m(\theta)) = \theta$ с матрицей ковариаций $J_m^{-1}\Sigma J_m^{-T}$.
\end{proof}

\begin{note}
	Метод моментов на самом деле является частным случаем метода подстановки, ибо посмотрим на решение системы и реальный вектор $\theta$ в развёрнутом виде:
	\[
		\theta^* = m^{-1} \begin{pmatrix}
			\int_\cX g_1(x)dP_n^*(x)
			\\
			\vdots
			\\
			\int_\cX g_k(x)dP_n^*(x)
		\end{pmatrix}
		=: G(P_n^*); \quad \theta = m^{-1} \begin{pmatrix}
			\int_\cX g_1(x)dP_\theta(x)
			\\
			\vdots
			\\
			\int_\cX g_k(x)dP_\theta(x)
		\end{pmatrix}
		= G(P_\theta)
	\]
\end{note}

\subsubsection*{Метод выборочных квантилей}

\begin{note}
	Здесь мы считаем, что $\cX = \R$
\end{note}

\begin{definition}
	Пусть $P$ --- распределение на $\R$, $F$ --- соответствующая функция распределения и $p \in (0; 1)$. Тогда \textit{p-квантилью распределения $P$} называется следующая точка $z_p$:
	\[
		z_p := \inf \{x \colon F(x) \ge p\}
	\]
\end{definition}

\begin{note}
	В силу правой непрерывности функции распределения $F$ инфинум достигается. При этом $F^{-1}(p)$ совершенно не обязательно всегда подходит:
	\begin{itemize}
		\item Если $F$ непрерывна, то существует точно решение уравнения $F(z_p) = p$, но оно не единственно в силу нестрогого возрастания
		
		\item Если $F$ строго монотонна, то у нас есть гарантия на единственность решения $F(z_p) = p$, однако оно не обязательно существует (инфинум-то найдём, но вот в случае разрыва просто может не быть точки, в которой $F(x) = p$)
	\end{itemize}
\end{note}

\begin{definition}
	Пусть $\{X_i\}_{i = 1}^n$ --- выборка. Статистика $z_{n, p} = X_{(\ceil{np})}$ называется \textit{выборочной $p$-квантилью}
\end{definition}

\begin{note}
	Несложно понять, что выборочный $p$-квантиль --- это просто $p$-квантиль для эмпирического распределения $P_n^*$
\end{note}

\begin{theorem} (О выборочных квантилях)
	Пусть $\{X_i\}_{i = 1}^n$ --- выборка из распределения $P$ с плотностью $f(x)$. Пусть $z_p$ --- это $p$-квантиль этого распределения, причём $f$ непрерывно дифференцируема в некоторой окрестности $z_p$ и $f(z_p) > 0$. Тогда имеет место сходимость:
	\[
		\sqrt{n}(z_{n, p} - z_p) \xrightarrow{d} N\ps{0, \frac{p(1 - p)}{f^2(z_p)}}
	\]
\end{theorem}

\begin{reminder}
	Пусть $\{X_i\}_{i = 1}^n$ --- выборка из распределения $P$ с плотностью $f(x)$, а $F$ --- функция распределения $P$, то верны следующие факты про порядковые статистики:
	\begin{itemize}
		\item $P(X_{(k)} \le x) = \sum_{m = k}^n C_n^m F^m(x)(1 - F(x))^{n - m}$
		
		\item $p_{X_{(k)}} = n C_{n - 1}^{k - 1} F^{k - 1}(x)(1 - F(x))^{n - k}f(x)$
	\end{itemize}
\end{reminder}

\begin{proof}
	Сходимость по распределению эквивалентна тому, что функции распределения сходятся во всех точках непрерывности своего предела. Мы пронормируем доказываемую сходимость так, чтобы при доказательстве получить справа $N(0, 1)$ (к результату теоремы же вернёмся просто при помощи теоремы о наследовании):
	\[
		\eta_n = \frac{\sqrt{n}(z_{n, p} - z_p)}{\sqrt{\frac{p(1 - p)}{f^2(z_p)}}}
	\]
	Итак, обозначим $k = \ceil{np}$. Идея состоит в том, чтобы найти плотность $p_{\eta_n}$ и показать, что она сходится к плотности нормального распределения, а дальше останется показать сходимость интегралов по этим плотностям.
	\begin{enumerate}
		\item (Разбираемся с плотностью) Запишем $\eta_n$ в более удобном виде:
		\[
			\eta_n = (z_{n, p} - z_n)\sqrt{\frac{nf^2(z_p)}{p(1 - p)}}
		\]
		Заметим, что $\eta_n$ является линейной комбинацией от $X_{(k)}$, чью плотность мы знаем. Стало быть, если $t_n(x) = z_p + \frac{x}{f(z_p)}\sqrt{\frac{p(1 - p)}{n}}$ (функция, переводящая $x$ из $F_\eta(x)$ в соответствующий параметр $F_{X_{(k)}}$), то имеет место равенство:
		\[
			p_{\eta_n}(x) = \sqrt{\frac{p(1 - p)}{nf^2(z_p)}} \cdot p_{X_{(k)}}(t_n(x))
		\]
		Разобьём $p_{\eta_n}$ на три части (теперь считаем, что $x$ фиксирован):
		\begin{itemize}
			\item $p_{\eta_n}(x) = A_1(n) \cdot A_2(n) \cdot A_3(n)$
			
			\item $A_1(n) = \sqrt{npq}C_{n - 1}^{k - 1}p^{k - 1}q^{n - k}$, где $q = 1 - p$
			
			\item $A_2(n) = \frac{f(t_n(x))}{f(z_p)}$
			
			\item $A_3(n) = \ps{\frac{F(t_n)}{p}}^{k - 1}\ps{\frac{1 - F(t_n)}{q}}^{n - k}$
		\end{itemize}
		Покажем, что при стремлении $n \to \infty$ каждая из этих частей даст нам сомножитель из плотности $N(0, 1)$:
		\begin{itemize}
			\item $A_1(n) \to \frac{1}{\sqrt{2\pi}}$ --- просто применение формулы Стирлинга
			
			\item $A_2(n) \to 1$. Действительно, ведь $f$ непрерывна, а $\lim_{n \to \infty} t_n(x) = z_p$
			
			\item $A_3(n) \to e^{-x^2 / 2}$. Для получения этой сходимости, мы рассмотрим отдельно каждый сомножитель и получим какие-то формы для логарифмов от них. Так как это делается аналогично, то опишем только первый. Итак, $F(z_p) = p$, $\lim_{n \to \infty} t_n(x) = z_p$ и $F$ как минимум дважды гладкая. Напишем формулу Тейлора до второй производной в точке $z_p$:
			\[
				F(t_n) = F(z_p) + (t_n - z_p) \cdot F'(z_p) + \frac{1}{2}(t_n - z_p)^2 \cdot F''(z_p) + o\big((t_n - z_p)^2\big),\ t_n \to z_p
			\]
			Её можно расписать так:
			\[
				F(t_n) = p + x\sqrt{\frac{p(1 - p)}{n}} + \frac{1}{2} \cdot \frac{x^2pq}{n} \cdot \frac{f'(z_p)}{f^2(z_p)} + o\ps{\frac{1}{n}},\ n \to \infty
			\]
			Осталось поделить на $p$ разложить логарифм (пользуемся методом $\ln x = \ln(1 + (x - 1))$):
			\[
				\ln \frac{F(t_n)}{p} = x\sqrt{\frac{q}{pn}} + \frac{1}{2} \cdot x^2\frac{q}{n} \cdot \frac{f'(z_p)}{f^2(z_p)} + o\ps{\frac{1}{n}} - \frac{x^2}{2} \cdot \frac{q}{pn}
			\]
			То же самое проделывается и для $\ln \frac{1 - F(t_n)}{q}$. В итоге:
			\[
				\ln A_3(n) = (k - 1)\ln \frac{F(t_n)}{p} + (n - k)\ln \frac{1 - F(t_n)}{q} \xrightarrow[n \to \infty]{} -\frac{x^2}{2}
			\]
		\end{itemize}
		Таким образом, $p_{\eta_n}(x) \to \frac{1}{\sqrt{2\pi}} e^{-x^2 / 2}$. Более того, если посмотреть ход наших рассуждений, то тривиально оказывается, что мы получили равномерную сходимость на любом отрезке $[-N; N]$.
		
		\item Сходимость по распределению эквивалентна сходимости функций распределения в основном:
		\[
			\forall x \in C(F)\quad F_n(x) = \int_{\rsi{-\infty; x}} p_{\eta_n}(x)d\mu(x) \xrightarrow[n \to \infty]{} \int_{\rsi{-\infty; x}} p(x)d\mu(x)
		\]
		Зафиксируем $x \in C(F)$ и посмотрим модуль разности интегралов:
		\[
			\md{\int_{\rsi{-\infty; x}} p(x)d\mu(x) - \int_{\rsi{-\infty; x}} p_{\eta_n}(x)d\mu(x)} = \md{\int_{\rsi{-\infty; x}} (p(x) - p_{\eta_n}(x))d\mu(x)}
		\]
		Для фиксированного $\eps > 0$ выберем такой компакт $[-N; N]$, что интегралы по $\R \bs [-N; N]$ не больше $\eps$. Тогда, либо $x < -N$ и всё тривиально, либо мы можем свести оценку модуля выше к исследованию интеграла на компакте, а там мы имеем равномерную оценку на подыинтегральную функцию и, следовательно, можем избавиться от интеграла вообще:
		\begin{multline*}
			\md{\int_{\rsi{-\infty; x}} (p(x) - p_{\eta_n}(x))d\mu(x)} \le
			\\
			\md{\int_{\R \bs [-N; N]} (p(x) - p_{\eta_n}(x))d\mu(x)} + \md{\int_{[-N; x]} (p(x) - p_{\eta_n}(x))d\mu(x)} \le
			\\
			2\eps + \int_{[-N; x]} |p(x) - p_{\eta_n}(x)|d\mu(x)
		\end{multline*}
		В силу равномерной оценки, можем найти такой номер $n_0$, что при $n \ge n_0$ верна оценка $|p(x) - p_{\eta_n}(x)| \le \frac{\eps}{x - (-N)}$. Тогда приходим к нужному результату:
		\[
			\md{F(x) - F_{\eta_n}(x)} \le 2\eps + \int_{[-N; x]} |p(x) - p_{\eta_n}(x)|d\mu(x) \le 2\eps + (x + N) \cdot \frac{\eps}{x + N} = 3\eps
		\]
	\end{enumerate}
\end{proof}
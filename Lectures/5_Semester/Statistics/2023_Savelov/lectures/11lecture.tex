\begin{theorem} (об ортогональном разложении, без доказательства)
	Пусть $X_1, \ldots, X_n \sim N(l, \sigma^2 E_n)$. Также $L_1 \oplus \ldots \oplus L_r = \R^n$ --- есть разложение через попарно ортогональные пространства. Обозначим $Y_i = \Pi_{L_i} X$. Тогда $Y_1, \ldots, Y_r$ --- независимые в совокупности гауссовские вектора, причём $\E Y_i = \Pi_{L_i} l$ и $\frac{1}{\sigma^2}(Y_i - \E Y_i)^2 \sim \chi_{\dim L_i}^2$
\end{theorem}

\begin{proposition}
	Статистика $S(X) = (\Pi_L X, \|\Pi_{L^\bot} X\|^2)$ является достаточной статистикой для $(l, \sigma^2)$.
\end{proposition}

\begin{proof}
	Воспользуемся теоремой о факторизации. Распишем плотность $X$ по формуле для многомерного нормального распределения:
	\[
		p(x) = \ps{\frac{1}{\sigma\sqrt{2\pi}}}^n\exp\ps{-\frac{1}{2\sigma^2}\sum_{i = 1}^n (x_i - l_i)^2}
	\]
	В силу теоремы Пифагора:
	\[
		\sum_{i = 1}^n (x_i - l_i)^2 = \|x - l\|^2 = \|\Pi_L (x - l)\|^2 + \|\Pi_{L^\bot} (x - l)\|^2 = \|\Pi_L x - l\|^2 + \|\Pi_{L^\bot} x\|^2
 	\]
 	Стало быть:
	\[
	 	p(x) = \ps{\frac{1}{\sigma\sqrt{2\pi}}}\exp\ps{-\frac{1}{2\sigma^2}\big(\|\Pi_L x - l\|^2 + \|\Pi_{L^\bot} x\|^2\big)}
	\]
	Значит, $S(X)$ действительно достаточная статистика.
\end{proof}

\begin{theorem} (без доказательства)
	Статистика $S(X) = (\Pi_L X, \|\Pi_{L^\bot} X\|^2)$ является полной.
\end{theorem}

\begin{corollary}
	\begin{itemize}
		\item $\wh{\theta}(X)$ --- оптимальная оценка для $\theta$
		
		\item $Z\wh{\theta}(X)$ --- оптимальная оценка для $l$
		
		\item $\frac{1}{n - k}\|X - Z\wh{\theta}(X)\|^2$ --- оптимальная оценка для $\sigma^2$
	\end{itemize}
\end{corollary}

\begin{proof}
	Достаточно доказать, что все вышеперечисленные оценки являются функциями от $S(X)$, которая уже является полной достаточной статистикой.
	\begin{itemize}
		\item Сразу имеем $Z\wh{\theta}(X) = \Pi_L X$
		
		\item Выразим $\wh{\theta}(X)$ из верхнего равенства:
		\[
			Z^TZ\wh{\theta}(X) = Z^T\Pi_L X \Ra \wh{\theta}(X) = (Z^TZ)^{-1}Z^T\Pi_L X
		\]
		
		\item \(\frac{1}{n - k}\|X - Z\wh{\theta}(X)\|^2 = \frac{1}{n - k} \|X - \Pi_L X\|^2 = \frac{1}{n - k}\|\Pi_{L^\bot} X\|^2\)
	\end{itemize}
\end{proof}

\begin{proposition}
	Статистики $\wh{\theta}(X)$ и $X - Z\wh{\theta}(X)$ независимы. Более того:
	\begin{itemize}
		\item $\frac{1}{\sigma^2}\|Z\wh{\theta}(X) - Z\theta\|^2 \sim \chi_k^2$
		
		\item $\frac{1}{\sigma^2}\|X - Z\wh{\theta}(X)\|^2 \sim \chi_{n - k}^2$
		
		\item $\wh{\theta}(X) \sim N(\theta, \sigma^2(Z^TZ)^{-1})$
	\end{itemize}
\end{proposition}

\begin{proof}
	Согласно теореме об ортогональном разложении, гауссовские векторы $Z\wh{\theta}(X) = \Pi_L X$ и $X - Z\wh{\theta}(X) = \Pi_{L^\bot} X$ являются независимыми, причём выполнены первые 2 пункта из утверждения. Для последнего заметим, что
	\[
		\wh{\theta}(X) = (Z^TZ)^{-1}Z^TZ\wh{\theta}(X) \text{ --- линейная функция от $Z\wh{\theta}(X)$}
	\]
	Стало быть, $\wh{\theta}(X) \indep X - Z\wh{\theta}(X)$, независимость установлена. Так как $\wh{\theta}(X)$ является линейной функцией от гауссовского вектора, то это тоже гауссовский вектор. Параметры мы уже находили ранее.
\end{proof}

\begin{definition}
	Пусть $\xi \sim N(0, 1)$, $\eta \sim \chi_k^2$ и $\xi \indep \eta$. Тогда случайная величина $\zeta = \frac{\xi}{\sqrt{\eta / k}}$ обладает \textit{распределением Стьюдента с $k$ степенями свободы}. Обозначается как $\zeta \sim T_k$
\end{definition}

\begin{proposition}
	У распределения Стьюдента есть несколько базовых свойств:
	\begin{enumerate}
		\item Если $\zeta \sim T_k$, то и $-\zeta \sim T_k$
		
		\item $T_1 \sim Cauchy(0, 1)$. Плотность этого распределения равна $p(x) = \frac{1}{\pi(1 + x^2)}$
		
		\item Если $\zeta_k \sim T_k$, то $\zeta_k \xrightarrow{d} N(0, 1)$
	\end{enumerate}
\end{proposition}

\begin{definition}
	Пусть $\xi \sim \chi_k^2$, $\eta \sim \chi_m^2$ и $\xi \indep \eta$. Тогда случайная величина $\zeta = \frac{\xi / k}{\eta / m}$ обладает \textit{распределением Фишера с параметрами $k, m$}. Обозначается как $\zeta \sim F_{k, m}$
\end{definition}

\begin{proposition}
	Распределение Фишера обладает следующими свойствами:
	\begin{enumerate}
		\item Если $\xi \sim T_m$, то $\xi^2 \sim F_{1, m}$
		
		\item Если $\xi \sim F_{k, m}$, то $\frac{1}{\xi} \sim F_{m, k}$
		
		\item Пусть $k$ фиксировано, $\xi_m \sim F_{k, m}$. Тогда $k\xi_m \xrightarrow{d} \chi_k^2$
		
		\item Пусть $\xi_{k, m} \sim F_{k, m}$. Тогда $\xi_{k, m} \xrightarrow[k, m \to \infty]{d} 1$
	\end{enumerate}
\end{proposition}

\subsubsection{Доверительные интервалы в гауссовской линейной модели}

Далее $\gamma$ --- уровень доверия.

\begin{itemize}
	\item Для $\sigma^2$: у нас есть статистика $\frac{1}{\sigma^2}\|X - Z\wh{\theta}(X)\|^2 \sim \chi_{n - k}^2$. Возьмём $u_{1 - \gamma}$ --- соответствующий квантиль $\chi_{n - k}^2$. Тогда
	\[
		\gamma = P(\frac{1}{\sigma^2}\|X - Z\wh{\theta}(X)\|^2 > u_{1 - \gamma}) = P\ps{\sigma^2 \in \Big(0; \frac{\|X - Z\wh{\theta}(X)\|^2}{u_{1 - \gamma}}\Big)}
	\]
	
	\item Для $\theta_i$: воспользуемся тем фактом, что $\wh{\theta}(X) \sim N(\theta, \sigma^2A)$, где $A = (Z^TZ)^{-1}$. Тогда $\theta_i \sim N(\theta_i, \sigma^2a_{ii})$, а значит $\frac{\wh{\theta}_i - \theta_i}{\sqrt{\sigma^2a_{ii}}} \sim N(0, 1)$. Чтобы убрать $\sigma^2$ из знаменателя, вспомним, что $\wh{\theta}(X) \indep X - Z\wh{\theta}(X)$. Стало быть, можем поделить оценку на корень из $\frac{1}{\sigma^2}\|X - Z\wh{\theta}(X)\|^2 \sim \chi_{n - k}^2$ и получить распределение Стюдента:
	\[
		\sqrt{\frac{n - k}{a_{ii}}} \cdot \frac{\wh{\theta}_i - \theta_i}{\sqrt{\|X - Z\wh{\theta}(X)\|^2}} \sim T_{n - k}
	\]
	Дальше уже понятно, как получить доверительный интервал.
	
	\item Для $\theta$: доверительный интервал тут по определению не получишь, однако мы сможем получить хорошее доверительное множество. Воспользуемся независимыми статистиками $\frac{1}{\sigma^2}\|Z\wh{\theta}(X) - Z\theta\|^2 \sim \chi_k^2$ и $\frac{1}{\sigma^2}\|X - Z\wh{\theta}\|^2 \sim \chi_{n - k}^2$. Их комбинацией можно получить распределение Фишера:
	\[
		\frac{n - k}{k} \cdot \frac{\|Z\wh{\theta}(X) - Z\theta\|^2}{\|X - Z\wh{\theta}(X)\|^2} \sim F_{k, n - k}
	\]
	Множество, которое из этой оценки задаётся как $\{\frac{n - k}{k} \cdot \frac{\|Z\wh{\theta}(X) - Z\theta\|^2}{\|X - Z\wh{\theta}(X)\|^2} < u_\gamma\}$, является доверительным эллипсоидом в $\R^k$.
\end{itemize}

\section{Проверка статистических гипотез}

\begin{note}
	Далее мы живём в вероятностно-статистической модели $(\cX, \B(\cX), \cP)$. Пусть наблюдение $X \sim P \in \cP$.
\end{note}

\begin{definition}
	\textit{Статистической гипотезой} называется предположение вида $P \in \cP_0$, где $\cP_0 \subseteq \cP$ --- подмножество распределений. Обозначается как $H_0 \colon P \in \cP_0$ --- гипотеза $H_0$.
\end{definition}

\begin{note}
	Какую-то зафиксированную, выделенную гипотезу мы будем называть \textit{основной}.
	
	Наша задача состоит в том, чтобы по наблюдению $X$ либо \textit{отвергнуть}, либо \textit{не отвергнуть (<<принять>>)} гипотезу $H_0$. Если гипотеза отвергнута, то мы считаем, что распределение $P \notin \cP_0$, а значит ответ надо искать среди $P \in \cP \bs \cP_0$ (при этом мы не сможем гарантировать, что мы не ошиблись в своих суждениях из-за неудачной реализации наблюдения).
\end{note}

\begin{definition}
	Пусть $H_0 \colon P \in \cP_0$ --- основная гипотеза. Тогда $H_1 \colon P \in \cP_1 \subseteq \cP \bs \cP_0$ называется \textit{альтернативой (альтернативной гипотезой)}.
\end{definition}

\begin{definition}
	Пусть $X$ принимает значения в выборочном множестве $\cX_1$, а $S \subseteq \cX$ --- некоторое подмножество. Если правило принятия гипотезы $H_0$ выглядит следующим образом:
	\[
		H_0 \text{ отвергается} \Lra X \in S
	\]
	То $S$ называется \textit{критическим множеством, или же критерием для проверки гипотезы $H_0$ (против альтернативы $H_1$, если она есть)}.
\end{definition}

\begin{definition}
	\textit{Ошибкой первого рода} называется ситуация, когда $H_0$ отвергли, но при этом гипотеза верна.
\end{definition}

\begin{definition}
	\textit{Ошибкой второго рода} называется ситуация, когда $H_0$ не отвергли, но при этом гипотеза неверна.
\end{definition}

\begin{anote}
	В первую очередь стремятся минимизировать ошибку первого рода, а потом вторую. Причину можно проиллюстрировать на жизненном примере: если гипотеза $H_0$ состоит в том, что человек болен раком, то мы бы сильно хотели, чтобы у него не было заболевания при отвержении гипотезы, а если мы его будем лечить от болезни, которой у него нет, то это не так страшно.
\end{anote}

\begin{definition}
	Пусть $S$ --- критерий для проверки гипотезы $H_0 \colon P \in \cP_0$. Функция $\beta(Q, S) = Q(X \in S)$, где $Q \in \cP$, называется \textit{функцией мощности критерия $S$}.
\end{definition}

\begin{definition}
	Если при некотором $\eps > 0$ для критерия $S$ выполнено неравенство
	\[
		\forall Q \in \cP_0\ \ \beta(Q, S) \le \eps\ \ (\text{это эквивалентно } \sup_{Q \in \cP_0} \beta(Q, S) \le \eps)
	\]
	то говорят, что \textit{критерий $S$ имеет уровень значимости $\eps$}.
\end{definition}

\begin{note}
	Уровень значимости критерия даёт верхнюю оценку на вероятность ошибки первого рода.
\end{note}

\begin{anote}
	Важно не путать уровень \textit{значимости} и уровень \textit{доверия}. Сумма этих величин даёт единицу.
\end{anote}

\begin{definition}
	Минимальный уровень значимости $\alpha(S) = \sup_{Q \in \cP_0} \beta(Q, S)$ называется \textit{размером критерия $S$}
\end{definition}

\begin{note}
	Далее $S$ --- это критерий проверки гипотезы $H_0$ против альтернативы $H_1$.
\end{note}

\begin{definition}
	Критерий $S$ называется \textit{несмещённым}, если выполнено условие (несмещённости):
	\[
		\sup_{Q \in \cP_0} \beta(Q, S) \le \inf_{Q \in \cP_1} \beta(Q, S)
	\]
\end{definition}

\begin{anote}
	Говоря другими словами, критерий несмещён, если вероятность ошибки первого рода меньше, чем вероятность отвержения гипотезы при верности альтернативы.
\end{anote}

\begin{definition}
	Пусть $S_n$ --- последовательность критериев (или просто критерий), отвечающих выборке $X = (X_1, \ldots, X_n)$. Тогда критерий $S_n$ называется \textit{состоятельным}, если
	\[
		\forall Q \in \cP_1\ \ \lim_{n \to \infty} \beta(Q, S_n) = 1
	\]
\end{definition}

\begin{anote}
	Другими словами, критерий состоятелен, если с увеличением выборки (следовательно, информации, которую мы знаем о распределении) вероятность отвергнуть гипотезу при верной альтернативе стремится к единице.
\end{anote}

\begin{definition}
	Критерий $S$ называется более мощным, чем критерий $R$ того же уровня значимости, если выполнено утверждение:
	\[
		\forall Q \in \cP_1\ \ \beta(Q, S) \ge \beta(Q, R)
	\]
	Иными словами, вероятность ошибки второго рода у критерия $S$ равномерно меньше ($1 - \beta(Q, S) \le 1 - \beta(Q, R)$).
\end{definition}

\begin{definition}
	Критерий $S$ называется \textit{равномерно наиболее мощным критерием (РНМК) уровня значимости $\eps$}, если его мощность $\alpha(S) \le \eps$ и $S$ мощнее любого другого критерия $R$ с уровнем значимости $\eps$ (эквивалентно $\alpha(R) \le \eps$).
\end{definition}

\begin{note}
	Хороший вопрос состоит в том, как искать РНМК. Для этого нужно конкретизировать гипотезы, с которыми мы работаем.
\end{note}

\begin{definition}
	Гипотеза $H_0 \colon P = P_0$, где $P_0$ --- известное распределение, называется \textit{простой}.
\end{definition}

\subsection{Проверка простых гипотез}

\begin{note}
	Далее мы полагаем, что у нас есть основная $H_0$ и альтернативная $H_1$ простые гипотезы, причём $P_i$ имеет плотность $p_i(x)$ по одной и той же мере $\mu(x)$.
	
	Также мы определим критерий $S_\lambda$ следующим образом:
	\[
		S_\lambda := \{x \colon p_1(x) - \lambda p_0(x) \ge 0\},\ \lambda \ge 0
	\]
\end{note}

\begin{anote}
	Если описывать словами критерий $S_\lambda$, то мы отвергаем основную гипотезу, если вероятность реализации выборки при $H_1$ в $\lambda$ раз больше, чем та же вероятность при $H_0$.
\end{anote}


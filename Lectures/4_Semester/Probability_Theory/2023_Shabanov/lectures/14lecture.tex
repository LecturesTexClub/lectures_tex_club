\newcommand\bigzero{\makebox(0,0){\text{\Huge0}}}

\begin{note}
    Изучив некоторые инструменты работы со сходимостью случайных векторов, хочется увидеть, где эти сходимости могут возникать. Для сходимости случайных величин знаем две такие теоремы: Закон Больших Чисел и Центральная Предельная Теорема.
\end{note}

\begin{theorem} (Усиленный Закон Больших Чисел в форме Колмогорова для случайных векторов)
    Пусть $\{\xi_n\}_{n = 1}^\infty$ --- независимые одинаково распределённые случайные векторы $\R^m \to \R$. Пусть $\E\xi_1 \neq \infty$. Тогда
    \[
        \frac{\xi_1 + \ldots + \xi_n}{n} \xrightarrow{P\text{ п.н.}} \E\xi_1
    \]
\end{theorem}

\begin{proof}
    Так как случайные векторы $\{\xi_n\}_{n = 1}^\infty$ независимы, то их координаты по каждому индексу $\{\xi_{t, n}\}_{n = 1}^\infty$ тоже независимы, как борелевские функции от соответствующих $\xi_n$. Координаты также наследуют одинаковую распределённость и конечность математического ожидания. Применив для них одномерный УЗБЧ в форме Колмогорова и учитывая то, что сходимость почти наверное случайных векторов эквивалентна соответствующей покоординатной сходимости, получим требуемое.
\end{proof}

\section{Многомерное нормальное распределение}

\begin{note}
    Хотим получить многомерный аналог Центральной Предельной Теоремы. Если переносить формулировку с одномерного случая, то возникнет вопрос: <<А что же такое многомерное нормальное распределение?>>
    
    Определить многомерное нормальное распределение более трудно, чем его одномерный случай. Если определять через плотность, то потеряется существенная часть ситуаций, а функция распределения выглядит не очень приятно уже в размерности один. Оказывается, хорошо определять через характеристическую функцию.
\end{note}

\begin{definition}
    Случайный вектор $\xi = (\xi_1, \ldots, \xi_n)$ называется \textit{гауссовским (или нормальным)}, если его характеристическая функция имеет такой вид:
    \[
        \phi(t) = e^{i \tbr{a, t} - \frac{1}{2} \tbr{\Sigma t, t}}
    \]
    где
    \begin{itemize}
        \item $\tbr{x, y}$ --- скалярное произведение $x$ и $y$ из $\R^n$
        
        \item $a \in \R^n$ --- произвольный вектор
        
        \item $\Sigma$ --- матрица $n \times n$, симметричная и неотрицательно определённая
    \end{itemize}
    Обозначение: $\xi \sim N(a, \Sigma)$.
\end{definition}

\begin{note}
    Сразу возникает вопрос: является ли $\phi(t)$ характеристической функцией (некоторого случайного вектора). И даже если верим, что является, хочется узнать какие-то свойства гауссовского вектора. Следующая теорема даёт ответ на эти вопросы.
\end{note}

\begin{theorem} (о трёх эквивалентных определениях гауссовского вектора)
    Следующие утверждения эквивалентны:
    \begin{enumerate}
        \item $\xi = (\xi_1, \ldots, \xi_n)$ --- гауссовский вектор, то есть имеет характеристическую функцию
        \[
            \phi_\xi(t) = e^{i \tbr{a, t} - \frac{1}{2} \tbr{\Sigma t, t}},
        \]
        где $a \in \R^n$, $\Sigma$ --- матрица $n \times n$, симметричная и неотрицательно определённая

        \item Для $\xi$ $P$-почти наверное выполнено равенство $\xi = B\eta + a$, где $\eta = (\eta_1, \ldots, \eta_m)$ --- случайный вектор, $\eta_1, \ldots, \eta_m \sim N(0, 1)$ --- независимые, $a \in \R^n$, $B$ --- матрица размера $n \times m$, причём $m \le n$

        \item $\forall \tau \in \R^n$ случайная величина  $\tbr{\tau, \xi}$ имеет одномерное нормальное распределение. Здесь константы также считаем вырожденными нормальными случайными величинами $N(\mu, 0)$.
    \end{enumerate}
\end{theorem}

\begin{proof}~
    \begin{itemize}
        \item[$1 \Ra 2$] $\Sigma$ --- это симметричная и неотрицательно определённая матрица $n \times n$. Напомним несколько фактов из курса алгебры и геометрии:
        \begin{itemize}
            \item Если $\Sigma$ --- симметричная квадратная матрица, то $\Sigma$ --- матрица квадратичной формы в ортонормированном базисе
            
            \item Тогда в некотором другом ортонормированном базисе $\Sigma$ имеет диагональный вид
            
            \item Матрица $C$ называется ортогональной, если это матрица перехода между двумя произвольными ортонормированными базисами. Она обладает двумя важными свойствами:
            \begin{enumerate}
            	\item $C^T C = E$
            	
            	\item $\forall x, y \in \R^n\ \ \tbr{Cx, Cy} = \tbr{x, y}$
            \end{enumerate}
        
            \item Если $C$ --- матрица перехода между двумя упомянутыми выше ортонормированными базисами, то диагональный вид $\Sigma$ можно записать как $D = C \Sigma C^T$
            
            \item Так как $\Sigma$ неотрицательно определена, то все числа на главной диагонали $D$ неотрицательны. Можем считать, что базис выбран так, что сначала на диагонали идут положительные числа, а затем нули.
        \end{itemize}
    	Продолжая использовать обозначения $C$ и $D$ для матриц указанных выше, укажем явный вид $D$, который используем:
    	\[
    		D = \Matrix{
    			d_1 & 0 & & \cdots & & 0 \\
    			0 & \ddots & & & & \\
    			& & d_m & & & \vdots \\
    			\vdots & & & 0 & & \\
    			& & & & \ddots & 0 \\
    			0 & & \cdots & & 0 & 0 \\
    		}, 
    		\quad
    		d_1, \ldots, d_m > 0
    	\]
    	Отметим связь между скалярными произведениями в $\R^m$ и $\R^n$, которая пригодится дальше:
    	\[
    		\forall x \in \R^m, y \in \R^n, S \in M_{m \times n}(\R)\ \ \tbr{x, Sy} = x^T(Sy) = (x^TS)y = (S^Tx)^Ty = \tbr{S^Tx, y}
    	\]
    	Итак, нам дан случайный вектор $\xi \sim N(a, \Sigma)$. Рассмотрим случайный вектор $\xi' = C(\xi-a)$ и найдём его характеристическую функцию:
        \begin{multline*}
            \phi_{\xi'}(t) = \E e^{i \tbr{\xi', t}} = \E e^{i \tbr{C\xi, t}} e^{-i \tbr{Ca, t}} = \E e^{i \tbr{\xi, C^{T}t}} e^{-i \tbr{a, C^{T}t}} =
            \\
            = \phi_{\xi}(C^{T}t) e^{-i \tbr{a, C^{T}t}} = \text{\normalsize[опр. гауссовского в-ра]\large} = e^{i \tbr{a, C^{T}t} - \frac{1}{2} \tbr{\Sigma C^{T}t, C^{T}t}} e^{-i \tbr{a, C^{T}t}} =
            \\
            = e^{- \frac{1}{2} \tbr{\Sigma C^{T}t, C^{T}t}} = e^{- \frac{1}{2} \tbr{C \Sigma C^{T}t, t}} = e^{- \frac{1}{2} \tbr{Dt, t}} = e^{-\frac{1}{2} \sum_{k = 1}^m d_k t_k^2} = \prod_{k = 1}^m e^{-\frac{1}{2} d_k t_k^2}
        \end{multline*}
        Заметим, что из вида характеристической функции для $\xi'$ мы по сути знаем вид и для координат:
        \begin{align*}
        	&{\forall k \in \range{1}{m}\ \ \phi_{\xi'_k}(t_k) = e^{-\frac{1}{2}d_kt_k^2} \Ra \xi'_k \sim N(0, d_k)}
        	\\
        	&{\forall k \in \range{m + 1}{n}\ \ \phi_{\xi'_k}(t_k) = 1 \Ra \xi'_k \sim Const(0) \Ra \xi'_k = 0 \text{ $P$-почти наверное}}
        \end{align*}
        Причём мы параллельно получили, что $\phi_{\xi'}(t) = \prod_{k = 1}^n \phi_{\xi'_k}(t_k)$, а тогда по теореме о единственности и критерию независимости в терминах харфункций компоненты $\{\xi'_k\}_{k = 1}^n$ случайного вектора $\xi'$ независимы и имеют в точности такое распределение, как указано выше. Введём для первых $m$ координат $\xi'$ отнормированные величины $\eta_k = \xi'_k / \sqrt{d_k}$, для которых верно, что $\eta_k \sim N(0, 1)$. Обозначим $\eta = (\eta_1, \ldots, \eta_m)^T$. Тогда, мы можем выразить $\xi'$ через $\eta$ в матричнлм виде:
        \[
            \xi' \stackrel{P\text{ п.н.}}{=}
            \begin{pmatrix}
                \sqrt{d_1} & \cdots & 0 \\
                \vdots & \ddots & \vdots \\
                0 & \cdots & \sqrt{d_m}\\
                0 & \cdots & 0 \\
                \vdots & & \vdots \\
                0 & \cdots & 0 \\
            \end{pmatrix}
            \cdot \eta = A \eta
        \]
        где $A \in M_{n \times m}(\R)$ --- обозначение для матрицы из центральной части равенства. Равенство для первых $m$ координат случайного вектора $\xi'$ выполнено всюду, равенство почти наверное возникает из-за того, что последние компоненты $\xi'$ равны нулю почти наверное. При этом, разумеется, в условии не обязательно соблюдать эти особенности равенства почти наверное, если переопределить $\eta$ на множестве нулевой вероятности, рассуждение не сломается. Таким образом, мы можем записать теперь $\xi$ через $\eta$:
        \[
            \xi = C^{-1}\xi' + a = C^T\xi' + a \stackrel{P\text{ п.н.}}{=} C^T(A\eta) + a = B\eta + a,\ \ B = C^TA
        \]

        \item[$2 \Ra 3$] Пусть $\tau \in \R^n$. Тогда:
        \[
            \tbr{\tau, \xi} \stackrel{P\text{ п.н.}}{=} \tbr{\tau, B\eta + a} = \tbr{\tau, a} + \tbr{\tau, B\eta} = \tbr{\tau, a} + \tbr{B^T\tau, \eta} = \sum_{k = 1}^m (B^T\tau)_k \eta_k + \tbr{\tau, a}
        \]

        Здесь $\eta_1, \ldots, \eta_m \sim N(0, 1)$ --- независимые случайные величины. Как понимаем, в том числе из примера к теореме о единственности из главы про характеристические функции, аффинная (линейная с константой) комбинация независимых нормальных случайных величин является нормальной случайной величиной, возможно, вырожденной. Поэтому $\tbr{\tau, \xi}$ является нормальной случайной величиной, возможно, вырожденной. Это ровно то, что нам нужно.

        \item[$3 \Ra 1$] Пусть любая линейная комбинация компонент $\{\xi_k\}_{k = 1}^n$ случайного вектора $\xi$ имеет нормальное распределение. В частности, $\{\xi_k\}_{k = 1}^n$ --- нормальные случайные величины. Тогда у них конечны математическое ожидание и дисперсия $\E\xi_k,\ D\xi_k$.

        Пусть $\tau \in \R^n$. Тогда обозначим $\tbr{\tau, \xi} \sim N(a_\tau, \sigma_\tau^2)$, где
        \begin{align*}
            & a_\tau = \E \tbr{\tau, \xi} = \E\sum_{k = 1}^n \tau_k \xi_k = \sum_{k = 1}^n \tau_k \E\xi_k = \tbr{\tau, \E\xi}
            \\
            & \sigma_\tau^2 = D \tbr{\tau, \xi} = \E (\tbr{\tau, \xi} - \E \tbr{\tau, \xi})^2 = \E (\tbr{\tau, \xi} - \tbr{\tau, \E\xi})^2 = \E (\tbr{\tau, \xi - \E\xi})^2 =
            \\
            & = \E\sum_{i, j = 1}^{n} \tau_i \tau_j (\xi_i - \E\xi_i) (\xi_j - \E\xi_j) = \sum_{i, j = 1}^{n} \tau_i \tau_j cov(\xi_i, \xi_j) = \tbr{D\xi \cdot \tau, \tau}
        \end{align*}

        Обозначим $a = \E\xi,\ \Sigma = D\xi$. Заметим, что $\phi_\xi(\tau) = \E e^{i \tbr{\xi, \tau}} = \phi_{\tbr{\tau, \xi}}(1)$. При этом $\tbr{\tau, \xi}$ нормально распределено, знаем математическое ожидание и дисперсию, тогда:
        \[
            \phi_\xi(\tau) = \phi_{\tbr{\tau, \xi}}(1) = e^{i a_\tau - \frac{1}{2} \sigma_\tau^2} = e^{i \tbr{\tau, \E\xi} - \frac{1}{2} \tbr{D\xi \tau, \tau}} = e^{i \tbr{\tau, a} - \frac{1}{2} \tbr{\Sigma \tau, \tau}}
        \]

        При этом $\Sigma = D\xi$, симметрична и неотрицательно определена как матрица ковариаций.
    \end{itemize}
\end{proof}

\begin{corollary}
    Определение гауссовского вектора корректно, то есть для любых заданных $a$, $\Sigma$ существует гауссовский вектор $\xi \sim N(a, \Sigma).$ Его распределение определено однозначно.
\end{corollary}

\begin{proof}
    Вспомним часть доказательства $1 \Ra 2$. Зная матрицу $\Sigma$, можно построить матрицу $B$. Отметим 1 полезный факт, который нам поможет в дальнейшем:
    \[
    	D = C \Sigma C^T \Lora \Sigma = C^{-1} D (C^T)^{-1} = C^T D C = C^T A A^T C = C^T A (C^T A)^T = B B^T
    \]
    Итак, пусть $\eta_1, \ldots, \eta_m \sim N(0, 1)$ --- независимые случайные величины, $\eta = (\eta_1, \ldots, \eta_m)^T$. Определим $\xi := B\eta + a$. Посчитаем характеристическую функцию $\xi$:
    \begin{multline*}
        \phi_\xi(t) = \E e^{i \tbr{\xi, t}} = \E e^{i \tbr{B\eta + a, t}} = e^{i \tbr{a, t}} \E e^{i \tbr{\eta, B^T t}} = e^{i \tbr{a, t}} \prod_{k = 1}^m \E e^{i \eta_k (B^T t)_k} =
        \\
        = e^{i \tbr{a, t}} \prod_{k = 1}^m e^{-\frac{(B^T t)_k^2}{2}} = e^{i \tbr{a, t} - \frac{1}{2} \sum_{k = 1}^m (B^T t)_k^2} = e^{i \tbr{a, t} - \frac{1}{2} \tbr{B^T t, B^T t}} = e^{i \tbr{a, t} - \frac{1}{2} \tbr{B B^T t, t}}
    \end{multline*}

    Здесь пользовались тем, что $\{\eta_k\}_{k = 1}^m$ независимы, для $\eta_1, \ldots, \eta_m \sim N(0, 1)$ знаем характеристическую функцию. Так как по построению $B$ и по замечанию в доказательстве $\Sigma = B B^T$, то характеристическая функция $\xi$ действительно имеет требуемый вид.

    Однозначность распределения следует из теоремы о единственности распределения для характеристических функций случайных векторов.
\end{proof}

\begin{corollary} (О смысле параметров)
    Если $\xi \sim N(a, \Sigma)$ --- гауссовский вектор, то $a = \E\xi$, $\Sigma = D\xi$. Если $\xi \stackrel{P\text{ п.н.}}{=} B\eta + a$, где $\eta$ --- случайный вектор из $\R^m$ с независимыми стандартными нормальными компонентами, то $a = \E\xi$ --- то же самое $a$, $\Sigma = B B^T$.
\end{corollary}

\begin{proof}
    Первое утверждение непосредственно следует из части доказательства $3 \Ra 1$. Второе утверждение непосредственно следует из доказательства предыдущего следствия, с учётом того, что отличие случайных векторов на множестве нулевой вероятности не влияет на характеристическую функцию.
\end{proof}

\begin{corollary}
    Аффинное преобразование, в частности линейное преобразование, гауссовского вектора является гауссовским вектором.
\end{corollary}

\begin{proof}
    Пусть $\xi$ --- гауссовский вектор, $\xi \stackrel{P\text{ п.н.}}{=} B\eta + a$, где $\eta = (\eta_1, \ldots, \eta_m)$ --- случайный вектор, $\eta_1, \ldots, \eta_m \sim N(0, 1)$ --- независимые.
    
    Пусть $\zeta = A \xi + b$ --- случайный вектор, полученный аффинным преобразованием $\xi$. Тогда его можно выразить через $\eta$:
    \[
        \zeta = A \xi + b = A (B\eta + a) + b = (AB)\eta +(Aa + b)
    \]

    По второму из трёх эквивалентных определений гауссовского вектора, $\zeta$ также гауссов.
\end{proof}

\begin{corollary} (Независимость компонент гауссовского вектора)
    Пусть $\xi = (\xi_1, \ldots, \xi_n)$ --- гауссовский вектор. Тогда $\xi_1, \ldots, \xi_n$ независимы в совокупности $\Leftrightarrow$ $\xi_1, \ldots, \xi_n$ попарно некоррелированы, то есть ковариации всех пар равны нулю.
\end{corollary}

\begin{proof}~
    \begin{itemize}
        \item[$\Ra$] Верно вообще всегда для случайных величин с конечными дисперсиями (тогда из оценки интегралов все ковариации пар существуют и конечны), здесь случайные величины нормально распределены, поэтому дисперсии, разумеется, конечны.

        \item[$\La$] Пусть $\xi \sim N(a, \Sigma)$. Если $\xi_1, \ldots, \xi_n$ попарно некоррелированы, то $\Sigma = D\xi$ диагональна. Обозначив $\sigma_k^2 = D \xi_k$, распишем характеристическую функцию $\xi$:
        \[
            \phi_\xi(t) = e^{i \tbr{a, t} - \frac{1}{2} \tbr{\Sigma t, t}} = \prod_{k = 1}^n e^{i a_k t_k - \frac{1}{2} \sigma_k^2 t_k^2} = \prod_{k = 1}^n \phi_{\xi_k}(t_k)
        \]

        Получили, что характеристическая функция случайного вектора $\xi$ распадается в произведение характеристических функций его компонент. По критерию независимости в терминах характеристических функций, $\xi_1, \ldots, \xi_n$ независимы.
    \end{itemize}
\end{proof}

\begin{corollary}  (Обобщение следствия про независимость компонент гауссовского вектора, без доказательства)
    Пусть $\xi = (\xi_1, \ldots, \xi_n) \sim N(a, \Sigma)$ --- гауссовский вектор, $\Sigma$ имеет блочно-диагональный вид:
    \[
        \Sigma =
        \begin{pmatrix}
            \Sigma_1 & 0 & \cdots & 0 \\
            0 & \ddots & & \vdots \\
            \vdots & & \ddots & 0 \\
            0 & \cdots & 0 & \Sigma_s \\
        \end{pmatrix}
    \]
    
    Здесь $\Sigma_1, \ldots, \Sigma_s$ --- квадратные матрицы-блоки размеров $k_1, \ldots, k_s$ соответственно.

    Тогда случайные векторы $(\xi_1, \ldots, \xi_{k_1})^T, (\xi_{k_1+1}, \ldots, \xi_{k_1+k_2})^T, \ldots, (\xi_{k_1 + \ldots + k_{s-1} +1}, \ldots, \xi_n)^T$ независимы в совокупности.
\end{corollary}

\begin{exercise}
    Если  $\xi_1, \ldots, \xi_n$ --- нормальные случайные величины, то случайный вектор $\xi = (\xi_1, \ldots, \xi_n)$ не обязательно является гауссовским.
\end{exercise}

\begin{exercise} (О плотности гауссовского вектора)
    Пусть $\xi = (\xi_1, \ldots, \xi_n)^T \sim N(a, \Sigma)$. Тогда $\xi$ имеет плотность тогда и только тогда, когда $\Sigma$ положительно определена, то есть когда $\Sigma$ не вырождена.

    В этом случае плотность задаётся следующей формулой:
    \[
        p_\xi(x) = \left( \frac{1}{\sqrt{2\pi}} \right)^n \frac{1}{\sqrt{det \Sigma}} e^{-\frac{1}{2} \tbr{\Sigma^{-1}(x-a),\ x-a}}
    \]
\end{exercise}

\begin{note}
    Отсюда видно, что если бы задавали гауссовский вектор через плотность, то упустили бы случай вырожденной матрицы ковариаций.
\end{note}

\begin{theorem} (Многомерная Центральная Предельная Теорема, без доказательства)
    Пусть $\{\xi_n\}_{n = 1}^\infty$ --- независимые одинаково распределённые случайные векторы, пусть $a = \E\xi_1$, $\Sigma = D\xi_1$ конечны. Обозначим $S_n = \xi_1 + \ldots + \xi_n$. Тогда имеет место сходимость:
    \[
        \sqrt{n} \left( \frac{S_n}{n} - a \right) \xrightarrow[n \to \infty]{d} N(0, \Sigma)
    \]
\end{theorem}

\begin{note}
    Доказательство аналогично одномерному случаю, но более громоздкое. Здесь важно, что переносили не исходную формулировку ЦПТ с делением на корень из дисперсии, а немного изменённый вариант, благодаря чему случай вырожденной матрицы ковариаций тоже осмыслен. Это также говорит о том, почему важно не упускать случай вырожденной дисперсии в определении гауссовского вектора.
\end{note}

\section{Условное математическое ожидание}

\begin{definition}
    Пусть $\xi$ --- случайная величина на вероятностном пространстве \\ $(\Omega, \F, P)$, а также $\cC \subseteq \F$, $\cC$ --- произвольная $\sigma$-алгебра. \textit{Условным математическим ожиданием случайной величины $\xi$ относительно $\sigma$-алгебры $\cC$} называется случайная величина $\E(\xi | \cC)$, удовлетворяющая двум свойствам:
    \begin{enumerate}
        \item (Свойство измеримости) $\E(\xi | \cC)$ является $\cC$-измеримой случайной величиной, то есть порождённая ей $\sigma$-алгебра $\F_{\E(\xi | \cC)} \subseteq \cC$.

        \item (Интегральное свойство)
        \[
            \forall A \in \cC \ \ \E(\xi \chi_A) = \E(\E(\xi | \cC) \chi_A)
        \]
        Или в терминах интегралов:
        \[
            \forall A \in \cC \ \ \int_A \xi dP = \int_A \E(\xi | \cC) dP
        \]
    \end{enumerate}
\end{definition}

\begin{note}
    Почему в общем случае $\xi \neq \E(\xi | \cC)$? Интегральное свойство, конечно, выполнено, но $\xi$ не обязательно $\cC$-измерима.
\end{note}

\begin{theorem} (Теорема о существовании и единственности УМО, без доказательства)
    Пусть $\xi$ --- случайная величина в вероятностном пространстве $(\Omega, \F, P)$. Если $\E|\xi| < +\infty$ (или, что то же самое, $\E\xi \neq \infty$), то для любой $\cC \subseteq \F$ --- $\sigma$-подалгебры --- условное математическое ожидание $\E(\xi | \cC)$ существует и единственно с точностью до равенства $P$-почти наверное.
\end{theorem}

\begin{note}
    Доказательство этого факта опирается на теорему Радона-Никодима.
\end{note}

\begin{lemma} (Дискретный случай УМО)
    Пусть $\xi$ --- случайная величина на пространстве $(\Omega, \F, P)$, $\sigma$-алгебра $\cC \subseteq \F$ порождена не более, чем счётным разбиением $\{D_n\}_{n = 1}^\infty$ множества $\Omega$, то есть $\Omega = \bscup_{n = 1}^{\infty} D_n$. Пусть также $\E|\xi| < +\infty$. Тогда
    \[
        \forall \omega \in \Omega\ \ \E(\xi | \cC)(\omega) = \sum_{n = 1}^{\infty} \frac{\E(\xi \chi_{D_n})}{P(D_n)} \chi_{D_n}(\omega)
    \]
\end{lemma}

\begin{proof}
    В силу теоремы о существовании и единственности УМО существует и единственно. Проверим, что случайная величина
    \[
        \eta = \sum_{n = 1}^{\infty} \frac{\E(\xi \chi_{D_n})}{P(D_n)} \chi_{D_n}
    \]
    подходит под определение УМО, нужно проверить два свойства:
    \begin{enumerate}
        \item (Измеримость) $\forall n \in \N \ \ D_n \in \cC$, то есть каждый индикатор $\chi_{D_n}$ $\cC$-измерим. Переходя сначала к конечным суммам, а затем к пределу, получим, что случайная величина $\eta$ $\cC$-измерима.

        \item (Интегральное свойство) Нужно проверить свойство для множеств $A \in \cC$. Рассмотрим случаи:
        \begin{enumerate}
        	\item $A = D_k,\ k \in \N$. Для таких множеств можно сказать следующее:
        	\begin{multline*}
        		\E(\eta \chi_A) = \E(\eta \chi_{D_k}) = \E \left( \sum_{n = 1}^{\infty} \frac{\E(\xi \chi_{D_n})}{P(D_n)} \chi_{D_n} \chi_{D_k} \right) = \E \left(\frac{\E(\xi \chi_{D_k})}{P(D_k)} \chi_{D_k} \right) =
        		\\
        		= \frac{\E(\xi \chi_{D_k})}{P(D_k)} P(D_k) = \E(\xi \chi_{D_k}) = \E(\xi \chi_A)
        	\end{multline*}
        	
        	\item $A \in \cC$ --- общий случай. Тогда $A$ является дизъюнктным объединением не более, чем счётного числа событий $D_n$. Осталось проверить условие конечности интеграла $\E|\eta|$, чтобы применить счётную аддитивность интеграла Лебега. Для этого необходимо и достаточно проверить выполнение этого факта на произвольном $D_n$, что делается тривиально:
        	\[
        		\E(|\eta|\chi_{D_k}) = |\E(\xi\chi_{D_k})| \le \E(|\xi|\chi_{D_k}) < \infty
        	\]
        	ибо теперь мы тривиально заявляем о конечности интеграла $|\eta|$ по конечным объединениям $D_k$, а по теореме Леви и просто есть неравенство $\E|\eta| \le \E|\xi| < +\infty$. Стало быть, у матожидания есть счётная аддитивность, которая сразу завершает доказательство.
        \end{enumerate}
    \end{enumerate}
\end{proof}

\begin{note}
    Смысл УМО: усреднение $\xi$ по $\cC$.

    \textcolor{red}{Здесь нужна картинка с лекции или что-то похожее}

    Это видно по дискретному случаю УМО: на каждом множестве $D_n$ мы заменяем функцию $\xi$ на $const$, равную среднему значению $\xi$ на этом множестве.
\end{note}
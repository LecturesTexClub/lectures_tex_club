\newcommand\bigzero{\makebox(0,0){\text{\Huge0}}}

\begin{note}
    Изучив некоторые инструменты работы со сходимостью случайных векторов, хочется увидеть, где эти сходимости могут возникать. Для сходимости случайных величин знаем две такие теоремы: Закон Больших Чисел и Центральная Предельная Теорема.
\end{note}

\begin{theorem} (Усиленный Закон Больших Чисел в форме Колмогорова для случайных векторов)
    Пусть $\{\xi_n\}_{n = 1}^\infty$ --- независимые одинаково распределённые случайные векторы $\R^m \to \R$. Пусть $\E\xi_1 \neq \infty$. Тогда
    \[
        \frac{\xi_1 + \ldots + \xi_n}{n} \xrightarrow{P\text{ п.н.}} \E\xi_1
    \]
\end{theorem}

\begin{proof}
    Так как случайные векторы $\{\xi_n\}_{n = 1}^\infty$ независимы, то их координаты по каждому индексу $\{\xi_{t, n}\}_{n = 1}^\infty$ тоже независимы, как борелевские функции от соответствующих $\xi_n$. Координаты также наследуют одинаковую распределённость и конечность математического ожидания. Применив для них одномерный УЗБЧ в форме Колмогорова и учитывая то, что сходимость почти наверное случайных векторов эквивалентна соответствующей покоординатной сходимости, получим требуемое.
\end{proof}

\section{Многомерное нормальное распределение}

\begin{note}
    Хотим получить многомерный аналог Центральной Предельной Теоремы. Если переносить формулировку с одномерного случая, то возникнет вопрос: <<А что же такое многомерное нормальное распределение?>>
    
    Определить многомерное нормальное распределение более трудно, чем его одномерный случай. Если определять через плотность, то потеряется существенная часть ситуаций, а функция распределения выглядит не очень приятно уже в размерности один. Оказывается, хорошо определять через характеристическую функцию.
\end{note}

\begin{definition}
    Случайный вектор $\xi = (\xi_1, \ldots, \xi_n)$ называется \textit{гауссовским (или нормальным)}, если его характеристическая функция имеет такой вид:
    \[
        \phi(t) = e^{i \langle a, t \rangle - \frac{1}{2} \langle \Sigma t, t \rangle}
    \]
    где
    \begin{itemize}
        \item $\tbr{x, y}$ --- скалярное произведение $x$ и $y$ из $\R^n$
        
        \item $a \in \R^n$ --- произвольный вектор
        
        \item $\Sigma$ --- матрица $n \times n$, симметричная и неотрицательно определённая
    \end{itemize}
    Обозначение: $\xi \sim N(a, \Sigma)$.
\end{definition}

\begin{note}
    Сразу возникает вопрос: является ли $\phi(t)$ характеристической функцией (некоторого случайного вектора). И даже если верим, что является, хочется узнать какие-то свойства гауссовского вектора. Следующая теорема даёт ответ на эти вопросы.
\end{note}

\begin{theorem} (о трёх эквивалентных определениях гауссовского вектора)
    Следующие утверждения эквивалентны:
    \begin{enumerate}
        \item $\xi = (\xi_1, \ldots, \xi_n)$ --- гауссовский вектор, то есть имеет характеристическую функцию
        \[
            \phi_\xi(t) = e^{i \langle a, t \rangle - \frac{1}{2} \langle \Sigma t, t \rangle},
        \]
        где $a \in \R^n$, $\Sigma$ --- матрица $n \times n$, симметричная и неотрицательно определённая

        \item Для $\xi$ $P$-почти наверное выполнено равенство $\xi = B\eta + a$, где $\eta = (\eta_1, \ldots, \eta_m)$ --- случайный вектор, $\eta_1, \ldots, \eta_m \sim N(0, 1)$ --- независимые, $a \in \R^n$, $B$ --- матрица размера $n \times m$

        \item $\forall \tau \in \R^n$ случайная величина  $\langle \tau, \xi \rangle$ имеет одномерное нормальное распределение. Здесь константы также считаем вырожденными нормальными случайными величинами $N(\mu, 0)$.
    \end{enumerate}
\end{theorem}

\begin{proof}~
    \begin{itemize}
        \item[$1 \Ra 2$] $\Sigma$ --- это симметричная и неотрицательно определённая матрица $n \times n$. Напомним несколько фактов из курса алгебры и геометрии:
        \begin{itemize}
            \item Если $\Sigma$ --- симметричная квадратная матрица, то $\Sigma$ --- матрица квадратичной формы в ортонормированном базисе
            
            \item Тогда в некотором другом ортонормированном базисе $\Sigma$ имеет диагональный вид
            
            \item Матрица $C$ называется ортогональной, если это матрица перехода между двумя произвольными ортонормированными базисами. Она обладает двумя важными свойствами:
            \begin{enumerate}
            	\item $C^T C = E$
            	
            	\item $\forall x, y \in \R^n\ \ \tbr{Cx, Cy} = \tbr{x, y}$
            \end{enumerate}
        
            \item Если $C$ --- матрица перехода между двумя упомянутыми выше ортонормированными базисами, то диагональный вид $\Sigma$ можно записать как $D = C \Sigma C^T$
            
            \item Так как $\Sigma$ неотрицательно определена, то все числа на главной диагонали $D$ неотрицательны. Можем считать, что базис выбран так, что сначала на диагонали идут положительные числа, а затем нули.
        \end{itemize}
    	Продолжая использовать обозначения $C$ и $D$ для матриц указанных выше, укажем явный вид $D$, который используем:
    	\[
    		D = \Matrix{
    			d_1 & 0 & & \cdots & & 0 \\
    			0 & \ddots & & & & \\
    			& & d_m & & & \vdots \\
    			\vdots & & & 0 & & \\
    			& & & & \ddots & 0 \\
    			0 & & \cdots & & 0 & 0 \\
    		}, 
    		\quad
    		d_1, \ldots, d_m > 0
    	\]
    	Отметим связь между скалярными произведениями в $\R^m$ и $\R^n$, которая пригодится дальше:
    	\[
    		\forall x \in \R^m, y \in \R^n, S \in M_{m \times n}(\R)\ \ \tbr{x, Sy} = x^T(Sy) = (x^TS)y = (S^Tx)^Ty = \tbr{S^Tx, y}
    	\]
    	Итак, нам дан случайный вектор $\xi \sim N(a, \Sigma)$. Рассмотрим случайный вектор $\xi' = C(\xi-a)$ и найдём его характеристическую функцию:
        \begin{multline*}
            \phi_{\xi'}(t) = \E e^{i \langle \xi', t \rangle} = \E e^{i \langle C\xi, t \rangle} e^{-i \langle Ca, t \rangle} = \E e^{i \langle \xi, C^{T}t \rangle} e^{-i \langle a, C^{T}t \rangle} =
            \\
            = \phi_{\xi}(C^{T}t) e^{-i \langle a, C^{T}t \rangle} = \text{\normalsize[опр. гауссовского в-ра]\large} = e^{i \langle a, C^{T}t \rangle - \frac{1}{2} \langle \Sigma C^{T}t, C^{T}t \rangle} e^{-i \langle a, C^{T}t \rangle} =
            \\
            = e^{- \frac{1}{2} \langle \Sigma C^{T}t, C^{T}t \rangle} = e^{- \frac{1}{2} \langle C \Sigma C^{T}t, t \rangle} = e^{- \frac{1}{2} \langle Dt, t \rangle} = e^{-\frac{1}{2} \sum_{k=1}^m d_k t_k^2} = \prod_{k=1}^m e^{-\frac{1}{2} d_k t_k^2}
        \end{multline*}
        \normalsize

        Заметим, что
        \begin{itemize}
            \item $\phi_{\xi'_k}(t_k) = e^{-\frac{1}{2} d_k t_k^2}$, $k=1, \ldots, m$ --- характеристические функции случайных величин $\xi'_k \sim N(0, d_k)$
            \item $\phi_{\xi'_k}(t_k) = 1$, $k=m+1, \ldots, n$ --- характеристические функции случайных величин $\xi'_k \sim Const(0)$, то есть $\xi'_k \stackrel{P\text{ п.н.}}{=} 0$
            \item $\phi_{\xi'}(t) = \prod_{k=1}^n \phi_{\xi'_k}(t_k)$
        \end{itemize}

        Тогда по теореме о единственности и критерию независимости в терминах характеристических функций компоненты $(\xi'_1, \ldots, \xi'_n)$ случайного вектора $\xi'$ независимы и имеют в точности такое распределение, как указано выше.

        Обозначим $\eta_k = \xi'_k/\sqrt{d_k},\ k = 1, \ldots, m$. Тогда $\eta_1, \ldots, \eta_m \sim N(0, 1)$ --- независимые одинаково распределённые случайные величины. Обозначим $\eta = (\eta_1, \ldots, \eta_m)$.

        Можем записать в матричном виде:
        \[
            \xi' \stackrel{P\text{ п.н.}}{=} \begin{pmatrix}
                \sqrt{d_1} & & \bigzero \\
                \bigzero & \ddots & \\
                 & & \sqrt{d_m} \\
                0 & \cdots & 0 \\
                \vdots & & \vdots \\
                0 & \cdots & 0
            \end{pmatrix} \cdot \eta = A \cdot \eta
        \]

        Здесь $A$ --- матрица размера $n \times m$ --- обозначение для матрицы из центральной части равенства. Равенство для первых $m$ координат случайного вектора $\xi'$ выполнено всюду, равенство почти наверное возникает из-за того, что последние компоненты $\xi'$ равны нулю почти наверное. При этом, разумеется, в условии не обязательно соблюдать эти особенности равенства почти наверное, если переопределить $\eta$ на множестве нулевой вероятности, рассуждение не сломается.

        Тогда получим, что:
        \[
            \xi = C^{-1}\xi' + a = C^T\xi' + a \stackrel{P\text{ п.н.}}{=} C^T(A\eta) + a = B\eta + a,\ \ B = C^TA
        \]
        И действительно, $\eta = (\eta_1, \ldots, \eta_m)$ --- случайный вектор, $\eta_1, \ldots, \eta_m \sim N(0, 1)$ --- независимые, $a \in \R^n$, $B$ --- матрица размера $n \times m$.

        На этом доказательство $1 \Ra 2$ закончилось, но заметим ещё один полезный факт:
        \begin{align*}
            & D = C \Sigma C^T
            \\
            & \Downarrow
            \\
            & \Sigma = C^{-1} D (C^T)^{-1} = C^T D C = C^T A A^T C = C^T A (C^T A)^T = B B^T
        \end{align*}

        \item[$2 \Ra 3$] Уже заметили одно полезное свойство скалярного произведения в $\R^m$ и $\R^n$, в предыдущей части доказательства:
        \[
            \forall x \in \R^m,\ y \in \R^n,\ S \text{ --- матрица } m \times n \ \ \langle x, Sy \rangle = \langle S^T x, y \rangle
        \]

        Пусть $\tau \in \R^n$. Тогда:
        \[
            \langle \tau, \xi \rangle \stackrel{P\text{ п.н.}}{=} \langle \tau, B\eta + a \rangle = \langle \tau, a \rangle + \langle \tau, B\eta \rangle = \langle \tau, a \rangle + \langle B^T\tau, \eta \rangle = \sum_{k=1}^m (B^T\tau)_k \eta_k + \langle \tau, a \rangle
        \]

        Здесь $\eta_1, \ldots, \eta_m \sim N(0, 1)$ --- независимые случайные величины. Как понимаем, в том числе из примера к теореме о единственности из главы про характеристические функции, аффинная (линейная с константой) комбинация независимых нормальных случайных величин является нормальной случайной величиной, возможно, вырожденной. Поэтому $\langle \tau, \xi \rangle$ является нормальной случайной величиной, возможно, вырожденной. Это ровно то, что нам нужно.

        \item[$3 \Ra 1$] Пусть любая линейная комбинация компонент $\xi_1, \ldots, \xi_n$ случайного вектора $\xi$ имеет нормальное распределение. В частности, $\xi_1, \ldots, \xi_n$ --- нормальные случайные величины. Тогда у них конечны математическое ожидание и дисперсия $\E\xi_k,\ D\xi_k$.

        Пусть $\tau \in \R^n$. Тогда обозначим $\langle \tau, \xi \rangle \sim N(a_\tau, \sigma_\tau^2)$, где
        \begin{align*}
            & a_\tau = \E \langle \tau, \xi \rangle = \E \sum_{k=1}^n \tau_k \xi_k = \sum_{k=1}^n \tau_k \E\xi_k = \langle \tau, \E\xi \rangle
            \\
            & \sigma_\tau^2 = D \langle \tau, \xi \rangle = \E (\langle \tau, \xi \rangle - \E \langle \tau, \xi \rangle)^2 = \E (\langle \tau, \xi \rangle - \langle \tau, \E\xi \rangle)^2 = \E (\langle \tau, \xi - \E\xi \rangle)^2 =
            \\
            & = \E \sum_{i, j = 1}^{n} \tau_i \tau_j (\xi_i - \E\xi_i) (\xi_j - \E\xi_j) = \sum_{i, j = 1}^{n} \tau_i \tau_j cov(\xi_i, \xi_j) = \langle D\xi \cdot \tau, \tau \rangle
        \end{align*}

        Обозначим $a = \E\xi,\ \Sigma = D\xi$. Заметим, что $\phi_\xi(\tau) = \E e^{i \langle \xi, \tau \rangle} = \phi_{\langle \tau, \xi \rangle}(1)$. При этом $\langle \tau, \xi \rangle$ нормально распределено, знаем математическое ожидание и дисперсию, тогда:
        \large
        \[
            \phi_\xi(\tau) = \phi_{\langle \tau, \xi \rangle}(1) = e^{i a_\tau - \frac{1}{2} \sigma_\tau^2} = e^{i \langle \tau, \E\xi \rangle - \frac{1}{2} \langle D\xi \tau, \tau \rangle} = e^{i \langle \tau, a \rangle - \frac{1}{2} \langle \Sigma \tau, \tau \rangle}
        \]
        \normalsize

        При этом $\Sigma = D\xi$, симметрична и неотрицательно определена как матрица ковариаций.
    \end{itemize}
\end{proof}

\begin{corollary}
    Определение гауссовского вектора корректно, то есть для любых заданных $a$, $\Sigma$ существует гауссовский вектор $\xi \sim N(a, \Sigma).$ Его распределение определено однозначно.
\end{corollary}

\begin{proof}
    Вспомним часть доказательства $1 \Ra 2$. Зная матрицу $\Sigma$, можно построить матрицу $B$. Пусть $\eta_1, \ldots, \eta_n \sim N(0, 1)$ --- независимые случайные величины, $\eta = (\eta_1, \ldots, \eta_n)$. Определим $\xi = B \eta + a$.

    Замечание автора конспекта: кажется, сразу нельзя сказать, что характеристическая функция $\xi$ имеет нужный вид, конечно, можно сказать, что $\xi \sim N(a', \Sigma')$, после этого можно по $\Sigma'$ построить $B'$, получить, что $\xi = B \eta + a = B' \eta' + a'$. Здесь встаёт вопрос об однозначности такого представления.

    Посчитаем характеристическую функцию $\xi$:
    \large
    \begin{multline*}
        \phi_\xi(t) = \E e^{i \langle \xi, t \rangle} = \E e^{i \langle B \eta + a, t \rangle} = e^{i \langle a, t \rangle} \E e^{i \langle \eta, B^T t \rangle} = e^{i \langle a, t \rangle} \prod_{k=1}^m \E e^{i \eta_k (B^T t)_k} =
        \\
        = e^{i \langle a, t \rangle} \prod_{k=1}^m e^{-\frac{(B^T t)_k^2}{2}} = e^{i \langle a, t \rangle - \frac{1}{2} \sum_{k=1}^m (B^T t)_k^2} = e^{i \langle a, t \rangle - \frac{1}{2} \langle B^T t, B^T t \rangle} = e^{i \langle a, t \rangle - \frac{1}{2} \langle B B^T t, t \rangle}
    \end{multline*}
    \normalsize

    Здесь пользовались тем, что $\eta_1, \ldots, \eta_m$ независимы, для $\eta_1, \ldots, \eta_m \sim N(0, 1)$ знаем характеристическую функцию. Так как по построению $B$ и по замечанию в доказательстве $\Sigma = B B^T$, то характеристическая функция $\xi$ действительно имеет требуемый вид.

    Однозначность распределения следует из теоремы о единственности распределения для характеристических функций случайных векторов.
\end{proof}

\begin{corollary} (О смысле параметров)
    Если $\xi \sim N(a, \Sigma)$ --- гауссовский вектор, то $a = \E\xi$, $\Sigma = D\xi$. Если $\xi \stackrel{P\text{ п.н.}}{=} B\eta + a$, где $\eta$ --- случайный вектор из $\R^m$ с независимыми стандартными нормальными компонентами, то $a = \E\xi$ --- то же самое $a$, $\Sigma = B B^T$.
\end{corollary}

\begin{proof}
    Первое утверждение непосредственно следует из части доказательства $3 \Ra 1$. Второе утверждение непосредственно следует из доказательства предыдущего следствия, с учётом того, что отличие случайных векторов на множестве нулевой вероятности не влияет на характеристическую функцию.
\end{proof}

\begin{corollary}
    Аффинное преобразование, в частности линейное преобразование, гауссовского вектора является гауссовским вектором.
\end{corollary}

\begin{proof}
    Пусть $\xi$ --- гауссовский вектор, $\xi \stackrel{P\text{ п.н.}}{=} B\eta + a$, где $\eta = (\eta_1, \ldots, \eta_m)$ --- случайный вектор, $\eta_1, \ldots, \eta_m \sim N(0, 1)$ --- независимые.
    
    Пусть $\zeta = A \xi + b$ --- случайный вектор --- аффинное преобразование $\xi$. Тогда
    \[
        \zeta = A \xi + b = A (B\eta + a) + b = (AB)\eta +(Aa + b)
    \]

    По второму из трёх эквивалентных определений гауссовского вектора им является $\zeta$.
\end{proof}

\begin{corollary} (Независимость компонент гауссовского вектора)
    Пусть $\xi = (\xi_1, \ldots, \xi_n)$ --- гауссовский вектор. Тогда $\xi_1, \ldots, \xi_n$ независимы в совокупности $\Leftrightarrow$ $\xi_1, \ldots, \xi_n$ попарно некоррелированы, то есть ковариации всех пар равны нулю.
\end{corollary}

\begin{proof}~
    \begin{itemize}
        \item[$\Ra$] Верно вообще всегда для случайных величин с конечными дисперсиями (тогда из оценки интегралов все ковариации пар существуют и конечны), здесь случайные величины нормально распределены, поэтому дисперсии, разумеется, конечны.

        \item[$\La$] Пусть $\xi \sim N(a, \Sigma)$. Если $\xi_1, \ldots, \xi_n$ попарно некоррелированы, то $\Sigma = D\xi$ диагональна. Обозначив $\sigma_k^2 = D \xi_k$, распишем характеристическую функцию $\xi$:
        \[
            \phi_\xi(t) = e^{i \langle a, t \rangle - \frac{1}{2} \langle \Sigma t, t \rangle} = \prod_{k=1}^n e^{i a_k t_k - \frac{1}{2} \sigma_k^2 t_k^2} = \prod_{k=1}^n \phi_{\xi_k}(t_k)
        \]

        Получили, что характеристическая функция случайного вектора $\xi$ распадается в произведение характеристических функций его компонент. По критерию независимости в терминах характеристических функций, $\xi_1, \ldots, \xi_n$ независимы.
    \end{itemize}
\end{proof}

\begin{corollary}  (Обобщение следствия про независимость компонент гауссовского вектора, без доказательства)
    Пусть $\xi = (\xi_1, \ldots, \xi_n) \sim N(a, \Sigma)$ --- гауссовский вектор, $\Sigma$ имеет блочно-диагональный вид:
    \[
        \Sigma = \begin{pmatrix}
            \Sigma_1 & & \bigzero \\
            \bigzero & \ddots & \\
             & & \Sigma_s
        \end{pmatrix}
    \]
    
    Здесь $\Sigma_1, \ldots, \Sigma_s$ --- квадратные матрицы-блоки размеров $k_1, \ldots, k_s$ соответственно.

    Тогда случайные векторы $(\xi_1, \ldots, \xi_{k_1})$, $(\xi_{k_1+1}, \ldots, \xi_{k_1+k_2})$, \ldots, $(\xi_{k_1 + \ldots + k_{s-1} +1}, \ldots, \xi_n)$ независимы в совокупности.
\end{corollary}

\begin{exercise}
    Если  $\xi_1, \ldots, \xi_n$ --- нормальные случайные величины, то случайный вектор $\xi = (\xi_1, \ldots, \xi_n)$ не обязательно является гауссовским.
\end{exercise}

\begin{exercise} (О плотности гауссовского вектора)
    Пусть $\xi = (\xi_1, \ldots, \xi_n) \sim N(a, \Sigma)$. Тогда $\xi$ имеет плотность тогда и только тогда, когда $\Sigma$ положительно определена, то есть когда $\Sigma$ не вырождена.

    В этом случае плотность задаётся следующей формулой:
    \[
        p_\xi(x) = \left( \frac{1}{\sqrt{2\pi}} \right)^n \frac{1}{\sqrt{det \Sigma}} e^{-\frac{1}{2} \langle \Sigma^{-1}(x-a),\ x-a \rangle}
    \]
\end{exercise}

\begin{note}
    Отсюда видно, что если бы задавали гауссовский вектор через плотность, то упустили бы случай вырожденной матрицы ковариаций.
\end{note}

\begin{theorem} (Многомерная Центральная Предельная Теорема, без доказательства)
    Пусть $(\xi_n,\ n \in \N)$ --- независимые одинаково распределённые случайные векторы, пусть $a = \E\xi_1 (= \E\xi_2 = \ldots)$, $\Sigma = D\xi_1 (= D\xi_2 = \ldots)$ конечны. Обозначим $S_n = \xi_1 + \ldots + \xi_n$. Тогда
    \[
        \sqrt{n} \left( \frac{S_n}{n} - a \right) \xrightarrow{d} N(0, \Sigma)
    \]
\end{theorem}

\begin{note}
    Доказательство аналогично одномерному случаю, но более громоздкое. Здесь важно, что переносили не исходную формулировку ЦПТ с делением на корень из дисперсии, а немного изменённый вариант, благодаря чему случай вырожденной матрицы ковариаций тоже осмыслен. Это также говорит о том, почему важно не упускать случай вырожденной дисперсии в определении гауссовского вектора.
\end{note}

\section{Условное математическое ожидание}

\begin{definition}
    Пусть $\xi$ --- случайная величина на вероятностном пространстве $(\Omega, \F, P)$. Пусть $\mathcal{C} \subset \F$, $\mathcal{C}$ --- тоже $\sigma$-алгебра. Условным математическим ожиданием случайной величины $\xi$ относительно $\sigma$-алгебры $\mathcal{C}$ называется случайная величина $\E(\xi | \mathcal{C})$, удовлетворяющая двум свойствам:
    \begin{enumerate}
        \item Свойство измеримости: $\E(\xi | \mathcal{C})$ является $\mathcal{C}$-измеримой случайной величиной, то есть порождённая ей $\sigma$-алгебра $\F_{\E(\xi | \mathcal{C})} \subset \mathcal{C}$.

        \item Интегральное свойство:
        \[
            \forall A \in \mathcal{C} \ \ \E(\xi I_A) = \E(\E(\xi | \mathcal{C}) I_A)
        \]
        Или в терминах интегралов:
        \[
            \forall A \in \mathcal{C} \ \ \int_A \xi dP = \int_A \E(\xi | \mathcal{C}) dP
        \]
    \end{enumerate}
\end{definition}

\begin{note}
    Почему в общем случае $\xi \neq \E(\xi | \mathcal{C})$? Интегральное свойство, конечно, выполнено, но $\xi$ не обязательно $\mathcal{C}$-измерима.
\end{note}

\begin{theorem} (Теорема о существовании и единственности УМО, без доказательства)
    Если $\E|\xi| < +\infty$ (или, что то же самое, $\E\xi$ конечно), то для любой $\mathcal{C}$ --- под-$\sigma$-алгебры $\F$ --- условное математическое ожидание $\E(\xi | \mathcal{C})$ существует и единственно с точностью до равенства почти наверное.
\end{theorem}

\begin{note}
    Доказательство этого факта опирается на теорему Радона-Никодима.
\end{note}

\begin{lemma} (Дискретный случай УМО)
    Пусть $\sigma$-алгебра $\mathcal{C}$ порождена не более, чем счётным разбиением $\{D_n,\ n \in \N\}$ множества $\Omega$, то есть $\Omega = \sqcup_{n=1}^{\infty} D_n$. Пусть $\E|\xi| < +\infty$, $\mathcal{C} \subset \F$. Тогда
    \[
        \E(\xi | \mathcal{C})(\omega) = \sum_{n=1}^{\infty} \frac{\E(\xi I_{D_n})}{P(D_n)} I_{D_n}(\omega)
    \]
\end{lemma}

\begin{proof}
    В силу теоремы о существовании и единственности УМО существует и единственно. Проверим, что случайная величина
    \[
        \eta = \sum_{n=1}^{\infty} \frac{\E(\xi I_{D_n})}{P(D_n)} I_{D_n}
    \]
    подходит под определение УМО, нужно проверить два свойства:
    \begin{enumerate}
        \item (Измеримость) $\forall n \in \N \ \ D_n \in \mathcal{C}$ $\Ra$ каждый индикатор $I_{D_n}$ $\mathcal{C}$-измерим. Переходя сначала к конечным суммам, а затем к пределу, получим, что случайная величина $\eta$ $\mathcal{C}$-измерима.

        \item (Интегральное свойство) Нужно проверить свойство для множеств $A \in \mathcal{C}$. Сначала рассмотрим случай множеств $A = D_k,\ k \in \N$. Для таких множеств:
        \begin{multline*}
            \E(\eta I_A) = \E(\eta I_{D_k}) = \E \left( \sum_{n=1}^{\infty} \frac{\E(\xi I_{D_n})}{P(D_n)} I_{D_n} I_{D_k} \right) = \E \left(\frac{\E(\xi I_{D_k})}{P(D_k)} I_{D_k} \right) =
            \\
            = \frac{\E(\xi I_{D_k})}{P(D_k)} P(D_k) = \E(\xi I_{D_k}) = \E(\xi I_A)
        \end{multline*}

        Точно так же получаем, что для таких множеств:
        \[
            \E(|\eta| I_A) = \E(|\eta| I_{D_k}) = |\E(\xi I_{D_k})| = |\E(\xi I_A)| \le \E(|\xi| I_A)
        \]
        
        Отсюда получаем, что интегралы от случайной величины $|\eta|$ по множествам $\sqcup_{n=1}^{N} D_n$ не превосходят интегралы по таким же множествам от случайной величины $|\xi|$, поэтому, в силу теоремы Леви переходя к пределу, $\E|\eta| \le E|\xi| < +\infty$. Это нужно для того, чтобы применить для $\xi$ и $\eta$ счётную аддитивность интеграла Лебега.

        Теперь общий случай: если $A \in \mathcal{C}$, то $A$ является дизъюнктным объединением не более, чем счётного числа событий $D_n$. В силу конечной и счётной аддитивности интеграла Лебега, интегральное свойство достаточно проверить для событий $D_n$, что уже сделали.
    \end{enumerate}
\end{proof}

\begin{note}
    Смысл УМО: усреднение $\xi$ по $\mathcal{C}$.

    \textcolor{red}{Здесь нужна картинка с лекции или что-то похожее}

    Это видно по дискретному случаю УМО: на каждом множестве $D_n$ мы заменяем функцию $\xi$ на $const$, равную среднему значению $\xi$ на этом множестве.
\end{note}
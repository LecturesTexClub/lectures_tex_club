\section{Автономные системы}
\subsection{Основные понятия}
Пусть $\Sigma \subset \mathbb R^n$ открыто, $f: \Sigma \to \mathbb R^n$ непрерывно дифференцируема.
Рассмотрим систему
\begin{equation}
    x' = f(x).
\end{equation}

\textbf{Определение.} Такая система называется \textit{автономной}, а $\Sigma$ --- её \textit{фазовым пространством}.

\textbf{Определение.} Пусть $x: I \to \mathbb R^n$ --- непродолжаемое решение системы (1).
Множество $\{x(t): t \in I\} \subset \mathbb R^n$ называется \textit{фазовой траекторией}.
В частности, это является проекцией графика на $\mathbb R^n$.

\textbf{Определение.} Пусть нашлось $\widehat x \in \Sigma$, такое что $f(\widehat x) = 0$.
Тогда $x(t) \equiv \widehat x$ является решением системы (1), а $\widehat x$ называется \textit{положением равновесия}, или особой точкой, или стационарной точкой.

\subsection{Свойства автономных систем}
\begin{enumerate}
    \item Если $x: (a, b) \to \mathbb R^n$ является решением системы (1), то для любого $c \in \mathbb R$ функция $y: (a - c, b - c) \to \mathbb R^n$, $y(t) = x(t + c)$ тоже является решением.

    \item Любые две траектории либо не пересекаются, либо совпадают.

        Пусть $x(\cdot), y(\cdot)$ --- два непродолжаемых решения (1).
        Предположим, что в каких-то двух точках $\tau \le s$ выполнено $x(s) = y(\tau)$.
        Положим $z(t) := y(t + \tau - s)$, по первому свойству она является решением.
        Более того, $z(s) = y(\tau) = x(s)$.
        Так, мы нашли два решения $x$ и $z$, совпадающие в точке $s$.
        По теореме о существовании и единственности $z(t) \equiv x(t)$, то есть $y(t + \tau - s) \equiv x(t)$, а значит, множества их значений --- траектории --- совпадают.

        \QED

    \item Пусть $x: \mathbb R \to \mathbb R^n$ --- решение системы (1).
        Предположим, что нашлись $t_1 < t_2$, такие что $x(t_1) = x(t_2)$, причём $x$ не является константой.
        Тогда $x(\cdot)$ --- периодическая функция с положительным минимальным периодом, а её траектория является замкнутой кривой без самопересечений.
        Более строго, это значит, что если $x(0) = x(T)$, то $T$ --- период.

        \textbf{Доказательство.} Положим $y(t) = x(t + t_2 - t_1)$ --- является решением (1).
        Более того, $y(t_1) = x(t_2) = x(t_1)$, то есть по теореме о существовании и единственности $y(t) \equiv x(t)$.
        Положим $d = t_2 - t_1$, тогда это тождество переписывается в виде $x(t + d) \equiv x(t)$.
        Докажем, что самопересечений нет.
        Пусть $P := \{p > 0~|~x(t + p) \equiv x(t)\}$ --- это множество по доказанному точно не пусто.

        Пусть $\widehat p = \inf(P)$, докажем, что $\widehat p \in P$.
        Рассмотрим последовательность $\{p_j\} \subset P$, такую что $p_j \downarrow \widehat p$ при $j \to \infty$.
        Тогда для любых $t$ и $j$ выполняется $x(t + p_j) = x(t)$.
        Переходя к пределу, получаем $x(t + \widehat p) = x(t)$, то есть $\widehat p \in P \cup \{0\}$.

        Докажем, что $\widehat p > 0$, от противного.
        Тогда найдётся $\{p_j\} \subset P$, стремящаяся к нулю.
        Это означает, что для любого $j$ выполнено $x(p_j) = x(0)$.
        Записывая покоординатно, получаем, что $x_i(p_j) = x_i(0)$ для всех $i, j$.
        По теореме Ролля для любого $i$ существует последовательность $\{\Theta_{i,j}\}$, стремящаяся к нулю, такая что $x_i'(\Theta_{i,j}) = 0$.
        Вспомним, что мы решали уравнение $x' = f(x)$, то есть $0 = f_i(x(\Theta_{i,j}))$.
        Из соображений непрерывности $f$ получаем, что $f_i(x(0)) = 0$ для всех $i$, то есть $f(x(0)) = 0$.
        Следовательно, $z(t) \equiv x(0)$ --- решение системы (1) и по теореме о существовании и единственности $x(t) \equiv z(t)$, то есть $x(t)$ --- это всё-таки константа, противоречие.

        Остаётся вопрос, почему нет самопересечений.
        Но это просто: в самом начале мы доказали, что если $x(t_1) = x(t_2)$, то $|t_2 - t_1|$ является периодом, из этого от противного можно доказать отсутствие самопересечений.

        \QED

    \item Траектория --- это либо точка, либо замкнутая кривая без самопересечений, либо незамкнутая кривая без самопересечений.
        Последнее означает, что если $t_1 \ne t_2$, то $x(t_1) \ne x(t_2)$.
        В качестве упражнения.

    \item Обозначим через $\phi(\cdot, \xi)$ непродолжаемое решение задачи Коши
        \begin{equation}
            \begin{cases}
                x' = f(x) \\
                x(0) = \xi
            \end{cases} .
        \end{equation}
        Пусть $\Theta$ --- область определения функции $\phi$.
        Тогда $\Theta$ открыто, а $\phi$ непрерывно дифференцируема.

        Следует из зависимости решения от параметра (последний параграф третьего семестра).

    \item (Групповое свойство автономной системы) Справедливо тождество $\phi(t, \phi(s, \xi)) = \phi(t + s, \xi)$, где $(s, \xi), (t + s, \xi) \in \Theta$.
        
        \begin{figure}[ht]
            \centering
            \incfig{4-1}{0.75\linewidth}
            \caption{Групповое свойство}
        \end{figure}

        Как это представлять: вот у нас есть какая-то траектория, стартующая из точки $\xi$.
        В точке $s$ она находится в какой-то точке $\phi(s, \xi)$, и из этой точки, как из стартовой, можно пустить ещё одну траекторию, которая в точке $t$ примет значение $\phi(t, \phi(s, \xi))$.
        Но то же самое значение примет и исходная траектория в точке $s + t$.

        И при чём тут группы: рассмотрим отображения $\phi(t, \cdot): \mathbb R^n \to \mathbb R^n$ (здесь $t$ --- параметр).
        Зададим операцию $\phi(t, \cdot) \circ \phi(s, \cdot) = \phi(t, \phi(s, \cdot))$.
        И вот по групповому свойству множество таких отображений является абелевой группой с единицей $\phi(0, \cdot)$.

        \textbf{Доказательство.} По свойству 1 функция $t \mapsto \phi(t + s, \xi)$ является решением системы (1).
        Также по определению $\phi(t, \phi(s, \xi))$ тоже является решением.
        При $t = 0$: $\phi(t + s, \xi) = \phi(s, \xi)$ и $\phi(t, \phi(s, \xi)) = \phi(0, \phi(s, \xi)) = \phi(s, \xi)$.
        По теореме о существовании и единственности для любого $s$ выполняется $\phi(t + s, \xi) \equiv \phi(t, \phi(s, \xi))$.

        \QED

        \textbf{Замечание.} Могут возникнуть проблемы с областями определения, но будем считать, что решения определены всюду, чтобы не заморачиваться.

\end{enumerate}

\subsection{Предельные множества траекторий}
Пусть $x: (t_0, +\infty) \to \mathbb R^n$ --- решение системы (1), $T$ --- его траектория.

\textbf{Определение.} $a \in \mathbb R^n$ называется \textit{$\omega$-предельной точкой} (траектории $T$), если существует последовательность $\{t_j\} \to +\infty$, такая что $x(t_j) \to a$ при $j \to +\infty$.

Обозначим через $\Omega(T)$ \textit{$\omega$-предельное множество} --- множество всех $\omega$-предельных точек траектории $T$.

\textbf{Пример 1.} Пусть $n = 1$, $t_0 = 0$ и $x' = x$.
Рассмотрим решение $x(t) = e^t$, тогда $T = (1, +\infty)$.
Тогда $\Omega(T) = \varnothing$.

\textbf{Пример 2.} То же самое, но $x' = -x$ и $x = e^{-t}$.
Тогда $T = (0, 1)$ и $\Omega(T) = \{0\}$.

\textbf{Теорема.} (б/д) Пусть $T$ ограничена и найдётся $\varepsilon > 0$, такое что $\varepsilon$-окрестность траектории $T$ вложена в $\Sigma$ (то есть вложено с запасом).
Тогда $\Omega(T)$ непусто, ограничено, замкнуто, связно и состоит из траекторий, то есть является дизъюнктным объединением каких-то траекторий.

\textbf{Теорема.} (Бендиксона, б/д) Предположим, что $n = 2$, $\Omega(T)$ ограничено и непусто, и на нём $f(x) \ne 0$, то есть не содержит положения равновесия.
Тогда $\Omega(T)$ --- это замкнутая траектория.

\setcounter{equation}{0}
\section{Автономные системы на плоскости}
\subsection{Линейные автономные системы}
Рассмотрим автономную систему $x' = Ax$ или же
\[
    \begin{cases}
        x_1' = a_{11} x_1 + a_{12} x_2 \\
        x_2' = a_{21} x_1 + a_{22} x_2
    \end{cases}
\]
Как минимум, у неё есть одно положение равновесия --- $x = 0$.
Будем рассматривать случаи: пусть $\lambda_1, \lambda_2$ --- собственные значения матрицы $A$.

\subsubsection{$\lambda_1, \lambda_2 \in \mathbb R \setminus \{0\}$, $\lambda_1 \ne \lambda_2$}
Пусть $h_1$ и $h_2$ --- два собственных вектора.
Тогда в их базисе система будет иметь вид
\[
    \begin{cases}
        y_1' = \lambda_1 y_1 \\
        y_2' = \lambda_2 y_2
    \end{cases}
\]
и её решением будет
\[
    \begin{cases}
        y_1 = c_1 e^{\lambda_1 t} \\
        y_2 = c_2 e^{\lambda_2 t}
    \end{cases} .
\]
Напишем уравнение её траектории, то есть исключим параметр $t$:
\[
    e^{\lambda_1 t} = \frac{y_1}{c_1} \Rightarrow y_2 = c_2(e^{\lambda_1 t})^{\frac{\lambda_2}{\lambda_1}} = c_2 \left(\frac{y_1}{c_1} \right)^{\frac{\lambda_2}{\lambda_1}}, 
\]
при $c_1 \ne 0$, а при $c_1 = 0$ получится уравнение $y_1 = 0$.

Теперь рассмотрим график, тут тоже есть несколько случаев.

Первый случай: $\lambda_1 \cdot \lambda_2 > 0$.
Если $\lambda_2 > \lambda_1 > 0$, то получится портет, как на рисунке 2 справа, если $\lambda_1 > \lambda_2 > 0$, то слева.
Направление в обоих случаях от начала координат, так как собственные значения положительные.

\begin{figure}[ht]
    \centering
    \incfig{knot-portrait}{0.8\linewidth}
    \caption{Портрет узла}
\end{figure}

\textbf{Определение.} Полученный портрет называется \textit{неустойчивым узлом}.

Если $\lambda_2 < \lambda_1 < 0$ или $\lambda_1 < \lambda_2 < 0$, то получатся аналогичные портреты, но с направлением к началу координат.

\textbf{Определение.} Полученный портрет называется \textit{устойчивым узлом}.

Во всех случаях положением равновесия является начало координат.

Второй случай: $\lambda_1 \cdot \lambda_2 < 0$, портрет приведён на рисунке 3.
Ориентация зависит от знаков, здесь проще подставить $t \to +\infty$, чтобы восстановить.

\begin{figure}[ht]
    \centering
    \incfig{seat-portrait}{0.8\linewidth}
    \caption{Портрет седла}
\end{figure}

\textbf{Определение.} Полученный портрет называется \textit{седлом}.

\subsubsection{$\lambda_1 = \lambda_2 = \lambda \in \mathbb R$}
Первый случай: $A$ имеет два линейно независимых собственных вектора $h_1$ и $h_2$.
Тогда аналогично первому случаю получаем, что кривая имеет вид
\[
    \begin{cases}
        y_2 = \frac{c_2}{c_1} y_1, & c_1 \ne 0 \\
        y_1 = 0, & c_1 = 0.
    \end{cases}
\]
График приведён на рисунке 4, направление, как обычно, к центру при $\lambda > 0$ и от центра при $\lambda < 0$.

\begin{figure}[ht]
    \centering
    \incfig{dicritical-knot}{0.5\linewidth}
    \caption{Дикритический узел}
\end{figure}

\textbf{Определение.} Полученный портрет называется \textit{дикритическим узлом}.
При $\lambda > 0$ называется \textit{неустойчивым}, при $\lambda < 0$ --- \textit{устойчивым}.

Второй случай: $h_1$ --- собственный вектор, $h_2$ --- присоединённый к нему.
Тогда в их базисе система будет иметь вид
\[
    \begin{cases}
        y_1' = \lambda y_1 + y_2 \\
        y_2' = \lambda y_2.
    \end{cases}
\]
Найдём решение:
\[
    \begin{cases}
        y_1 = c_1 e^{\lambda t} + c_2 t e^{\lambda t} \\
        y_2 = c_1 e^{\lambda t}.
    \end{cases}
\]
Выразим $t$:
\[
    e^{\lambda t} = \frac{y_2}{c_2} \Rightarrow t = \frac{1}{\lambda} \ln \left( \frac{y_2}{c_2} \right).
\]
Подставим в первое уравнение:
\[
    y_1 = c_1 \frac{y_2}{c_2} + \frac{c_2}{\lambda} \ln \left( \frac{y_2}{c_2} \right) \frac{y_2}{c_2}.
\]
График приведён на рисунке 5. При $\lambda > 0$ фазовые трактории направлены от центра и вдоль оси $Oy_1$, при $\lambda < 0$ --- к центру.

\begin{figure}[ht]
    \centering
    \incfig{degenerate-knot}{0.5\linewidth}
    \caption{Вырожденный узел}
\end{figure}

\textbf{Определение.} Сей портрет называется \textit{вырожденным узлом}. Неустойчивый при $\lambda > 0$, устойчивый при $\lambda < 0$.

\subsubsection{$\lambda_{1, 2} = \alpha \pm i \beta \in \mathbb C$}
Тогда собственные векторы имеют вид $h_{1,2} = a \pm i b$, где $a$ и $b$ --- линейно независимые векторы.
Как известно, фундаментальной системой решений здесь будет
\[
    \begin{cases}
        x_1 = e^{\alpha t}(a \cos(\beta t) - b \sin(\beta t)) \\
        x_2 = e^{\alpha t}(a \sin(\beta t) + b \cos(\beta t))
    \end{cases}
\]
В базисе $a$, $b$ она имеет вид
\[
    \begin{cases}
        y_1 = e^{\alpha t}
        \begin{pmatrix}
            \cos(\beta t) \\
            -\sin(\beta t)
        \end{pmatrix} \\

        y_2 = e^{\alpha t}
        \begin{pmatrix}
            \sin(\beta t) \\
            \cos(\beta t)
        \end{pmatrix}
    \end{cases}
\]
Собирая вместе и опуская вычисления, получаем, что решение имеет вид
\[
    y(t) = r e^{\alpha t}
    \begin{pmatrix}
        \cos(\beta (t - \Theta)) \\
        \sin(\beta (t - \Theta))
    \end{pmatrix}
\]
для всех $r$ и $\Theta$.
Берётся из $c_1 y_1 + c_2 y_2$, формул косинуса суммы, синуса суммы и безыдейной арифметики.
Теперь смотрим на это уравнение и разбираем случаи.

При $\alpha = 0$ получается уравнение окружности, портрет приведён на рисунке 6.
Для определения направления лучше снова просто посмотреть на сдвиг при увеличении $t$.

\begin{figure}[ht]
    \centering
    \incfig{circle-portrait}{0.5\linewidth}
    \caption{Портрет центра}
\end{figure}

\textbf{Определение.} Данный портрет называется \textit{центром}.

При $\alpha > 0$: теперь расстояния до начала координат увеличивается с увеличением $t$, поэтому получается спираль, вращающаяся против часовой стрелки при $\beta > 0$, и по часовой --- при $\beta < 0$.
В окрестности нуля происходит бесконечное число витков (при $t \to -\infty$), поэтому там обычно график не рисуют.
Направление в обоих случаях --- от начала координат.

\begin{figure}[ht]
    \centering
    \incfig{focus-portrait}{0.5\linewidth}
    \caption{Портрет фокуса}
\end{figure}

\textbf{Определение.} Полученный портрет называется \textit{неустойчивым фокусом}.

При $\alpha < 0$ --- всё то же самое, но теперь к центру координат: при $\beta < 0$ спираль идёт против часовой стрелки, если смотреть в направлении движения к центру координат, то есть зеркально тому, что было при $\alpha > 0$ и $\beta < 0$.

\subsection{Нелинейные автономные системы}
Пусть нам даны открытые множества $\Omega$ и $\Theta$ в $\mathbb R^2$ и отображения $f \in C^2(\Omega, \mathbb R^2)$ и $g \in C^2(\Theta, \mathbb R^2)$.
Рассмотрим системы
\begin{equation}
    x' = f(x)
\end{equation}
и
\begin{equation}
    y' = g(y).
\end{equation}

\textbf{Определение.} Автономные системы (1) и (2) \textit{качественно эквивалентны}, если существует гомеоморфизм $\pi: \Omega \to \Theta$, такой что для любой траектории $X$ системы системы (1) $\pi(X)$ --- траектория системы (2).
И наоборот: для любой траектории $Y$ системы (2) $\pi^{-1}(Y)$ --- траектория системы (1).
Более того, $\pi$ и $\pi^{-1}$ сохраняют ориентацию траекторий, или же, более строго, если $X$ --- траектория (1), $x(\cdot)$ --- соответствующее решение (1), $y(\cdot)$ --- решение (2), траектория которого совпадает с $\pi(X)$, и $t_1 < t_2$, то существуют $\tau_1 < \tau_2$, такие что $\pi(x(t_1)) = y(\tau_1)$ и $\pi(x(t_2)) = y(\tau_2)$.

Пусть $x_*$ --- положение равновесия системы (1) (то есть $f(x_*) = 0$).
Тогда, раскладывая по Тейлору, получаем
\[
    f(x) \equiv f(x_*) + \frac{\partial f}{\partial x}(x_*)(x - x_*) + R(x - x_*),
\]
где $R \in C^2$ и $R = o(x - x_*)$. Более того, $f(x_*) = 0$, так что про этот член можно забыть.
($\frac{\partial f}{\partial x}$ --- это матрица Якоби)

Рассмотрим систему
\begin{equation}
    y' = \frac{\partial f}{\partial x}(x_*) y.
\end{equation}
Утверждается, что при некоторых условиях она ведёт себя так же, как и система (1).

\textbf{Теорема.} Пусть $Re(\lambda) \ne 0$ для любого элемента $\lambda$ матрицы $\frac{\partial f}{\partial x}(x_*)$ системы (1).
Тогда в некоторой окрестности точки $x_*$ система (1) качественно эквивалентна (3).

\textbf{Замечание.} У теоремы такие странные условия, потому что иначе она не работает.

\textbf{Примеры.}
\begin{itemize}
    \item $x' = -x^3$ и $x' = x^3$. Матрица Якоби нулевая, и направления движений противоположные.
    \item
        \[
            \begin{cases}
                x_1' = -x_2 - x_1|x|^2 \\
                (x_2')^2 = x_1 - x_2|x|^2
            \end{cases}
        \]
        и $x_* = (0, 0)$.
        Здесь
        \[
            \frac{\partial f}{\partial x}(x_*) =
            \begin{pmatrix}
                0 & -1 \\
                1 & 0
            \end{pmatrix}
        \]
        Собственные значения --- $\lambda = \pm i$, получаем концентрические окружности, то есть решение вида
        \[
            \begin{cases}
                x_1(t) = r(t) \cos(\phi(t)) \\
                x_2(t) = r(t) \sin(\phi(t)).
            \end{cases}
        \]
        Подставляя его в исходное уравнение, получаем
        \[
            \begin{cases}
                r' \cos(\phi) - r \sin(\phi) \phi' = -r \sin(\phi) - r^3 \cos(\phi) \\
                r' \sin(\phi) + r \cos(\phi) \phi' = r \cos(\phi) - r^3 \sin(\phi)
            \end{cases}
        \]
        Умножая первое уравнение на $\cos(\phi)$, второе --- на $\sin(\phi)$ и складывая, получаем $r' = -r^3$.
        Умножая второе на $-\sin(\phi)$, второе --- на $\cos(\phi)$ и складывая, получаем $\phi' = 1$.
        У этого уравнения решением будет спираль, а у линеаризованной системы --- концентрические окружности, ибо собственными числами будут $\pm i$.
\end{itemize}

\setcounter{equation}{0}
\section{Устойчивость по Ляпунову и асимптотическая устойчивость}
\subsection{Определение и примеры}
\textbf{Определение.} Пусть задано открытое $\Omega \subset \mathbb R^n$, отображение $f: \Omega \to \mathbb R^n$, $f \in C^1$, и точка $x^* \in \Omega$ --- положение равновесия, то есть $f(x^*) = 0$.
Рассмотрим систему
\begin{equation}
    x' = f(x).
\end{equation}
Пусть $\phi(\cdot, \xi)$ --- непродолжаемое решение задачи Коши $x' = f(x)$, $x(0) = \xi$.
Положение равновесия $x^*$ называется \textit{устойчивым по Ляпунову}, если:
\begin{enumerate}
    \item Существует $r > 0$, такое что для любого $\xi \in \Omega \cap B_r(\xi)$ отображение $\phi(\cdot, \xi)$ определено на $[0, +\infty)$.
    \item Для любого $\varepsilon > 0$ существует $\delta > 0$, такое что для всех $\xi \in \Omega \cap B_\delta(x^*)$ верно $|\phi(t, \xi) - x^*| < \varepsilon$ для всех $t \ge 0$.
        (Чем-то напоминает непрерывность)
\end{enumerate}

Положение равновесия $x^*$ называется \textit{асимптотически устойчивым}, если:
\begin{enumerate}
    \item $x^*$ устойчиво по Ляпунову.
    \item Существует $d > 0$, такое что для всех $\xi \in \Omega \cap B_d(x^*)$ функция $\phi(t, \xi) \to x^*$ при $t \to +\infty$.
\end{enumerate}

\textbf{Примеры.} 
\begin{enumerate}
    \item Пусть $n = 2$, $f(x) = Ax$ и $x^* = 0$.
        Возвращаясь к случаям из предыдущего параграфа, по Ляпунову устойчивы все устойчивые портреты \textit{и центр}.
        Асимптотически устойчивы из них все, кроме центра.
    
    \item $n = 1$, $\Omega = \mathbb R$. Функция $f(x) > 0$ при $x \in (x^* - \varepsilon, x^*)$, $f(x^*) = 0$ и $f(x) < 0$ при $x \in (x^*, x^* + \varepsilon)$.
        Тогда можно проверить, что решение асимптотически устойчиво.
        Более того, если одно из строгих неравенств становится нестрогим, то теряется асимптотическая устойчивость, а если даже нет нестрогого, то и по Ляпунову.
\end{enumerate}

\subsection{Устойчивость линейных систем}
Пусть дана матрица $A \in \mathbb R^{n \times n}$ и система
\begin{equation}
    x' = Ax.
\end{equation}
Пусть $X(t)$ --- фундаментальная система решений системы (2), такая что $X(0) = I$ (единичная).
Она существует, так как можно рассмотреть $n$ задач Коши $x' = Ax$ и $x(0) = e_i$, где $e_i$ --- $i$-ый базисный вектор $\mathbb R^n$.
Обозначим через $\lambda_1, \dots, \lambda_s$ собственные значения матрицы $A$, а через $k_1, \dots, k_s$ --- размеры соответствующих жордановых клеток.

\textbf{Теорема.} 
\begin{itemize}
    \item Если все $Re(\lambda_j) < 0$, то $x^* = 0$ --- асимптотически устойчивое положение равновесия.
    \item Если все $Re(\lambda_j) \le 0$, а для тех $j$, что $Re(\lambda_j) = 0$, выполнено $k_j = 1$, то $x^* = 0$ устойчиво по Липунову.
    \item В остальных случаях не устойчиво.
\end{itemize}

\textbf{Доказательство.} Любое решение $x(t)$ системы (2) представимо в виде
\[
    x(t) = \sum_{j=1}^{s} P_j(t) e^{\lambda_j t}, t \ge 0.
\]
Здесь $P_j$ --- многочлен (с векторными значениями), такой что $\deg(P_j) \le k_j - 1$.

Первая часть:
\[
    e^{\lambda_j t} = e^{(Re(\lambda_j)) t} (\cos(Im(\lambda_j) t) + i \sin(Im(\lambda_j) t)).
\]
Это стремится к нулю быстрее любого многочлена, поэтому для любого решения $x(\cdot)$ выполнено $x(t) \to 0$ при $t \to +\infty$.
Докажем устойчивость по Ляпунову.
Так как столбцы $X(t)$ являются решениями, $\|X(t)\| \to 0$ при $t \to +\infty$.
В частности, $\|X(t)\|$ равномерно ограничена каким-то числом $c$.
Заметим, что $\phi(\cdot, \xi)$ при любом $\xi$ определено на $[0, +\infty)$, ибо (2) является линейной системой с постоянными коэффициентами.
Зафиксируем $\varepsilon > 0$, положим $\delta = \frac{\varepsilon}{c}$.
Имеем при $|\xi| < \delta$
\[
    |\phi(t, \xi)| = |X(t) \xi| \le \| X(t) \| \cdot |\xi| < c \cdot \frac{\varepsilon}{c} = \varepsilon.
\]
Следовательно, $|\phi(t, \xi)| \to 0$ --- асимптотическая устойчивость.

Вторая часть. Если $Re(\lambda_j) < 0$, то по первой части $P_j(t) e^{\lambda_j t}$ ограничено на $[0, +\infty)$.
Остаётся случай $Re(\lambda_j) = 0$ и $k_j = 1$.
В этом случае $P_j(t)$ имеет степень 0, то есть константа.
Таким образом,
\[
    P_j(t) e^{\lambda_j t} \equiv C \cdot (\cos(Im(\lambda_j) t) + i \sin(Im(\lambda_j) t)),
\]
где $C$ --- константа.
В этом случае решение вновь ограничено, поэтому существует $c$, такое что $\|X(t)\| \le c$ для всех $t \in [0, +\infty)$.
Аналогично первой части устойчиво по Ляпунову.

Третья часть. Пусть существует $j$, такой что $Re(\lambda_j) > 0$.
Обозначим $\lambda_j = \alpha + i \beta$.
Пусть $v = a + ib$ --- соответствующий собственный вектор.
Тогда
\[
    x(t) = e^{\alpha t} (\cos(\beta t) a - \sin(\beta t) b)
\]
является вещественным решением системы (2).
Домножим на $\gamma > 0$:
\[
    x_\gamma(t) = \gamma \cdot e^{\alpha t} (\cos(\beta t) a - \sin(\beta t) b).
\]
Система линейна, поэтому это всё ещё решение, причём $x_\gamma(\cdot)$ не ограничено на $[0, +\infty)$.
Более того, мы можем брать сколь угодно малое число $\gamma$, чтобы получать $|x_\gamma(0)| < \delta$, дальше легко доказывается отсутствие устойчивости по Ляпунову.

\QED

\setcounter{equation}{0}
\section{Условия устойчивости}
Пусть $f: \mathbb R^n \to \mathbb R^n$, $x^* \in \mathbb R^n$ --- положение равновесия, то есть $f(x^*) = 0$.
Рассмотрим систему
\begin{equation}
    x' = f(x),
\end{equation}
для которой $\phi(\cdot, \xi)$ является непродолжаемым решением задачи Коши
\[
    \begin{cases}
        x' = f(x) \\
        x(0) = \xi.
    \end{cases}
\]
Пусть $\Omega \subset \mathbb R^n$ открыто, $v: \Omega \to \mathbb R \in C^1$.

\textbf{Определение.} \textit{Производной в силу системы (1)} называется
\[
    \frac{dv}{dt} \bigg|_{(1)}(x) := \left< v'(x), f(x) \right>,
\]
где $x \in \Omega$.
Пусть $x(\cdot)$ --- решение системы (1). Тогда
\[
    \frac{d}{dt} v(x(t)) \equiv \left< v'(x(t)), x'(t) \right> \equiv \left< v'(x(t)), f(x(t)) \right> \equiv \frac{dv}{dt} \bigg|_{(1)}(x(t)).
\]

\textbf{Теорема.} (Ляпунова об устойчивости) Пусть существует $\rho > 0$, и $v \in C^1(B_{\rho}(x^*), \mathbb R)$, такие что $v(x^*) = 0$, $v(x) > 0$ при $x \ne x^*$ и для всех $x$ выполнено $\frac{dv}{dt} \big|_{(1)} (x) \le 0$.
Тогда $x^*$ устойчиво по Ляпунову.

Геометрическая интуиция: если взять достаточно маленький $\varepsilon$, то множество $\{x: v(x) = \varepsilon \}$ (линия уровня) образует замкнутую кривую вокруг $x^*$.
А условие на производную говорит, что при попадании на границу решение будет двигаться внутрь, то есть не выйдет за кривую.

\textbf{Доказательство.} Зафиксируем $\varepsilon > 0$, будем считать, что $\varepsilon < \rho$.
Положим $m := \min_{|x - x^*| = \varepsilon} (v(x))$.
По условию $m > 0$.
В частности, найдётся $\delta > 0$, такое что для всех $x \in B_{\delta}(x^*)$ верно $v(x) < m$.

Зафиксируем $\xi \in B_\delta(x^*)$. Допустим, что $\phi(\cdot, \xi)$ не определено на $[0, +\infty)$ или $\phi(\cdot, \xi)$ не содержится в шаре $B_\varepsilon(x^*)$ --- отрицание устойчивости по Ляпунову.
Вспомним теорему о продолжении решения до границы компакта: если рассмотреть цилиндр $\{(t, x): |x - x^*| \le \varepsilon\}$, то в первом случае решение должно выйти из него в какой-то точке $t_1$, то есть $|\phi(t_1, \xi) - x^*| = \varepsilon$.
Во втором случае допускается то же самое, поэтому существование такой точки мы и хотим опровергнуть.

По выбору $m$ мы знаем, что $v(x(t_1)) \ge m$ и $v(x(0)) = v(\xi) < m$.
Теперь вспомним, что у нас было условие на производную:
\[
    \frac{d}{dt} v(x(t)) \equiv \frac{dv}{dt} \bigg|_{(1)} (x(t)) \le 0.
\]
То есть $v$ не возрастает, но при этом в $\xi$ она меньше $m$, а в $t_1 > \xi$ --- больше, противоречие.

\QED

\textbf{Пример.}
\[
    \begin{cases}
        x_1' = -x_2 \\
        x_2' = x_1
    \end{cases}
    ,
    x^* = (0, 0)^T.
\]
Положим $v(x) = x_1^2 + x_2^2$. Тогда $v(0) = 0$ и $v(x) > 0$ при $x \ne 0$.
Найдём производную:
\[
    \frac{dv}{dt} \bigg|_{(1)} (x) = \left< (2x_1, 2x_2)^T, (-x_2, x_1)^T \right> \equiv 0.
\]
Следовательно, по теореме Ляпунова ноль устойчив.
Но асимптотической устойчивости нет: портретом является центр.

\textbf{Теорема.} (Ляпунова об асимптотической устойчивости) Пусть существует $\rho > 0$, и $v \in C^1(B_{\rho}(x^*), \mathbb R)$, такие что $v(x^*) = 0$, $v(x) > 0$ при $x \ne x^*$ и для всех $x \ne x^*$ выполнено $\frac{dv}{dt} \big|_{(1)} (x) < 0$.
Тогда $x^*$ асимптотически устойчиво.
От предыдущей теоремы отличается только последним условием.

\textbf{Доказательство.} По предыдущей теореме $x^*$ устойчиво по Ляпунову.
Положим $\varepsilon = \frac{\rho}{2}$, $\delta$ возьмём из определения устойчивости.
Вновь будем доказывать от противного: допустим, что существует $\xi \in B_\delta(x^*)$, $r > 0$ и возрастающая $t_j \to +\infty$, такая что $|\phi(t_j, \xi) - x^*| \ge r$.
Будем считать, что $r < \varepsilon$, также будем обозначать $x(t) = \phi(t, \xi)$.
По условию существует $\mu > 0$, такое что для всех $x$, таких что $r \le |x - x^*| \le \varepsilon$, выполнено $v(x) \ge \mu$.
Из условия на производную мы знаем, что $v(x(t))$ строго убывает на $[0, +\infty)$.

Отсюда следует, что $v(x(t)) \ge \mu$ для всех $t \in [0, +\infty)$.
Обосновывается это тем, что для любого $t$ найдётся $t_j > t$ из допущения, такая что $|\phi(t_j, \xi) - x^*| \ge r$, а это значит, что $v(x(t_j)) \ge \mu$.
В силу убывания и $v(x(t)) \ge v(x(t_j)) \ge \mu$.
По непрерывности $v$ существует $\delta_1 > 0$, такое что $v(x) < \mu$ при $|x - x^*| < \delta_1$.
Из доказанного $|x(t) - x^*| \ge \delta_1$.
Ещё существует $\beta > 0$, такое что $\frac{dv}{dt} \big|_{(1)}(x) \le -\beta$ для всех $x$, таких что $\delta_1 \le |x - x^*| \le \varepsilon$.
Это следует из определения производной в силу системы: функция непрерывна.
Теперь по формуле Ньютона-Лейбница
\[
    v(x(t)) - v(x(0)) \equiv \int_0^t \frac{d}{ds} v(x(s)) dx \equiv \int_0^t \frac{dv}{dt} \bigg|_{(1)}(x(s)) ds \le -\beta t
\]
в силу неравенства выше.

Следовательно, перенося $v(x(0))$ в правую часть, при $t \to +\infty$ получаем $v(x(t)) \le v(x(0)) - \beta t$.
Выражение справа стремится к $-\infty$, что противоречит тому, что $v(x) \ge 0$.

\QED

\textbf{Пример.} Пусть $n = 1$, $f: \mathbb R \to \mathbb R$, $f(x^*) = 0$, $f(x) > 0$ при $x < x^*$, $f(x) < 0$ при $x > x^*$.
Положим $v(x) := (x - x^*)^2$.
Найдём производную:
\[
    \frac{dv}{dx} \bigg|_{(1)} (x) = 2(x - x^*) f(x).
\]
После разбора случаев можно заметить, что это меньше нуля при $x \ne x^*$.

\textbf{Определение.} Функция $v$ в обеих теоремах называется \textit{функцией Ляпунова}.

\textbf{Теорема.} (Хассело-..., б/д) Теоремы Ляпунова --- это не только достаточное условие, но и необходимое.

Теперь остаётся важный вопрос: как найти функцию $v$?
Обычно это сложная задача, но в одном частном случае это можно сделать:

\textbf{Теорема.} (Об устойчивости по первому приближению) Пусть $f(x) = A(x - x^*) + o(x - x^*)$.
Пусть все $Re(\lambda) < 0$ для собственных значений $\lambda$ матрицы $A$.
Тогда $x^*$ асимптотически устойчиво.

\textbf{Доказательство.} Положим $\psi(t, \xi) = e^{tA} \xi$ для $t \ge 0$, $\xi \in \mathbb R^n$.
Заметим, что это является решением задачи Коши
\[
    \begin{cases}
        y' = Ay \\
        y(0) = \xi
    \end{cases},
\]
то есть линеаризованной версии исходной системы.
Тогда существуют $c, \alpha > 0$, такие что $|\phi(t, \xi)| \le c \cdot e^{-\alpha t} |\xi|$ --- следует из $Re(\lambda) < 0$.
Положим
\[
    v(x) = \int_0^{+\infty} |\psi(\tau, x)|^2 d\tau = \int_0^{+\infty} |e^{\tau A} \cdot x|^2 d\tau =
\]
В силу неравенства выше эта функция определена для всех $x$.
Заметим, что это является квадратичной формой, в которую подставлен $x$:
\[
    = \int_0^{+\infty} \left< e^{\tau A} x, e^{\tau A} x \right> d\tau = \int_0^{+\infty} \left<(e^{\tau A})^T e^{\tau A} \cdot x, x \right> d\tau =
\]
Тепеь можно два раза вынести $x$ из интеграла:
\[
    = \left< \int_0^{+\infty} (e^{\tau A})^T e^{\tau A} x \cdot d\tau, x \right>
    = \left< \int_0^{+\infty} (e^{\tau A})^T e^{\tau A} d\tau \cdot x, x \right>.
\]
Положим $Q = \int_0^{+\infty} (e^{\tau A})^T e^{\tau A} d\tau$, тогда это всё равно $\left<Qx, x \right>$.
Более того, $Q$ положительно определена, так как это интеграл квадрата.

Подставим $\psi$ в функцию $v$:
\[
    v(\psi(t, \xi)) = \int_0^{+\infty} |e^{\tau A} e^{tA} \xi|^2 d\tau = \int_t^{+\infty} |e^{sA} \xi|^2 ds
\]
--- в конце сделали замену $s = t + \tau$.

Теперь найдём производную от этого:
\[
    \frac{d}{dt} v(\psi(t, \xi))|_{t = 0} = -|e^{tA} \xi|^2 |_{t = 0} = -|\xi|^2.
\]
И производную в силу системы (2) (линеаризованной):
\[
    \frac{dv}{dt} \bigg|_{(2)} = \frac{d}{dt} v(\psi(t, \xi))|_{t = 0} = -|\xi|^2.
\]
Тогда
\[
    \frac{dv}{dt} \bigg|_{(1)} = \left< v'(\xi), f(\xi) \right> = \left<v'(\xi), A \xi + o(\xi) \right> =
\]
\[
    = \left<v'(\xi), A\xi \right> + \left<v'(\xi), o(\xi) \right> = \frac{dv}{dt} \bigg|_{(2)}(\xi) + \left<2Q\xi, o(\xi) \right> =
\]
\[
    = -|\xi|^2 + \left<2Q\xi, o(\xi) \right>.
\]
Так как второе слагаемое --- о-малое, в некоторой окрестности это всё не превосходит $-\frac{1}{2} |\xi|^2$.
Следовательно, $x^*$ асимптотически устойчиво.

\QED

\textbf{Замечание.} (б/д) Если существует собственное число $\lambda$ матрицы $A$, такое что $Re(\lambda) > 0$, то $x^*$ не устойчиво по Ляпунову.

\textbf{Замечание 2.} Таким образом, мы охарактеризовали все системы, кроме тех, у которых все собственные значения мнимые, но они встечаются редко.

\setcounter{equation}{0}
\section{Первые интегралы}
\subsection{Первые интегралы нормальных ОДУ}
Пусть нам даны $n \in \mathbb N$, открытое $\Omega \subset \mathbb R^{n+1}$ и отображение $f: \Omega \to \mathbb R^n$, $f \in C^1$.
Рассмотрим систему
\begin{equation}
    x' = f(t, x).
\end{equation}

\textbf{Определение.} \textit{Первым интегралом в области} $D \subset \Omega$ уравнения (1) называется функция $v: D \to \mathbb R$, такая что $v \in C^1$ и для любого решения $\phi(\cdot)$ системы (1), для которого $\forall t~(t, \phi(t)) \in D$, $v(t, \phi(t))$ является константой.

\textbf{Замечание.} Первый интеграл всегда существует, например, $v = C$.

\textbf{Определение.} Производная в силу системы для функции $v$ в этом случае определяется, как
\[
    \frac{dv}{dt} \bigg|_{(1)}(t, x) := \frac{\partial v}{\partial t}(t, x) + \left< \frac{\partial v}{\partial x} (t, x), f(t, x) \right>.
\]

\textbf{Утверждение.} Функция $v: D \to \mathbb R$, $v \in C^1$ является первым интегралом системы (1) тогда и только тогда $\frac{dv}{dt} \big|_{(1)}(t, x) = 0$ для всех $(t, x) \in D$.

\textbf{Доказательство.} $\Rightarrow$. Зафиксируем $(\tau, \xi) \in D$. 
Как известно, существует решение $\phi(\cdot)$ задачи Коши
\[
    \begin{cases}
        x' = f(t, x) \\
        x(\tau) = \xi
    \end{cases} .
\]
Сужая при необходимости область определения $\phi(\cdot)$, будем считать, что все $(t, \phi(t)) \in D$.
Так как функция $t \mapsto v(t, \phi(t))$ --- константа, $\frac{d}{dt}(v(t, \phi(t))) \equiv 0$.
Тогда
\[
    \frac{\partial v}{\partial t}(t, \phi(t)) + \left< \frac{\partial v}{\partial x}(t, \phi(t)), f(t, \phi(t)) \right> \equiv 0
\]
по производной композиции, также воспользовались тем, что $\frac{\partial \phi}{\partial t}(t) = f(t, \phi(t))$.
Подставляя $t = \tau$ и $\xi = \phi(t)$, получаем требуемое.

$\Leftarrow$. Аналогично, но в обратную сторону.

\QED

\textbf{Определение.} Пусть $v_1, \dots, v_k$ --- первые интегралы системы (1).
Они называются \textit{независимыми} в области $D$, если $\rank \left( \frac{\partial v_i}{\partial x_j}(t, x) \right) = k$.

\textbf{Замечание.} Зачем это всё нужно? Пусть $n = 2$, и $(t_0, x_0)$ --- какая-то точка в $\mathbb R^3$.
Возьмём два первых интеграла, проходящих через неё, $v_1, v_2$.
Тогда $A = \{(t, x): v_1(t, x) = v_1(t_0, x_0)\}$ и $B = \{(t, x): v_2(t, x) = v_2(t_0, x_0)\}$ --- это какие-то поверхности, и \textit{их пересечение является решением системы}.
Это утверждение является теоремой, которая не будет доказываться.

\textbf{Теорема.} Для любой точки $(t_0, x_0) \in \Omega$ существует окрестность $D \subset \Omega$, а в ней --- независимые в $D$ первые интегралы $v_1, \dots, v_n$.

\textbf{Доказательство.} Для любого $(t_0, \xi) \in D$ существует единственное непродолжаемое решение $\phi(\cdot, \xi)$ задачи Коши, ещё и непрерывно дифференцируемое:
\[
    \begin{cases}
        x' = f(t, x) \\
        x(t_0) = \xi 
    \end{cases} .
\]
Решим уравнение $x - \phi(t, \xi) = 0$ относительно $\xi$ с параметрами $(t, x)$ в окрестности $(t_0, x_0)$, соответствующей решению $x = x_0$, $t = t_0$, $\xi = x_0$.
Тогда $\phi(t_0, \xi) \equiv \xi$ по определению $\phi$.
Теперь
\[
    \frac{\partial}{\partial \xi} (x - \phi(t, \xi)) \bigg|_{\substack{t = t_0 \\ x = x_0}}(x_0) = -E.
\]
($x_0$ встречается дважды, так как сначала мы подставили параметр $x = x_0$, а потом неизвестную $\xi = x_0$)

Следовательно, применима теорема о неявной функции: существует окрестность $D_1 \subset \Omega$ точки $(t_0, x_0)$ и отображение $V = (v_1, \dots, v_n): D_1 \to \mathbb R^n$, такое что:
\begin{itemize}
    \item $V \in C^1$.
    \item Для всех $(t, x) \in D_1$ выполнено $x - \phi(t, V(t, x)) \equiv 0$.
    \item $V(t_0, x_0) = x_0$.
    \item Так как количество уравнений совпадает с количеством неизвестных, существует окрестность $\Delta$ точки $x_0$, такая что если $u \in \Delta$ и $x - \phi(t, u) = 0$, то $u = V(t, x)$.
\end{itemize}
Продифференцируем по $x$ второе свойство:
\[
    E \equiv \frac{\partial \phi}{\partial \xi}(t, V(t, x)) \cdot \frac{\partial V}{\partial x}(t, x).
\]
Подставим $(t_0, x_0)$: заметим, что $V(t_0, x_0) = x_0$, откуда это будет равно
\[
    E = \frac{\partial \phi}{\partial \xi}(t_0, x_0) \cdot \frac{\partial V}{\partial x}(t_0, x_0).
\]
Первый множитель равен $E$, поэтому
\[
    E = \frac{\partial V}{\partial x}(t_0, x_0).
\]
Отсюда ранг этой матрицы равен $n$, а значит, существует окрестность $D \subset D_1$, такая что в ней $\rank \left( \frac{\partial V}{\partial x} (t, x) \right) = n$.

Пусть $x(\cdot)$ --- решение задачи Коши $x' = f(t, x)$, $x(t_0) = \xi$.
По второму свойству $x(t) - \phi(t, V(t, x(t))) \equiv 0$.
Более того, из обозначений $x(t) - \phi(t, \xi) \equiv 0$.
Уменьшая область определения $x(\cdot)$, можно добиться того, чтобы $x(t)$ всегда попадал в $\Delta$, откуда по четвёртому свойству решение единственно и должно совпадать, поэтому $V(t, x(t)) \equiv \xi$, то есть $v_i(t, x(t)) \equiv \xi_i$ --- константы.

\QED

\subsection{Первые интегралы автономных систем}
Пусть нам даны $n \in \mathbb N$, открытое $\Omega \subset \mathbb R^n$ и отображение $f: \Omega \to \mathbb R^n$, $f \in C^1$.
Рассмотрим систему
\begin{equation}
    x' = f(x).
\end{equation}

\textbf{Теорема.} Для любого $x_0 \in \Omega$, такого что $f(x_0) \ne 0$ существует окрестность $D \subset \Omega$ точки $x_0$ и $n - 1$ независимых первых интегралов $v_i: D \to \mathbb R$.
От предыдущего случая отличается тем, что $v_i$ не зависит от $t$.

\textbf{Доказательство.}
Так как $f(x_0) \ne 0$, у него существует ненулевая координата.
Без ограничения общности это $n$-ая: $f_n(x_0) \ne 0$.
Из непрерывности $f_n$ получаем, что $f_n(x) \ne 0$ в некоторой окрестности $x_0$.
Рассмотрим неавтономную систему
\begin{equation}
    \frac{dx_i}{dx_n} = \frac{f_i(x)}{f_n(x)}.
\end{equation}
Здесь $(n - 1)$ уравнение, откуда по теореме из предыдущего пункта существует окрестность $D$ точки $x_0$ и независимые первые интегралы системы (3) $v_1, \dots, v_{n-1}: D \to \mathbb R$.

Пусть $\phi(\cdot) = (\phi_1(\cdot), \dots, \phi_n(\cdot))$ --- какое-то решение системы (2), такое что для всех $t$ $\phi(t) \in D$, то есть $\phi_n'(t) = f_n(\phi(t)) \ne 0$
Итак, мы получили строго монотонную функцию $\phi_n(t)$ на интервале --- по первому семестру матанализа существует обратная к ней функция $t(\phi_n)$.
Обозначим $x_i(x_n) = \phi_i(t(x_n))$.
Тогда
\[
    \frac{dx_i}{dx_n}(x_n) \equiv \frac{d \phi_i}{dt}(t(x_n)) \cdot \frac{dt}{dx_n}(x_n) \equiv f_i(\phi(t(x_n))) \cdot \frac{1}{\phi_n'(t(x_n))} \equiv
\]
\[
    \equiv \frac{f_i(\phi(t(x_n)))}{f_n(\phi(t(x_n)))} \equiv \frac{f_i(x)}{f_n(x)}.
\]
Вернёмся к первым интегралам: по определению для всех $i$
\[
    v_i(\phi_1(t(x_n)), \dots, \phi_{n-1}(t(x_n)), x_n) \equiv const.
\]
Обозначая $\tau = t(x_n)$, получаем
\[
    v_i(\phi_1(\tau), \dots, \phi_n(\tau)) \equiv const.
\]
Значит, $v_i$ являются первыми интегралами системы (2).
Проверим их независимость.
Мы знаем, что они независимы в системе (3), тогда векторы
\[
    \left(\frac{\partial v_i}{\partial x_1}(x), \dots, \frac{\partial v_i}{\partial x_{n-1}}(x) \right)
\]
линейно независимы.
Нам нужны $n$-мерные векторы, поэтому добавим к ним ещё одну координату:
\[
    \left(\frac{\partial v_i}{\partial x_1}(x), \dots, \frac{\partial v_i}{\partial x_{n-1}}(x), \frac{\partial v_i}{\partial x_n}(x) \right).
\]
При добавлении новой координаты линейная независимость не ломается, поэтому они подходят в систему (2).
Таким образом, ранг матрицы 
\[
    \left( \frac{\partial v_i}{\partial x_j} \right)_{\substack{i = \overline {1, n - 1} \\ j = 1, n}}
\]
равен $n - 1$, то есть $v_1, \dots, v_{n-1}$ --- искомые первые интегралы.

\QED

\textbf{Замечание.} Зачем: возьмём $n = 2$ и первый интеграл $v_1$.
Тогда кривая $v_1(x) = v_1(x_0)$ является фазовой траекторией.
Аналогично в трёхмерном случае, но тогда будет пересечение поверхностей.

\textbf{Пример.} Рассмотрим систему
\[
    \begin{cases}
        x_1' = -x_2 \\
        x_2' = x_1
    \end{cases}
\]
и её положение равновесия $x_0 = (0, 0)$.
Так как $f(x_0) = (0, 0)$, предположение теоремы нарушается.
Проверим, что следствие теоремы тоже нарушится: от противного, пусть существует первый интеграл $v(x_1, x_2)$ в окрестности $x_0$.
Мы ещё требуем невырожденность, поэтому
\[
    \left( \frac{\partial v}{\partial x_1}(0, 0), \frac{\partial v}{\partial x_2}(0, 0) \right) \ne (0, 0).
\]
Без ограничения общности $\frac{\partial v}{\partial x_1}(0, 0) > 0$, тогда это верно и в некоторой окрестности нуля.
По определению первого интеграла производная в силу системы должна быть тождественным нулём:
\[
    -x_2 \frac{\partial v}{\partial x_1}(x_1, x_2) + x_1 \frac{\partial v}{\partial x_2}(x_1, x_2) \equiv 0.
\]
Возьмём $x_1 = 0$ и $x_2 = \frac{1}{N}$, где $N$ --- какое-то достаточно большое число.
Тогда из доказанного выше $\frac{\partial v}{\partial x_1}(x_1, x_2) > 0$.
Вернёмся к тождеству выше:
\[
    -x_2 \frac{\partial v}{\partial x_1}(x_1, x_2) \equiv 0.
\]
Но мы взяли $x_1$, $x_2$ так, что оба множителя не равны нулю --- противоречие.

\QED

\subsection{Множество всех первых интегралов}
\textbf{Утверждение.} Если у нас есть $n$ первых интегралов $v_1(t, x), \dots, v_n(t, x)$, то любая функция вида $F(v_1(t, x), \dots, v_n(t, x))$ является первым интегралом для $F \in C^1$.

\textbf{Доказательство.} Действительно, пусть $x(\cdot)$ --- это решение системы (1), тогда
\[
    F(v_1(x_1(t)), \dots, v_k(x_n(t))) \equiv F(const_1, \dots, const_n) \equiv const.
\]

\QED

Возникает логичный вопрос: все ли первые интегралы представимы в таком виде, если $v_1, \dots, v_n$ независимы? Ответ положительный.

\textbf{Теорема.} Пусть $D$ --- окрестность точки $(t_0, x_0)$, $v_1, \dots, v_n: D \to \mathbb R$ --- независимые первые интегралы системы (1).
Обозначим $v := (v_1, \dots, v_n): D \to \mathbb R^n$ и $c_0 = v(t_0, x_0)$.
Тогда
\begin{itemize}
    \item Если $x(\cdot)$ --- решение задачи Коши $x' = f(t, x)$, $x(t_0) = x_0$, то $x(\cdot)$ является решением алгебраической системы $v(t_0, x) = c_0$.
    \item Если $\phi(\cdot, c)$ --- это решение алгебраической системы $v(t, x) = c$ и $c$ достаточно близко к $c_0$, то $\phi(\cdot, c)$ --- это решение системы (1).
\end{itemize}
В каком-то смысле дифференциальная система и алгебраическая система на первых интегралах эквивалентны.

\textbf{Доказательство.} Первая часть: для любого $t$
\[
    v(t, x(t)) \equiv (v_i(t, x(t)))_{i=\overline{1, n}} \equiv const = v(t_0, x(t_0)) = c_0,
\]
что и требовалось.

Вторая часть: пусть $v_1, \dots, v_n$ --- независимые первые интегралы, тогда ранг матрицы
\[
    \left( \frac{\partial v_i}{\partial x_j}(t_0, x_0) \right)_{i,j = \overline{1, n}}
\]
равен $n$.
Пусть $x(\cdot)$ --- решение системы (1).
По первому пункту $x$ является решением системы $v(t, x) = v(t_0, x_0)$.
Тогда по теореме о неявной функции $\phi(\cdot, c)$, как второе решение этой системы, совпадает с $x(t)$.
Следовательно, $\phi(t, v(t_0, x(t_0)))$ --- решение системы (1).

(Что здесь происходит...)

\QED


\textbf{Утверждение.} Пусть $D$ --- окрестность $(t_0, x_0)$, $v_1, \dots, v_n: D \to \mathbb R$ --- независимые первые интегралы системы (1), $v$ и $c_0$ из теоремы.
Тогда существует окрестность $D' \subset D$ точки $(t_0, x_0)$, такая что любой первый интеграл $\omega: D' \to \mathbb R$ системы (1) представим в виде $\omega(t, x) = F(v(t, x))$.

\textbf{Доказательство.} Пусть $\phi(t, c)$ --- решение системы $v(t, x) = c$, тогда $\phi(\cdot, c)$ по теореме является решением системы (1).
Пусть $\phi(t, v(t, \xi))$ --- решение системы $v(t, x) = v(t, \xi)$.
Тогда по теореме о неявной функции существует окрестность $D' \subset D$, такая что $x = \xi$ --- единственное решение для всех $x$, достаточно близких к $x_0$, такое что для всех $(t, \xi) \in D'$ выполняется $\phi(t, v(t, \xi)) \equiv \xi$.

Положим $F(c) := \omega(t_0, \phi(t_0, c))$.
По определению первого интеграла $\omega(t, \phi(t, c))$ --- константа по $t$.
Подставим $t_0$: $\omega(t_0, \phi(t_0, c)) = F(c)$.
Теперь подставим $c = v(t, \xi)$: $\omega(t, \xi) \equiv F(v(t, \xi))$ --- ровно искомое тождество.

\QED

Смысл доказательств --- переход от дифференциальных уравнений к алгебраическим и последующее применение теоремы о неявной функции.

\subsection{Множество первых интегралов автономных систем}
Будем доказывать те же теоремы для автономных систем.
Преамбула такая же, как и в пункте 2.

\textbf{Лемма.} Пусть $f_n(x_0) \ne 0$, где $x_0 \in \Omega$, функции $v_1, \dots, v_{n-1}: \Omega \to \mathbb R$ --- независимые первые интегралы системы (2).
Тогда $v_1, \dots, v_{n-1}$ --- независимые первые интегралы системы $\frac{d x_i}{d x_n} = \frac{f_i(x)}{f_n(x)}$.
Это аналог теоремы из пункта 2, но теперь в роли времени выступает $x_n$.

\textbf{Доказательство.} Из определения первого интеграла для любого $j$ выполняется
\[
    \sum_{i=1}^{n} \frac{\partial v_j}{\partial x_i}(x) f_i(x) \equiv 0.
\]
Тогда можно разделить на $f_n(x)$:
\[
    \sum_{i=1}^{n-1} \frac{\partial v_j}{\partial x_i}(x) \frac{f_i(x)}{f_n(x)} + \frac{\partial v_j}{\partial x_n}(x) \equiv 0.
\]
По признаку для неавтономных систем получаем, что $v_j$ являются первыми интегралами системы $\frac{dx_i}{dx_n}$. Докажем их независимость.
Рассмотрим матрицу
\[
    \left( \frac{\partial v_j}{\partial x_i}(x) \right)_{\substack{i = \overline{1, n} \\ j = \overline{1, n - 1}}}.
\]
Её ранг равен $n - 1$ для всех $x$, причём её последняя строка, из доказанного, выражается через первые $n - 1$.
Следовательно, ранг матрицы
\[
    \left( \frac{\partial v_j}{\partial x_i}(x) \right)_{\substack{i = \overline{1, n - 1} \\ j = \overline{1, n - 1}}}
\]
равен $n - 1$, что доказывает независимость первых интегралов.

\QED

\textbf{Теорема.} (О множестве первых интегралов) Пусть $v_1, \dots, v_{n-1}: D \to \mathbb R$ --- независимые первые интегралы автономной системы (2), $x_0 \in D$, $f(x_0) \ne 0$.
Тогда существует окрестность $D' \subset D$ точки $x_0$, такая что для любого первого интеграла системы (2) $\omega: D' \to \mathbb R$ существует функция $F \in C^1$, такая что $\omega(x) \equiv F(v_1(x), \dots, v_{n-1}(x))$.

\textbf{Доказательство.} Без ограничения общности $f_n(x_0) \ne 0$, причём, уменьшая, при необходимости, $D$, это верно на всём $D$.
По лемме $v_1, \dots, v_{n-1}$ является первыми интегралами системы $\frac{dx_i}{dx_n}$, откуда из утверждения для неавтономных систем существует окрестность $D'$ точки $x_0$, такая что выполняется всё, что нужно.

\QED

\section{Оффтоп: ОДУ и случайные графы}
Ответ на вопрос, зачем нам дифференциальные уравнения, если у нас нет физики.
На экзамене не будет.

Везде неявно фиксируется индекс $n$.
Пусть $(\Omega, \mathcal F, P)$ --- вероятностное пространство, $\mathcal F_0 \subset \mathcal F_1 \subset \dots \subset \mathcal F$, где $\mathcal F_0 = \{\varnothing, \Omega\}$, а $\mathcal F_t = \sigma(A_1^t, \dots, A_{m(t)}^t)$, и $\Omega = \bigsqcup_{l=1}^{m(t)} A_l^t$ для всех $t$.
Неформально мы берём множество $\Omega$ и постепенно разбиваем его на более мелкие куски.
Пусть $a = a(n) \in \mathbb N$, $(Y_1(t), \dots, Y_a(t))$ --- случайные $\mathcal F_t$-измеримые величины.
Предположим, что существует $c_0$, такое что для всех $i$ $|Y_i(t)| \le c_0 n$.

\textbf{Теорема 1.} Пусть $D \subset \mathbb R^{a+1}$ --- открытое, связное и ограниченное множество, такое что $(0, Y_1(0)/n, \dots, Y_a(0)/n) \in D$.
Положим 
\[
    T_D = \min \left\{ t \in \mathbb N \cup \{+\infty\}: \left( \frac{t}{n}, \frac{Y_1(t)}{n}, \dots, \frac{Y_a(t)}{n} \right) \not\in D \right\}
\]
--- случайная величина.
Пусть даны функции $f_i: \mathbb R^{a+1} \to \mathbb R$, $f_i \in C^1$, $1 \le i \le a$.
Зафиксируем $\beta > 0$ (не зависит от $n$), $\lambda = \lambda(n) = \mathcal O(1)$.
Предположим, что для любого $t$ и $\omega \in \{t < T_D\}$ величина $\max |Y_i(t+1) - Y_i(t)| \le \beta$, а также для любого $t$, $i = \overline {1, a}$ и $A_l^t \subset \{t < T_D\}$ верно
\[
    \left| E(Y_i(t + 1) - Y_i(t)~|~A_l^t) - f_i \left(\frac{t}{n}, \frac{Y_1(t)}{n}, \dots, \frac{Y_a(t)}{n} \right) \right| \le \lambda.
\]
Тогда:
\begin{itemize}
    \item Задача Коши
        \[
            \begin{cases}
                z_i' = f_i(x, z_1, \dots, z_a) \\
                z_i(0) = \frac{1}{n} Y_i(0)
            \end{cases}
        \]
        имеет единственное решение, продолжаемое до $\partial D$.
        Далее зафиксируем это решение $z_i$.

    \item Существует число $C > 0$, такое что для любых $i$ и $t \le \sigma \cdot n$
        \[
            Y_i(t) = n \cdot z_i \left( \frac{t}{n} \right) + \mathcal O(\lambda n)
        \]
        с вероятностью $1 - \mathcal O \left(\frac{e^{-\frac{n \lambda^3}{\beta^3}}}{\lambda} \right)$, где $\sigma$ --- такое число, что $\rho_{\infty}((x, z(z)), \partial D) \ge C\lambda$ для любого $x \in [0, \sigma]$.
\end{itemize}

\subsection{Процесс минимальных степеней}
Рассмотрим последовательность графов $G_0, G_1, \dots, G_{C_n^2}$, где каждый следующий граф получен следующим образом: берём равновероятно вершину наименьшей степени, равновероятно выбираем другую вершину, в которую нет ребра, и проводим ребро ($G_0$ --- пустой граф).
Обозначим $\Delta(G)$ --- наименьшая степень вершины в $G$ и $Y_i(t)$ --- количество вершин степени $i$ в графе $G_t$.
Положим $f_i(x, z_0, z_1, \dots, z_{n-1}) = -I\{i = 0\} + I\{I = 1\} + z_{i-1} - z_i$.
Рассмотрим задачу Коши
\[
    \begin{cases}
        z_i = -I\{i = 0\} + I\{i = 1\} + z_{i-1} - z_i \\
        z_0(0) = \frac{1}{n} Y_0(0) = 1 \\
        z_i(0) = \frac{1}{n} Y_i(0) = 0
    \end{cases}
\]
(здесь $z_{-1} \equiv 0$)
Решая это уравнение по индукции, получаем решение
\[
    \begin{cases}
        z_0(x) = 2e^{-x} - 1 \\
        z_i(x) = 2 \frac{x^i}{i! e^x}
    \end{cases}.
\]

\textbf{Теорема 2.} 
\[
    Y_i(t) = nz_i \left( \frac{t}{n} \right) + \mathcal O(n^{3/4})
\]
с вероятностью $1 - n^{3/4} \cdot e^{-\frac{n^{1/4}}{8}}$ для любого $n$ достаточно большого, $i < n$ и $t \le n \ln(2) - n^{4/5}$.

\textbf{Доказательство.} Будем подгонять под теорему 1.
Пусть $\Omega$ --- последовательности таких графов, которые мы строим.
Положим 
\[
    \mathcal F_1 = \sigma \{\{\omega: G_0 = \widehat G_0\}~|~\widehat G_0\}.
\]
Теперь 
\[
    \mathcal F_2 = \sigma \{\{\omega: G_0 = \widehat G_0, G_1 = \widehat G_1 \}~|~\widehat G_0, \widehat G_1\}.
\]
И так далее.
Положим для всех $t$ множество $S_t = \{\Delta(G_t) = 0\} \subset \Omega$ и случайную величину $X_i(t)$ --- индикатор того, что степень вершины, в которую проведено новое ребро, равно $i$.
Тогда для $\omega S_t$ верно, что $X_0(t) \omega = 1$ тогда и только тогда, когда новое ребро проведено между вершинами степени 0, а $X_i(t) \omega = 1$ тогда и только тогда, когда мы провели ребро в вершину степени $i$.
Заметим, что для $\omega \in S_t$
\[
    Y_0(t + 1) = Y_0(t) - 1 - X_0(t).
\]
Аналогично можно написать для всех:
\[
    Y_1(t + 1) = Y_1(t) + 1 + X_0(t) - X_1(t),
\]
\[
    Y_{i+1}(t) = Y_i(t) + X_{i-1}(t) - X_i(t).
\]
Найдём математическое ожидание:
\[
    E(Y_0(t + 1) - Y_0(t)~|~\{G_t = \widehat G_t\}) = -1 - \frac{Y_0(t) - 1}{n - 1}.
\]
Справа случайная величина, потому что слева --- тоже, математическое ожидание зависит от $\omega = (\dots, \widehat G_t, \dots)$.
Аналогично
\[
    E(Y_1(t + 1) - Y_1(t)~|~\{G_t = \widehat G_t\}) = 1 + \frac{Y_0(t) - 1}{n - 1} - \frac{Y_1(t)}{n - 1}
\]
и
\[
    E(Y_i(t + 1) - Y_i(t)~|~\{G_t = \widehat G_t\}) = \frac{Y_{i-1}(t) - 1}{n - 1} - \frac{Y_i(t)}{n - 1}.
\]
Положим
\[
    D - \left\{(x, z) \in \mathbb R^{n+1}: -1 < x < \frac{n}{2}, z_0 > 0, -1 < z_i < 2\right\}.
\]
Здесь важно, что все $z_i \in (-1, 2)$, но $z_0 \in (0, 2)$.
Теперь возьмём $a = n$, $c_0 = 1$ и проверим, что предположение теоремы выполняется.
Для вектора $\left( \frac{t}{n}, \frac{Y_0(t)}{n}, \dots, \frac{Y_{n-1}(t)}{n} \right)$ выполняется, что $0 \le \frac{Y_i(t)}{n} \le 1$ и $\frac{t}{n} \ge 0$.
Тогда событие $t < T_D$ --- это ``существует вершина степени ноль в графах $G_0, \dots, G_t$``, то есть $S_t$.
Нужно ограничение на разность $|Y_i(t + 1) - Y_i(t)|$.
Его получить нетрудно, ибо при проведении ребра количество вершин степени $i$ изменилось не более, чем на 2 --- положим $\beta := 2$.
Оценим разность
\[
    \left| E(Y_i(t + 1) - Y_i(t)~|~\{G_t = \widehat G_t\}) - f_i \left( \frac{t}{n}, \frac{Y_0(t)}{n}, \dots, \frac{Y_{n-1}(t)}{n} \right) \right| = 
\]
\[
    = \left| \frac{Y_{i-1}(t) - Y_i(t)}{n - 1} - \frac{Y_{i-1}(t) - Y_i(t)}{n} \right| = \frac{|Y_{i-1}(t) - Y_i(t)|}{n(n-1)} \le
\]
\[
    \le \frac{|Y_{i-1}(t)| + |Y_i(t)|}{n(n - 1)} \le \frac{1}{n-1} \le n^{-1/4} =: \lambda.
\]
Все условия выполнены, поэтому по второму следствию теоремы получаем, что найдётся $c > 0$, такое что и так далее.
Заметим, что $z_i(x) \in [0, 1]$ при $x \ge 0$, а $z_0(x) \in (0, 1]$ при $x \in [0, \ln(2))$ и $z_0(\ln(2)) = 0$.
Засчёт гениального подгона условий во множестве $D$, мы получаем, что $\dist_\infty((x, z(x)), \partial F) = z_0(x)$, так как по остальным координатам есть большой запас.
В то же время $z_0(\ln(2) - n^{-1/5}) \approx n^{-1/5} >> cn^{-1/4} = c\lambda$.
Теперь берём $\sigma = \ln(2) - n^{-1/5}$ и получаем что-то интересное.

\QED

\setcounter{equation}{0}
\section{Оффтоп: производящие функции}
\subsection{Линейные рекуррентные соотношения}
Рассмотрим рекуррентное соотношение
\begin{equation}
    a_0 u_{n+k} + a_1 u_{n+k-1} + \dots + a_n u_k = 0.
\end{equation}
Её решением является какая-то последовательность $\{u_k\}_{k=0}^\infty$.
Также будем считать, что нам известны первые $n - 1$ членов $\widehat u_0, \dots, \widehat u_{n-1}$ --- очень похоже на задачу Коши.
Рассмотрим экспоненциальную производящую функцию
\[
    x(t) = \sum_{j=0}^{\infty} \frac{u_j t^j}{j!}.
\]
Её производная ---
\[
    x'(t) = \sum_{j=0}^{\infty} \frac{u_{j+1} t^j}{j!}.
\]
В общем случае
\[
    x^{(n)}(t) = \sum_{j=0}^{\infty} \frac{u_{j+n} t^j}{j!}.
\]
Рассмотрим линейную комбинацию $a_0 x^{(n)} + \dots + a_{n-1} x' + a_n x$.
Она равна
\[
    \sum_{j=0}^{\infty} \frac{t^j}{j!} (a_0 u_{j+n} + \dots + a_{n-1} u_{j+1} + a_n u_j).
\]

\textbf{Теорема.} Пусть $x(\cdot)$ --- это решение задачи Коши
\[
    \begin{cases}
        a_0 x^{(n)} + \dots + a_{n-1} x' + a_n x = 0 \\
        x(0) = \widehat u_0, \dots, x^{(n-1)}(0) = \widehat u_{n-1}
    \end{cases}.
\]
Тогда $\{u_k\}$ является решением (1), они берутся из разложения $x(t)$ в ряд.

\subsection{Числа Стирлинга второго рода}
\textbf{Определение.} Пусть $n \ge k$. \textit{Числом Стирлинга второго рода} $S(n, k)$ называется количество неупорядоченных разбиений $n$-элементного множества на $k$ неупорядоченных подмножеств.

В частности, $S(n, 0) = 0$ при $n > 0$, $S(0, 0) = 1$ и $S(n, k) = S(n - 1, k - 1) + k \cdot S(n - 1, k)$.
Более того,
\[
    S(n, k) = \frac{1}{k!} \sum_{j=0}^{k} (-1)^{k+1} C_k^j \cdot j^n.
\]
Рассмотрим экспоненциальный степенной ряд
\[
    x_k(t) = \sum_{n=k}^{\infty} S(n, k) \frac{t^n}{n!}.
\]
Используя рекурренту, получаем
\[
    x_k(t) = \sum_{n=k}^{\infty} (S(n - 1, k - 1) + k \cdot S(n - 1, k)) \cdot \frac{t^n}{n!}.
\]
Формально продифференцируем:
\[
    x'_k(t) = \sum_{n=k}^{\infty} S(n - 1, k - 1) \cdot \frac{t^{n-1}}{(n-1)!} + k \cdot \sum_{n=k}^{\infty} S(n - 1, k) \cdot \frac{t^{n-1}}{(n-1)!}.
\]
Во второй сумме получилась неприятность в виде того, что нам нужно $S(n - 1, n)$, а мы такое не определяли, поэтому доопределим нулём.
Методом пристального взгляда заключаем, что $x'_k(t) = x_{k-1}(t) + k \cdot x_k(t)$.
Остаётся найти $x_0(t) = \sum_{n = 0}^{\infty} S(n, 0) \cdot \frac{t^n}{n!} = 1$, и мы научились находить $x_k$ решением задачи Коши с добавлением условия $x_k(0) = 0$.
Можно проверить, что $x_k(t) = \frac{1}{k!} (e^t - 1)^k$.

\setcounter{equation}{0}
\section{Линейные однородные уравнения в частных производных}
Пусть $\Omega \subset \mathbb R^n$ --- область, $a: \Omega \to \mathbb R^n$ --- вектор-функция, $a \in C^1$.
Рассмотрим уравнение
\begin{equation}
    a_1(x) \cdot \frac{\partial u}{\partial x_1}(x) + \dots + a_n(x) \cdot \frac{\partial u}{\partial x_n}(x) = 0.
\end{equation}
Его решением является функция $u: D \to \mathbb R$, где $D \subset \Omega$ --- область и $u \in C^1$, при подстановке которой получается тождественный ноль.
В сокращённой записи
\[
    \left< a(x), \frac{\partial u}{\partial x}(x) \right> = 0.
\]

\textbf{Определение.} Такое уравнение называется \textit{линейным однородным уравнением в частных производных первого порядка}.

\textbf{Определение.} Система
\begin{equation}
    x' = a(x)
\end{equation}
называется \textit{характеристической системой} уравнения.

Найдём связь между решениями уравнения (1) и его характеристической системы (2).
Пусть $\overline x \in \Omega$ --- какая-то точка, причём $a(\overline x) \ne 0$.

\textbf{Теорема.} 1) Любой первый интеграл системы (2) является решением системы (1).

2) Пусть $v_1, \dots, v_{n-1}: \Omega \to \mathbb R$ --- независимые первые интегралы системы (2).
Тогда существует окрестность $D$ точки $\overline x$, такая что для любого решения $u: D \to \mathbb R$ уравнения (1) существует гладкая функция $F$, такая что $u(x) \equiv F(v_1(x), \dots, v_{n-1}(x))$.

\textbf{Доказательство.} 1) Пусть $u(\cdot)$ --- первый интеграл (2).
По критерию первого интеграла $\left<a(x), \frac{\partial u}{\partial x}(x) \right> \equiv 0$, что мы и хотели.

2) По теореме о первом интеграле существует окрестность $D$, такая что любой первый интеграл в ней представим в виде $F(v_1(x), \dots, v_{n-1}(x))$.
Рассмотрим произвольное решение $u: D \to \mathbb R$ уравнения (1).
Раз решение, то $\left<a(x), \frac{\partial u}{\partial x}(x) \right> \equiv 0$, а значит, $u(\cdot)$ --- первый интеграл системы (2), то есть имеет искомое представление в окрестности $D$.

\QED

Пусть заданы гладкие функции $g, \phi: \Omega \to \mathbb R$, причём $\frac{\partial g}{\partial x}(x) \ne 0$ на $\Omega$, и $g(\overline x) = 0$.
Тогда существует непустое множество $\gamma := \{x: g(x) = 0\}$, более того, это $(n-1)$--мерная поверхность.
Рассмотрим задачу Коши
\begin{equation}
    \begin{cases}
        \left<a(x), \frac{\partial u}{\partial x}(x) \right> \equiv 0 \\
        u(x) = \phi(x) \text{ при $x \in \gamma$}
    \end{cases}.
\end{equation}
Как и у любой уважающей себя задачи Коши, для неё есть теорема о существовании и единственности решения, но это чуть позже.

\textbf{Определение.} $\overline x$ называется \textit{характеристической точкой} задачи (3), если $\left<a(\overline x), \frac{\partial u}{\partial x}(\overline x) \right> = 0$.

\textbf{Теорема.} Пусть точка $\overline x$ не является характеристической.
Тогда существует окрестность $D$ точки $\overline x$ и функция $u: D \to \mathbb R$, такая что $u$ является единственным решением (3) в этой окрестности.

\textbf{Доказательство.} Мы знаем, что $a(\overline x) \ne 0$.
Поэтому можно применить теорему о первых интегралах: существует область $\widetilde D$ точки $\overline x$, в которой есть независимые первые интегралы $v_1, \dots, v_{n-1}: \widetilde D \to \mathbb R$ системы (2).
Рассмотрим систему
\[
    \begin{cases}
        v_1(x) = u_1 \\
        \vdots \\
        v_{n-1}(x) = u_{n-1} \\
        g(x) = \Theta
    \end{cases}.
\]
(здесь все значения в правых частях --- это параметры)
Хотим применить теорему об обратной функции, проверим, что условия выполнены.
Для этого рассмотрим матрицу Якоби:
\[
    A = 
    \begin{pmatrix}
        \frac{\partial v_1}{\partial x}(\overline x) \\
        \vdots \\
        \frac{\partial v_{n-1}}{\partial x}(\overline x) \\
        \frac{\partial g}{\partial x}(\overline x) \\
    \end{pmatrix}
\]
Докажем от противного, что она невырождена: пусть
\[
    \frac{\partial g}{\partial x}(\overline x) = \sum_{j=1}^{n-1} \lambda_j \frac{\partial v_j}{\partial x}(\overline x).
\]
Это рассматривать достаточно, так как первые $n - 1$ строк точно линейно независимы.
Тогда
\[
    \left< a(\overline x), \frac{\partial g}{\partial x}(\overline x) \right> = \sum_{j=1}^{n-1} \lambda_j \left< a(\overline x), \frac{\partial v_j}{\partial x}(\overline x) \right> = 0,
\]
так как это первые интегралы.
Противоречие с тем, что $\overline x$ не является характеристической точкой.

Теперь по теореме об обратной функции найдётся окрестность $\Gamma$ точки $(v_1(\overline x), \dots, v_{n-1}(\overline x), 0)$ (в конце ноль, так как $g(\overline x) = 0$) и $\chi: \Gamma \to \mathbb R^n$, $\chi \in C^1$, такие что
\[
    \begin{cases}
        v_1(\chi(u_1, \dots, u_{n-1}, \Theta)) = u_1 \\
        \vdots \\
        v_{n-1}(\chi(u_1, \dots, u_{n-1}, \Theta)) = u_{n-1} \\
        g(\chi(u_1, \dots, u_{n-1}, \Theta)) = \Theta \\
    \end{cases}
\]
и
\[
    \chi(v_1(x), \dots, v_{n-1}(x), g(x)) \equiv x.
\]
Тогда $\chi(u_1, \dots, u_{n-1}, \Theta)$ является единственным решением построенной системы, причём $\chi(v_1(\overline x), \dots, v_{n-1}(\overline x), 0) = \overline x$.

Теперь восстановим единственное решение задачи Коши.
Положим $u(x) := \phi(\chi(v_1(x), \dots, v_{n-1}(x), 0))$.
Возьмём достаточно малую окрестность $D$ точки $\overline x$, такую что для всех $x \in D$ выполнено $(v_1(x), \dots, v_{n-1}(x), 0) \in \Gamma$.

Почему это решение уравнения в частных производных? Потому что взяли гладкую функцию от первых интегралов.
\sloppy Почему это решение задачи Коши? При $x \in \gamma$ выполнено $g(x) = 0$, то есть $u(x) = \phi(\chi(v_1(x), \dots, v_{n-1}(x), g(x))) = \phi(x)$, так как обратная функция.

Единственность остаётся в качестве упражнения.
Доказательство от автора конспекта: рассмотрим произвольное решение $F(v_1, \dots, v_{n-1})$ и точку $x_1 \in D$.
Положим $x_2 = \chi(v_1(x_1), \dots, v_{n-1}(x_1), 0) \in \gamma$.
Заметим, что $v_i(x_2) = v_i(x_1)$ для всех $i$, так как $\chi$ --- обратная к отображению $x \mapsto (v_1(x), \dots, v_{n-1}(x), g(x))$.
Следовательно, $F(v_1(x_1), \dots, v_{n-1}(x_1)) = F(v_2(x_2), \dots, v_{n-1}(x_2)) = \phi(x_2)$, то есть $F(v_1, \dots, v_{n-1})$ однозначно определено в $x_1$.

\QED

\section{Вариационное исчисление}
\subsection{Простейшая задача вариационного исчисления}
Рассмотрим пространство функций $C^1[a, b]$, как нормированное пространство, и его подмножество $M$.
Зададим на нём метрику $\rho(x_1, x_2) = \max_{t \in [a, b]} |x_1(t) - x_2(t)|$ и $\rho_1(x_1, x_2) = \rho(x_1, x_2) + \rho(x_1', x_2')$.
Пусть у нас есть функционал $I: M \to\ \mathbb R$.

\textbf{Определение.} Точка $\widehat x \in M$ называется \textit{слабым локальным минимумом} функционала $I$, если $\exists \varepsilon > 0: \forall x \in M~(\rho_1(x, \widehat x) < \varepsilon \Rightarrow I(\widehat x) \le I(x))$
Аналогично для максимума.

\textbf{Определение.} Точка $\widehat x \in M$ называется \textit{сильным локальным минимумом}, если вместо $\rho_1$ используется $\rho$.

\textbf{Утверждение.} Если $\widehat x$ --- сильный локальный минимум, то он также является слабым. Очевидно.

Рассмотрим дважды гладко дифференцируемую (в $C^2$) функцию $F: \mathbb R^3 \to \mathbb R$ и числа $A, B \in \mathbb R$.
Положим
\[
    M = \{x \in C^1[a, b]: x(a) = A, x(b) = B\}
\]
и
\[
    I(x) := \int_a^b F(t, x(t), x'(t)) dt, x \in M.
\]

\textbf{Определение.} \textit{Простейшей задачей вариационного исчисления} называется задача нахождения слабых локальных экстремумов функционала $I$.

Положим
\[
    \mathring C^1[a, b] := \{x \in C^1[a, b]: x(a) = x(b) = 0\}.
\]
Тогда множество $M$ замкнуто относительно прибавления функций из $\mathring C^1[a, b]$.

Положим для $\widehat x \in M$, $\widehat x \in C^2$, $\eta \in \mathring C^1[a, b]$ функцию
\[
    \phi(\mu) := I(\widehat x + \mu \eta) = \int_a^b F(t, \widehat x(t) + \mu \eta(t), \widehat x'(t) + \mu \eta'(t)) dt.
\]
Продифференцируем её:
\[
    \phi'(\mu)|_{\mu = 0} = \int_a^b \left( \frac{\partial F}{\partial x}(t, \widehat x(t), \widehat x'(t)) \eta(t) + \frac{\partial F}{\partial x'}(t, \widehat x(t), \widehat x'(t)) \eta'(t) \right) dt =
\]
Проинтегрируем по частям:
\[
    = \int_a^b \frac{\partial F}{\partial x}(t, \widehat x(t), \widehat x'(t)) \eta(t) dt + \frac{\partial F}{\partial x'}(\dots) \eta(t) \big|_a^b - \int_a^b \frac{d}{dt} \frac{\partial F}{\partial x'}(\dots) \eta(t) dt =
\]
Второе слагаемое рано нулю, так как $\eta(a) = \eta(b) = 0$
\[
    = \int_a^b \left(\frac{\partial F}{\partial x}(\dots) - \frac{d}{dt} \frac{\partial F}{\partial x'}(\dots) \right) \eta(t) dt.
\]
Таким образом, если $\widehat x$ является слабым локальным минимумом, то 0 --- стационарная точка функции $\phi$.

\textbf{Определение.} $\delta I[\widehat x, \eta] := \phi'(0)$ --- первая вариация функционала $I$ на $\widehat x$.

\textbf{Утверждение.} Если $\widehat x \in M$ --- слабый локальный экстремум, то для любого $\eta \in \mathring C^1[a, b]$ точка $0$ является локальным экстремумом функции $\phi$.

\textbf{Доказательство.} Будем считать, что мы работаем с точкой минимума.
По определению существует $\varepsilon > 0$, такое что для любого $x \in M$, удовлетворяющему $\rho_1(x, \widehat x) < \varepsilon$ верно $I(x) \ge I(\widehat x)$.

Тогда для любого $\eta \in \mathring C^1[a, b]$, не равного тождественному нулю, положим $\delta = \frac{\varepsilon}{\rho_1(\eta, 0)}$.
Возьмём произвольный $\mu \in (-\delta, \delta)$.
Имеем
\[
    \rho_1(\widehat x + \mu \eta, \widehat x) = \max_{t \in [a, b]} |\mu \eta(t)| + \max_{t \in [a, b]} |\mu \eta'(t)| =
\]
\[
    = |\mu| \left( \max_{t \in [a, b]} |\eta(t)| + \max_{t \in [a, b]} |\eta'(t)| \right) = |\mu| \cdot \rho_1(\eta, 0) < \varepsilon.
\]
Таким образом, мы попали в $\varepsilon$-окрестность функции $\widehat x$, то есть $\phi(\mu) = I(\widehat x + \mu \eta) \ge I(\widehat x) = \phi(0)$.

\QED

\textbf{Утверждение.} (Лемма Лагранжа) Пусть $v \in C[a, b]$, такая что $\forall \eta \in \mathring C^1[a, b]$ выполнено
\[
     \int_a^b v(t) \eta(t) dt = 0.
\]
Тогда $v(t) \equiv 0$.

\textbf{Доказательство.} От противного: допустим, что существует $\tilde \tau \in [a, b]$, такое что $v(\tilde \tau) > 0$.
Тогда существует $\tau \in (a, b)$, такое что $v(\tau) > 0$ из непрерывности.
Отсюда существует $\varepsilon > 0$, такой что $(\tau - \varepsilon, \tau + \varepsilon) \subset [a, b]$ и $v(t) > \frac{v(\tau)}{2}$ для $t \in (\tau - \varepsilon, \tau + \varepsilon)$.

Теперь построим гладкую функцию, принимающую положительные значения на $T := (\tau - \varepsilon, \tau + \varepsilon)$ и ноль вне этого интервала.
В частности,
\[
    \eta(t) :=
    \begin{cases}
        (t - (\tau - \varepsilon))^2 (t - (\tau + \varepsilon))^2, & t \in T \\
        0, & \text{иначе}
    \end{cases}.
\]
Отсюда по условию
\[
    0 = \int_a^b v(t) \eta(t) dt = \int_T v(t) \eta(t) dt.
\]
Противоречие, так как мы взяли интеграл по непустому интервалу произведения двух положительных функций.

\QED

\textbf{Теорема.} Пусть $F \in C^2$, $\widehat x \in M$, $\widehat x \in C^2$ --- слабый локальный экстремум.
Тогда $\widehat x$ является решением уравнения Эйлера
\[
    \frac{\partial F}{\partial x} (t, x, x') - \frac{d}{dt} \frac{\partial F}{\partial x'} (t, x, x') = 0.
\]

\textbf{Доказательство.} Поскольку $\widehat x$ является слабым локальным экстремумом, по утверждению для любой $\eta \in \mathring C^1[a, b]$ точка $0$ является локальным экстремумом функции $\phi$, то есть $\phi'(0) = 0$.
Выражение для $\phi'(0)$ мы уже писали выше --- теперь заметим, что по утверждению про локальный экстремум $\phi$ получаем $\phi'(0) = 0$, а по лемме Лагранжа ---
\[
    \frac{\partial F}{\partial x} (t, \widehat x, \widehat x'(t)) - \frac{d}{dt} \frac{\partial F}{\partial x'} (t, \widehat x, \widehat x'(t)) \equiv 0.
\]
Следовательно, $\widehat x$ является решением уравнения Эйлера.

\QED

\textbf{Замечание.} Повсюду мы говорили, что $\widehat x \in C^2$.
Но теоретически экстремумом может являться и функция из $C^1$.
Пусть $F, \widehat x \in C^1$.
Если $\widehat x$ --- слабый локальный экстремум, то функция
\[
    t \mapsto \frac{\partial F}{\partial x'} (t, \widehat x(t), \widehat x'(t))
\]
непрерывно дифференцируема, и $\widehat x$ является решением уравнения Эйлера.
Иными словами, прошлая теорема верна и в этом случае, но доказывать мы это не будем.

\textbf{Определение.} Решение уравнения Эйлера называется \textit{экстремальным}.
Тогда прошлую теорему можно переформулировать, как ``слабый локальный экстремум является экстремальным``.

\subsection{Задача о брахистохроне}
Людям с острой непереносимостью физики рекомендуется пропустить.
Остальным: для понимания достаточно школьных знаний.

Пусть у нас есть две материальные точки $A$ и $B$, причём $A$ выше $B$.
Мы хотим провести между ними кривую, такую что материальная точка, двигаясь по ней исключительно под силой тяжести, достигнет точку $B$ за минимальное время.
Эта кривая называется \textit{брахистрохоной}.

\begin{figure}[ht]
    \centering
    \incfig{811}{0.5\linewidth}
\end{figure}

Запишем закон сохранения энергии:
\[
    mg \cdot y(x) = \frac{m v^2(x)}{2}.
\]
Тогда
\[
    v(x) = \sqrt {2g \cdot y(x)}.
\]
Запишем скорость, как производную от пройденного пути $s$:
\[
    v(x) = \frac{ds}{dt} = \frac{ds}{dx} \cdot \frac{dx}{dt} = \frac{d}{dx} \int_0^x \sqrt{ 1 + (y'(\xi))^2 } d\xi \cdot \frac{dx}{dt} = \sqrt{1 + (y'(x))^2} \cdot \frac{dx}{dt}.
\]
Выразим $dt$:
\[
    dt = \frac{\sqrt{1 + (y'(x))^2}}{\sqrt{2g \cdot y(x)}} dx,
\]
то есть
\[
    t = \int_0^b \sqrt{ \frac{1 + (y'(x))^2}{2g \cdot y(x)}} \cdot dx.
\]
Итак, итак, простейшая вариационная задача.
Выкинем лишние константы:
\[
    t(y) = \int_0^b \sqrt{ \frac{1 + (y')^2}{y}} dx \to \min.
\]
Здесь $y(0) = 0$, $y(b) = B$.
Уравнением Эйлера будем
\[
    \sqrt{1 + (y')^2} \left( -\frac{1}{2} \cdot \frac{1}{(\sqrt y)^3} \right) - \frac{d}{dx} \cdot \frac{2y'}{\sqrt y \cdot 2 \cdot \sqrt{1 + (y')^2}} = 0.
\]
Заметим, что это то же самое, что
\[
    \frac{d}{dx} \left( \sqrt{\frac{1 + (y')^2}{y}} - \frac{(y')^2}{\sqrt{y (1 + (y')^2)}} \right) = 0.
\]
То есть $y(y + (y')^2) = c_1$ --- константа.
Сделаем замену: $y'(x(\tau)) = \ctg(\tau)$.
Тогда 
\[
    y(x(\tau)) = c_1 \sin^2(\tau) = \frac{1}{2} c_1 (1 - \cos(2\tau)).
\]
Теперь
\[
    dx = \frac{dy}{y'} = \frac{2c_1 \sin(\tau) \cos(\tau)}{\ctg(\tau)} d\tau = c_1(1 - \cos(2\tau)) d\tau.
\]
Значит,
\[
    x(\tau) = c_2 + \frac{c_1}{2} (2\tau - \sin(2\tau)).
\]
Теперь остаётся проверить, какие из них являются экстремумами, делается напрямую.

\setcounter{equation}{0}
\subsection{Задача со свободным концом}
Пусть $F: \mathbb R^3 \to \mathbb R \in C^2$, числа $a, b, A \in \mathbb R$ фиксированы.
Рассмотрим функционал
\begin{equation}
    I(x) = \int_a^b F(t, x(t), x'(t)) dt
\end{equation}
при условии $x(a) = A$.

Мы хотим найти экстремумы $I: M \to \mathbb R$, где $M = \{x \in C^1[a, b]: x(a) = A\}$.

\textbf{Теорема.} Пусть $\widehat x \in M$, $\widehat x \in C^2$ --- решение (1), то есть слабый локальный экстремум $I$.
Тогда $\widehat x$ является решением уравнения Эйлера
\[
    \frac{\partial F}{\partial x}(t, x, x') - \frac{d}{dt} \frac{\partial F}{\partial x'}(t, x, x') = 0,
\]
а также
\begin{equation}
    \frac{\partial F}{\partial x'}(b, \widehat x(b), \widehat x'(b)) = 0.
\end{equation}

\textbf{Доказательство.} Зафиксируем допустимое приращение $\eta \in C^1[a, b]$, $\eta(a) = 0$.
Положим
\[
    \Phi(\alpha) := I(\widehat x + \alpha \eta) = \int_a^b F(t, \widehat x(t) + \alpha \eta(t), \widehat x'(t) + \alpha \eta'(t)) dt.
\]
Найдём производную в нуле:
\[
    \Phi'(0) = \int_a^b \left( \frac{\partial F}{\partial x}(t, \widehat x(t), \widehat x'(t)) \eta(t) + \frac{\partial F}{\partial x'} (t, \widehat x(t), \widehat x'(t)) \eta'(t) \right) dt =
\]
Проинтегрируем по частям
\[
    = \int_a^b \frac{\partial F}{\partial x}(\dots)\eta(t) dt + \frac{\partial F}{\partial x'} (t, \widehat x(t), \widehat x'(t)) \eta(t) \bigg|_{t=a}^{t=b} - \int_a^b \frac{d}{dt} \frac{\partial F}{\partial x'}(\dots) \eta(t) dt =
\]
\[
    = \int_a^b \left( \frac{\partial F}{\partial x}(\dots) - \frac{d}{dt} \frac{\partial F}{\partial x'}(\dots) \right) \eta(t) dt + \frac{\partial F}{\partial x'}(b, \widehat x(b), \widehat x'(b)) \eta(b),
\]
так как $\eta(a) = 0$.

Как доказывалось в простейшей задаче вариационного исчисления, $0$ является локальным экстремумом функции $\Phi$, то есть $\Phi'(0) = 0$.
Таким образом, выражение выше равно нулю.

Подставим в выражение выше функцию $\eta$ с $\eta(b) = 0$, тогда останется только
\[
    \int_a^b \left( \frac{\partial F}{\partial x}(t, \widehat x(t), \widehat x'(t)) - \frac{d}{dt} \frac{\partial F}{\partial x'}(t, \widehat x(t), \widehat x'(t)) \right) \eta(t) dt = 0.
\]
По лемме Лагранжа получаем уравнение Эйлера.
Теперь остаётся только 
\[
    \frac{\partial F}{\partial x'} (b, \widehat x(b), \widehat x'(b)) \eta(b) = 0
\]
для всех функций $\eta$, то есть
\[
    \frac{\partial F}{\partial x'} (b, \widehat x(b), \widehat x'(b))  \equiv 0.
\]

\QED

\textbf{Замечание.} Опять же если $F, \widehat x \in C^1$, то функция
\[
    \frac{\partial F}{\partial x'}(t, \widehat x(t), \widehat x'(t))
\]
непрерывно дифференцируема по $t$, $\widehat x$ является решением уравнения Эйлера и выполняется (2).

\textbf{Замечание 2.} Можно рассматривать и задачу с другим свободным концом, тогда (2) будет иметь вид
\[
    \frac{\partial F}{\partial x'}(a, \widehat x(a), \widehat x'(a)) = 0.
\]
А если оба конца свободны, то условие выше и условие (2) выполняются одновременно.

\subsection{Задача для функционалов, зависящих от нескольких функций}
Пусть у нас есть функция $F: \mathbb R \times \mathbb R^n \times \mathbb R^n \to \mathbb R \in C^2$, заданы числа $a, b \in \mathbb R$ и $A, B \in \mathbb R^n$, где $A = (A_i)_{i = \overline{1, n}}$ и $B = (B_i)_{i = \overline{1, n}}$.

Рассмотрим задачу нахождения экстремумов функционала
\begin{equation}
    I(x) = \int_a^b F(t, x(t), x'(t)) dt,
\end{equation}
где $I: M \to \mathbb R$ для $M = \{x \in C^1([a, b], \mathbb R^n)~|~x(a) = A, x(b) = B\}$.
Мы будем искать слабый локальный минимум/максимум по метрике
\[
    \rho_1(x, u) = \max_{a \le t \le b} |x(t) - u(t)| + \max_{a \le t \le b}|x'(t) - u'(t)|.
\]

\textbf{Теорема.} Пусть $\widehat x \in M$, $\widehat x \in C^2$ --- решение (3), то есть слабый локальный экстремум $I$.
Тогда $\widehat x$ является решением уравнения Эйлера
\[
    \frac{\partial F}{\partial x_i}(t, x, x') - \frac{d}{dt} \frac{\partial F}{\partial x_i'}(t, x, x') = 0
\]
для всех $i = \overline{1, n}$.

\textbf{Доказательство.} Можно сделать те же самые рассуждения с леммой Лагранжа, как и в двух предыдущих случаях, но можно доказать проще с использованием уже полученных результатов.

Положим
\[
    M_1 := \{x_1 \in C^1[a, b]: x_a(a) = A_1, x_1(b) = B_1\}.
\]
и
\[
    I_1(x_1) = \int_a^b F(t, x_1(t), \widehat x_2(t), \dots, \widehat x_n(t), x_1'(t), \widehat x_2'(t), \dots, \widehat x_n'(t)) dt.
\]
Так как $\widehat x$ является решением (3), $\widehat x_1$ является решением задачи нахождения экстремума $I_1(x_1)$, так как нужно внимательно посмотреть на то, что получается при подстановке.

Следовательно, по теореме для простейшей задачи вариационного исчисления
\[
    \frac{\partial F}{\partial x_1}(t, \widehat x_1(t), \dots, \widehat x_n(t), \widehat x_1'(t), \dots, \widehat x_n'(t)) -
\]
\[
    - \frac{d}{dt} \frac{\partial F}{\partial x_1'}(t, \widehat x_1(t), \dots, \widehat x_n(t), \widehat x_1'(t), \dots, \widehat x_n'(t)) \equiv 0.
\]
Теперь аналогично доказываем для $x_2, \dots, x_n$.

\QED

\subsection{Функционалы, содержащие производные высших порядков}
Пусть у нас есть $F: \mathbb R^{n + 2} \to \mathbb R$, $F \in C^{n+1}$, а также числа $a, b, A_i, B_i \in \mathbb R$ для $i = \overline{0, n - 1}$.
Рассмотрим функционал
\begin{equation}
    I(x) = \int_a^b F(t, x(t), x'(t), \dots, x^{(n)}(t)) dt.
\end{equation}
при условиях $x^{(i)}(a) = A_i$ и $x^{(i)}(b) = B_i$ для всех $i$.
Как обычно, положим
\[
    M = \{x \in C^n[a, b]: x^{(i)}(a) = A_i, x^{(i)}(b) = B_i \text{ для всех $i$}\}.
\]
Положим метрику
\[
    \rho_n(x, u) = \sum_{i=0}^{n} \rho(x^{(i)}, u^{(i)}).
\]
Опять же хотим найти слабый локальный минимум.

Введём множество допустимых вариаций:
\[
    \mathring C^n[a, b] = \{\eta \in C^n[a, b]: \eta^{(i)}(a) = \eta^{(i)}(b) = 0 \text{ для всех $i$}\}.
\]
Возьмём произвольную допустимую вариацию $\eta \in \mathring C^n[a, b]$, $\widehat x \in C^{2n}$ и положим
\[
    \Phi(\alpha) = I(\widehat x + \alpha \eta) = \int_a^b F(t, \widehat x(t) + \alpha \eta(t), \dots, \widehat x^{(n)}(t) + \alpha \eta^{(n)}(t)) dt.
\]
Дифференцируем по параметру в нуле:
\[
    \Phi'(0) = \int_a^b \sum_{i=0}^{n} \frac{\partial F}{\partial x^{(i)}} (t, \widehat x(t), \dots, \widehat x^{(n)}(t)) \eta^{(i)}(t) dt =
\]
Интегрируем, как обычно, по частям всё, кроме первого слагаемого, и сразу, как и раньше, сокращаем нули
\[
    = \int_a^b \frac{\partial F}{\partial x}(\dots) \eta(t) dt - \int_a^b \sum_{i=1}^{n} \frac{d}{dt} \frac{\partial F}{\partial x^{(i)}}(\dots) \eta^{(i-1)}(t) dt =
\]
Отправим первое слагаемое суммы в первое слагаемое всего выражения, а остаток проинтегрируем по частям
\[
    = \int_a^b \left(\frac{\partial F}{\partial x}(\dots) - \frac{d}{dt} \frac{\partial F}{\partial x^{(1)}}(\dots) \right) \eta(t) dt + \sum_{i=2}^{n} \frac{d^2}{dt^2} \frac{\partial F}{\partial x^{(i)}}(\dots) \eta^{i-2}(t) dt =
\]
Делаем то же самое:
\[
    = \int_a^b \left( \frac{\partial F}{\partial x} (\dots) - \frac{d}{dt} \frac{\partial F}{\partial x^{(1)}} (\dots) + \frac{d^2}{dt^2} \frac{\partial F}{\partial x^{(2)}}(\dots) \right) \eta(t) dt + \dots =
\]
По методу неполной индукции получаем, что это всё равняется
\[
    \int_a^b \left( \sum_{i=0}^{n} (-1)^i \frac{d^i}{dt^i} \frac{\partial F}{\partial x^{(i)}} (\dots) \right) \eta(t) dt.
\]

\textbf{Замечание.} Если посмотреть на $n$-ое слагаемое полученной суммы, то можно увидеть, почему условия на непрерывную дифференцируемость функций именно такие.

\textbf{Лемма.} (Лагранжа) Пусть $f \in C[a, b]$ и $\int_a^b f(t) \eta(t) dt = 0$ для всех $\eta \in \mathring C^n[a, b]$.
Тогда $f(t) \equiv 0$.

\textbf{Доказательство.} Всё так же, как и в одномерном случае.
Точная формула для функции:
\[
    \eta(t) =
    \begin{cases}
        (t - (\tau + \varepsilon))^{2n} (t - (\tau - (\tau - \varepsilon))^{2n}, & t \in (\tau - \varepsilon, \tau + \varepsilon) \\
        0, & \text{иначе}
    \end{cases} .
\]
Как альтернатива, можно использовать функцию пенёк из 3 семестра.

\QED

\textbf{Теорема.} Пусть $F \in C^{n+1}$, $\widehat x \in M$ --- слабый локальный экстремум, причём $\widehat x \in C^{2n}$.
Тогда $\widehat x$ является решением уравнения Эйлера, которое в этом случае имеет вид
\[
    \frac{\partial F}{\partial x}(t, x, x', \dots, x^{(n)}) - \frac{d}{dt} \frac{\partial F}{\partial x'} (t, x, x', \dots, x^{(n)}) + \frac{d^2}{dt^2} \frac{\partial F}{\partial x''}(\dots) + \dots +
\]
\[
    + (-1)^n \frac{d^n}{dt^n} \frac{\partial F}{\partial x^{(n)}}(\dots) \equiv 0.
\]

\textbf{Доказательство.} Ничего не меняется. Если $\widehat x$ --- слабый локальный экстремум, то $0$ --- локальный экстремум функции $\Phi$, то есть $\Phi'(0)$, откуда по равенству, полученному выше, и лемме Лагранжа получаем искомое.

\QED

\textbf{Замечание.} И то же самое замечание: достаточно $C^n$ для всех функций.

\section{Пропущенная лекция}
Если я не ошибаюсь, в экзамене её не будет.

\section{Приложения в социологии}
\subsection{Предсказание популяции в вакууме}
Пусть $x(t)$ --- численность популяции в момент времени $t$, $k$ --- некоторое постоянное число, \textit{поддерживающая ёмкость среды}, то есть максимальное число людей, существование которых может поддержать среда, $r$ --- скорость размножения.

Уравнение Ферхюльста или логистическое уравнение --- это
\[
    x' = rx \left(1 - \frac{x}{k} \right),
\]
и оно позволяет довольно точно описывать динамику численности населения.
Его решением является
\[
    x(t) = \frac{k \cdot x_0 \cdot e^{rt}}{k + x_0(e^{rt} - 1)},
\]
где $x_0 = x(0)$ --- начальная численность популяции.
У него есть два положения равновесия --- $0$ и $k$.

\subsection{Предсказание популяции хищников и жертв}
Пусть $x(t)$ --- популяция жертв, $y(t)$ --- популяция хищников, $\alpha$ --- коэффициент размножения, $\beta$ --- количество жертв, которое съедает хищник.
Тогда их можно описать системой
\[
    \begin{cases}
        x' = (\alpha - \beta y) x \\
        y' = (-a + bx) y
    \end{cases},
\]
где $a$ и $b$ --- константы.

\section{Приложения в математическом анализе}
\subsection{Теоремы о среднем для функций многих переменных}
Пусть $x_0 \in \mathbb R^n$, $R > 0$, $f: \mathbb R^n \to \mathbb R$ --- дифференцируемая функция, $B = B(x_0, R)$ --- шар, $S$ --- его граница.
Тогда если $f|_S = const$, то найдётся $\xi \in \Int(B)$, такая что $f'(\xi) = 0$ --- аналог теоремы Ролля.

\textbf{Теорема.} (Аналог теоремы Лагранжа) Найдётся $\xi \in B$, такая что 
\[
    |f'(\xi)| \le \frac{\sup_S(f) - \inf_S(f)}{2R}.
\]
В частности, отсюда следует аналог теоремы Ролля выше.

\textbf{Доказательство.} Умаляя общность, будем считать, что $f \in C^2$.
Предположим противное, пусть $\gamma = \frac{\sup_S(f) - \inf_S(f)}{2R}$, тогда для всех $\gamma \in B$ выполнено $|f'(\xi)| > \gamma$.
Тогда, в силу компактности шара, производную можно отделить от $\gamma$: найдётся $\varepsilon > 0$, такое что для всех $\gamma \in B$ верно $|f'(\xi)| \ge \gamma + \varepsilon$.

Рассмотрим задачу Коши
\[
    \begin{cases}
        x' = \frac{f'(x)}{|f'(x)|^2} \\
        x(0) = x_0
    \end{cases}
    .
\]
Так как $f \in C^2$, $\frac{f'(x)}{|f'(x)|^2} \in C^1$, так что можно применить теорему о существовании и единственности решения задачи Коши: существует интервал $I$, числа $a < 0$ и $b > 0$ и решение $x: I \to \mathbb R^n$, такие что:
\begin{itemize}
    \item $x$ является решением задачи Коши.
    \item $(a, b) \subset I$.
    \item $x(a), x(b) \in S$ и $x(t) \in \Int(B)$ для всех $t \in (a, b)$.
\end{itemize}

Посчитаем производную от $f(x(t))$:
\[
    \frac{d}{dt} f(x(t)) \equiv \left<f'(x(t)), x'(t) \right> \equiv \left<f'(x(t)), \frac{f'(x(t))}{|f'(x(t))|^2} \right> \equiv 1.
\]
Отсюда мы знаем, что $f(x(t))$ имеет вид $t + const$, откуда $f(x(b)) - f(x(0)) = b$ и $f(x(0)) - f(x(a)) = -a$.
Также мы знаем, что 
\[
    R = |x(b) - x(0)| = \left| \int_0^b x'(t) dt \right| \le \int_0^b \left| \frac{f'(x(t))}{|f'(x(t))|^2} \right| dt = \int_0^b \frac{dt}{|f'(x(t))|} \le
\]
Теперь по предположению, сделанному в начале, мы знаем, что $|f'(x(t))| \ge \gamma + \varepsilon$, то есть
\[
    \le \int_0^b \frac{1}{\gamma + \varepsilon} dt = \frac{b}{\gamma + \varepsilon}.
\]
Следовательно, $b \ge R(\gamma + \varepsilon)$ и аналогично $-a \ge R(\gamma + \varepsilon)$.
Тогда и $f(x(b)) - f(x(0)), f(x(0)) - f(x(a)) \ge R(\gamma + \varepsilon)$.
Сложим сии два неравенства: $f(x(b)) - f(x(a)) \ge 2R(\gamma + \varepsilon)$.
Теперь поймём, почему это противоречие, раскрыв $\gamma$:
\[
    f(x(b)) - f(x(a)) \ge 2R \varepsilon + \sup_S(f) - \inf_S(f).
\]
Но разность слева не превосходит $\sup_S(f) - \inf_S(f)$, просто из определения супремума и инфимума, --- противоречие.

\QED

\textbf{Теорема.} Пусть $x_0 = 0$, $f \in C^1$. Тогда существует $\xi \in B$, такая что
\[
    |f'(\xi)| \le \frac{\sup_B |f(x) - f(-x)|}{2R}.
\]
Её интерес заключается в том, что если функция $f$ чётная или близка к чётной, то оценка получается очень сильная.

\section{Приложения вариационного исчисления}
Пусть у нас на прямой стоит тележка. Изначально она стоит в $x(0) = 0$, и её скорость --- $x'(0) = 0$.
Мы хотим подвинуть её в точку $a > 0$ за минимальное время $T$ так, чтобы она не пролетела её, то есть $x'(T) = 0$ и $x(T) = a$.
Всё, что мы можем, --- это применять к ней силу $u(t)$, причём она ограничена: $|u(t)| \le \gamma$, то есть $x'' = u(t)$.
Интуитивно понятно, что надо до середины толкать изо всех сил вперёд, а потом --- назад.

Формализуем задачу: у нас есть множество функций
\[
    \{u(\cdot)~|~u:[0, T] \to \mathbb R, |u(t)| \le U, T > 0\},
\]
уравнение $x'' = u$, а также начальные условия $x(T) = a$, $x(0) = x'(T) = x'(0) = 0$.
Решать задачу не стали :(.

\setcounter{equation}{0}
\section{Лемма о выпрямлении траекторий}
Описание данной темы будет похоже на описание в учебнике Романко, но здесь будет более формально.

\textbf{Определение.} Пусть $\mathcal F, \mathcal G \subset \mathbb R^n$, отображение $f: \mathcal F \to \mathbb R^n$, $f \in C^1$.
Рассмотрим уравнение
\begin{equation}
    x' = f(x).
\end{equation}

\textit{Гладкая замена переменных} --- это отображение $\gamma: \mathcal G \to \mathcal F$, такое что оно взаимно однозначно, $\gamma, \gamma^{-1} \in C^1$ и для всех $y \in \mathcal G$ выполнено $\det(\gamma'(y)) \ne 0$.
(Условие на определитель можно убрать, ибо оно следует из остальных, но для наглядности оставим)

Зафиксируем гладкую замену переменных $\gamma$.

\textbf{Определение.} Система (1) на $\mathcal F$ \textit{принимает вид}
\begin{equation}
    y' = g(y)
\end{equation}
на $\mathcal G$, если для любого решения $y(\cdot)$ автономной системы (2) функция $x(\cdot) = \gamma(y(\cdot))$ является решением автономной системы (1).

\textbf{Утверждение.} Если система (1) на $\mathcal F$ принимает вид (2) на $\mathcal G$, то:
\begin{enumerate}
    \item Для любого решения $x(\cdot)$ автономной системы (1) функция $y(\cdot) = \gamma^{-1}(x(\cdot))$ является решением автономной системы (2).
    \item Отображение $g$ из системы (2) определяется однозначно.
    \item $g(y) \equiv (\gamma'(y))^{-1} f(\gamma(y))$ для $y \in \mathcal G$.
\end{enumerate}

\textbf{Доказательство.} 1) Пусть $x: I \to \mathbb R^n$ --- решение системы (1), $\tau \in I$ --- какое-то число.
Обозначим $\widehat x := x(\tau)$, $y(t) := \gamma^{-1}(x(t))$.
Будем доказывать, что это и есть решение, для этого рассмотрим задачу Коши
\[
    \begin{cases}
        y' = g(y) \\
        y(\tau) = \gamma^{-1}(\widehat x)
    \end{cases}.
\]
У неё есть единственное решение $y_g(\cdot)$, тогда по определению $\gamma(y_g(\cdot))$ является решением системы (1).
В частности, это решение задачи Коши
\[
    \begin{cases}
        x' = g(x) \\
        x(\tau) = \widehat x
    \end{cases}.
\]
Более того, ещё одним решением этой задачи является зафиксированный в самом начале $x: I \to \mathbb R^n$.
По теореме о единственности $x(t) \equiv \gamma(y_g(t))$.
Также мы взяли $y(\cdot)$ так, что $x(t) \equiv \gamma(y(t))$, откуда $y_g(t) \equiv y(t)$.

2) Пусть система (1) на $\mathcal F$ принимает вид
\begin{equation}
    y' = h(y)
\end{equation}
на $\mathcal G$. Докажем, что $h \equiv g$.
Для любого $\widehat y \in \mathcal G$ верно, что $y_g$ --- решение задачи Коши
\[
    \begin{cases}
        y' = g(t) \\
        y(0) = \widehat y
    \end{cases}
\]
и $y_h$ --- решение задачи Коши
\[
    \begin{cases}
        y' = h(t) \\
        y(0) = \widehat y
    \end{cases}.
\]
Тогда $\gamma(y_g(\cdot))$ и $\gamma(y_h(\cdot))$ --- два решения системы (1).
Кроме того, $\gamma(y_g(0)) = \gamma(\widehat y) = \gamma(y_h(0))$, откуда получается, что оба этих решения являются решениями задачи Коши
\[
    \begin{cases}
        x' = f(x) \\
        x(0) = \gamma(\widehat y)
    \end{cases}.
\]
Следовательно, они совпадают в окрестности нуля, а значит, $y_g \equiv y_h$ в окрестности нуля, $y_g'(0) = y_h'(0)$, и $g(y_g(0)) = h(y_h(0))$ из условий задач Коши, то есть $g(\widehat y) = h(\widehat y)$.
Это выполнено для любого $\widehat y \in \mathcal G$, откуда $g = h$.

3) Пусть $y(\cdot)$ --- решение системы (2), $\gamma(y(\cdot))$ --- решение (1).
Заметим, что
\[
    \frac{d}{dt} \gamma(y(t)) \equiv f(\gamma(y(t))).
\]
Тогда
\[
    \gamma'(y(t)) y'(t) \equiv f(\gamma(y(t))),
\]
то есть
\[
    y'(t) \equiv (\gamma'(y(t)))^{-1} f(\gamma(y(t))).
\]
Следовательно, $y(\cdot)$ является решением системы
\[
    y' = (\gamma'(y))^{-1} f(\gamma(y)),
\]
откуда по пункту 2 $g(y)$ может быть равно только правой части.

\QED

Пусть $\Omega \subset \mathbb R^n$ --- открытое множество, $f: \Gamma \to \mathbb R^n$, $f \in C^1$, а также $x^* = (x_1^*, \dots, x_n^*) \in \Omega$.

\textbf{Лемма.} Если $f(x^*) \ne 0$ (то есть не является положением равновесия), то найдётся окрестность $\mathcal F$ точки $x^*$, окрестность $\mathcal G$ точки $(x_1^*, \dots, x_{n-1}^*, 0) =: y^*$ и гладкая замена переменных $\gamma: \mathcal G \to \mathcal F$, такая что (1) на $\mathcal F$ принимает вид
\begin{equation}
    \begin{cases}
        y_1' = 0 \\
        \vdots \\
        y_{n-1}' = 0 \\
        y_n' = 1
    \end{cases}
\end{equation}
на $\mathcal G$.

Смысл утверждения: любой траектории $x(t)$ мы можем сопоставить ``прямую`` траекторию, у которой меняется только последняя координата.

\textbf{Доказательство.} Так как $f(x^*) \ne 0$, будем считать без ограничения общности, что $f_n(x^*) \ne 0$.
Пусть $\phi(t, \xi)$ --- решение задачи Коши
\[
    \begin{cases}
        x' = f(x) \\
        x(0) = \xi
    \end{cases},
\]
которое определено в какой-то окрестности точки $(0, x^*)$.
Положим
\[
    \gamma(y) := \phi(y_n, y_1, \dots, y_{n-1}, x_n^*),
\]
где $y$ берётся из окрестности точки $y^*$.
Найдём матрицу Якоби $\gamma'(y^*)$ по столбцам:
\[
    \gamma_{y_1}'(y^*) = \frac{\partial}{\partial y_1} \phi(0, y_1, x_2^*, \dots, x_{n-1}^*, x_n^*)|_{y_1 = x_1^*} =
    \begin{pmatrix}
        1 \\
        0 \\
        \vdots \\
        0
    \end{pmatrix}.
\]
Аналогично в столбцах $2, \dots, n - 1$ будет единица в соответствующей строке.
А для $y_n$ немного по-другому:
\[
    \gamma_{y_n}'(y^*) = \frac{\partial}{\partial y_n} \phi(y_n, x^*)|_{y_n = 0} = \frac{d}{dt} \phi(t, x^*)|_{t = 0} = f(\phi(0, x^*)) = f(x^*).
\]
Таким образом,
\[
    \gamma'(y^*) =
    \begin{pmatrix}
        1 & 0 & \dots & 0 & f_1(x^*) \\
        0 & 1 & \dots & 0 & f_2(x^*) \\
        \vdots & \vdots & \ddots & \vdots & \vdots \\
        0 & 0 & \dots & 1 & f_{n-1}(x^*) \\
        0 & 0 & \dots & 0 & f_n(x^*)
    \end{pmatrix}.
\]
Определитель равен $f_n(x^*) \ne 0$, то есть применима теорема об обратной функции: найдётся $\mathcal F$ --- окрестность $x^*$, $\mathcal G$ --- окрестность $y^*$ и гладкая замена переменных $\gamma: \mathcal G \to \mathcal F$.
Проверим, что на $\mathcal G$ система будет иметь искомый вид.
Возьмём решение системы (4):
\[
    y(t) \equiv
    \begin{pmatrix}
        c_1 \\
        \vdots \\
        c_{n-1} \\
        t + c_n
    \end{pmatrix}.
\]
Подставим в $\gamma$:
\[
    \gamma(y(t)) \equiv \phi(t + c_n, c_1, \dots, c_{n-1}, x_n^*)
\]
--- действительно решение системы (1).

\QED

\textbf{Замечание.} Эту лемму можно использовать для доказательства фактов про первые интегралы, ибо у системы (4) есть $n - 1$ независимый первый интеграл --- проекции на $y_1, \dots, y_{n-1}$.


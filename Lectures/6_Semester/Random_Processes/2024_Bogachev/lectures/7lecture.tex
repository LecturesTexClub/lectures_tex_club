\begin{definition}
	Пусть $T \in \{\N, \Z\}$. Случайный процесс $\xi$ называется \textit{предсказуемым относительно фильтрации $(\F_n)_{n \in \N}$}, если $\xi_n$ измеримы относительно $\F_{n - 1}$.
\end{definition}

\begin{proposition}
	Пусть $T = \N_0$, $\xi$ --- мартингал относительно $(\F_n)_{n \in \N_0}$. Тогда $\xi$ предсказуем относительно $(\F_n)_{n \in \N_0}$ тогда и только тогда, когда $\xi_n =\aal{P} \xi_0$.
	
	В частности, если $\xi_0 =\aal{P} 0$, то и $\xi_n =\aal{P} 0$.
\end{proposition}

\begin{proof}
	Из условия:
	\[
		\xi_n = \E(\xi_n | \F_{n - 1}) = [\text{опр. мартингала}] = \xi_{n - 1}
	\]
\end{proof}

\begin{theorem} (Разложение Дуба-Мейера)
	Пусть $T = \N_0$, $\xi$ --- случайный процесс, согласованный с фильтрацией $(\F_n)_{n \in \N_0}$, причём $\xi_n \in L_1(P)$. Тогда, существует единственное (с точностью $P$-почти наверное) разложение
	\[
		\xi_n = \mu_n + \pi_n
	\]
	где $\mu = (\mu_n)_{n \in \N_0}$ --- мартингал относительно $(\F_n)_{n \in \N_0}$, $\pi = (\pi_n)_{n \in \N_0}$ --- случайный процесс, предсказуемый относительно $(\F_n)_{n \in \N_0}$, причём $\pi_0 = 0$.
\end{theorem}

\begin{proof}~
	\begin{itemize}
		\item (Существование) Предъявим явную формулу для $\pi_n$:
		\[
			\pi_n = \sum_{k = 1}^n \E(\xi_k - \xi_{k - 1} | \F_{k - 1})
		\]
		Тогда $\mu_n := \xi_n - \pi_n$. Из определения сразу получаем, что $\pi$ предсказуем. Кроме того, $\pi_n \in L_1(P)$ по определению УМО. Из этого следует, что $\mu$ --- согласованный случайный процесс, причём $\mu_n \in L_1(P)$. Проверим, что $\mu$ является мартингалом:
		\[
			\E(\mu_n | \F_{n - 1}) = \E(\xi_n | \F_{n - 1}) - \pi_n
		\]
		Подставим определение $\pi_n$:
		\begin{multline*}
			\E(\mu_n | \F_{n - 1}) = \E(\xi_n | \F_{n - 1}) - \sum_{k = 1}^n \E(\xi_k - \xi_{k - 1} | \F_{k - 1}) =
			\\
			\E(\xi_{n - 1} | \F_{n - 1}) - \sum_{k = 1}^{n - 1} \E(\xi_k - \xi_{k - 1} | \F_{k - 1}) = \xi_{n - 1} - \pi_{n - 1} = \mu_{n - 1}
		\end{multline*}
		
		\item (Единственность) Пусть $\xi_n = \mu_n + \pi_n = \mu'_n + \pi'_n$. Тогда $\mu_n - \mu'_n = \pi'_n - \pi_n$. Слева написан мартингал, а справа --- предсказуемый процесс. Из вырождения предсказуемого мартингала и $\pi'_0 = \pi_0 = 0$ следует, что $\mu_n = \mu'_n$ и $\pi_n = \pi'_n$.
	\end{itemize}
\end{proof}

\section{Марковские моменты и теорема об остановке}

\begin{note}
	В этом параграфе всегда $T \subseteq \R$, $(\Omega, \F, P)$ --- вероятностное пространство.
\end{note}

\begin{definition}
	Пусть $\tau \colon \Omega \to T \cup \{+\infty\}$ --- случайная величина, $(\F_t)_{t \in T}$ --- фильтрация. Тогда $\tau$ называется \textit{марковским моментом относительно фильтрации $(\F_t)_{t \in T}$}, если
	\[
		\forall t \in T\ \ \{\omega \in \Omega \colon \tau(\omega) \le t\} \in \F_t
	\]
\end{definition}

\begin{definition}
	Пусть $\tau$ --- марковский момент. Если $\tau < +\infty$ $P$-почти наверное, то $\tau$ также называют \textit{моментом остановки}.
\end{definition}

\begin{example}
	Пусть $T = \N_0$, $\xi$ --- случайный процесс, $(\F_n)_{n \in \N_0}$ --- порождённая фильтрция. Можно рассмотреть $B \in \B(\R)$ и определить функцию первого момента попадания в это множество:
	\[
		\tau_B(\omega) = \min \{n \in \N_0 \colon \xi_n(\omega) \in B\}
	\]
	Если же таких $n$ нет, положим $\tau_B(\omega) = +\infty$. Естественно, получился марковский момент. Проверим это по определению:
	\[
		\forall n \in \N_0\ \ \{\omega \in \Omega \colon \tau(\omega) \le n\} = \{\omega \in \Omega \colon \exists m \le n,\ \xi_m(\omega) \in B\} = \bigcup_{m = 1}^n \{\omega \in \Omega \colon \xi_m(\omega) \in B\} \in \F_n
	\]
\end{example}

\begin{note}
	Конструкция выше --- один из основных примеров марковских моментов. Глобально идея состоит в том, что для понимания процесса нам не нужно изучать его на всём $T$, если мы знаем марковский момент.
\end{note}

\begin{theorem} (об остановке, дискретный случай)
	Пусть $T = \N_0$, $\xi$ --- мартингал относистельно фильтрации $(\F_n)_{n \in \N_0}$, $\tau$ --- ограниченный момент остановки. Тогда
	\[
		\E\xi_{\tau} = \E\xi_0
	\]
\end{theorem}

\begin{proof}
	По условию, $\exists m > 0 \such \forall \omega \in \Omega\ \ \tau(\omega) \le m$. Коль скоро это так, то (в каждом конкретном исходе $\xi_\tau$ будет просто равно какому-то слагаемому справа)
	\[
		|\xi_\tau| \le |\xi_0| \plusdots |\xi_m|
	\]
	А значит $\xi_\tau \in L_1(P)$ и $\E\xi_\tau$ --- конечное число. Чтобы посчитать это среднее, разобьём пространство $\Omega$ на подмножества $\{\tau = j\}$. Найдём эти средние:
	\begin{multline*}
		\E(\xi_\tau\chi_{\tau = j}) = \E(\xi_j\chi_{\tau = j}) = [\text{опр. мартингала}] =
		\\
		\E(\E(\xi_{j + 1} | \F_j)\chi_{\tau = j}) = [\{\tau = j\} \in \F_j] = \E(\xi_{j + 1}\chi_{\tau = j})
	\end{multline*}
	Продолжая рекурсивно, дойдём до равенства $\E(\xi_\tau\chi_{\tau = j}) = \E(\xi_m\chi_{\tau = j})$. Осталось просуммировать:
	\[
		\E\xi_\tau = \sum_{j = 0}^m \E(\xi_\tau\chi_{\tau = j}) = \sum_{j = 0}^m \E(\xi_m\chi_{\tau = j}) = \E\xi_m = \E\xi_0
	\]
\end{proof}

\begin{example}
	Где применима эта теорема? Рассмотрим пример с бросанием монеток. Пусть $\xi_n \colon \Omega \to \{\pm 1\}$, вероятность каждого исхода равна $1 / 2$. Пусть $S_n = \xi_1 \plusdots \xi_n$ --- выигрыш человека за $n$ бросков, $S_0 = 0$. Можно попробовать применить теорему об остановке, чтобы оценить момент, на котором нужно остановить игру для получия определённого выигрыша. Для начала, $(S_n)_{n \in \N_0}$ естественно является мартингалом. Положим за $\tau$ первый бросок, на котором игрок заработает ровно 2 очка:
	\[
		\tau(\omega) = \min\{n \in \N_0 \colon S_n = 2\}
	\]
	Тогда $\xi_\tau = 2$ всегда, а значит $\E\xi_\tau = 2 \neq 0 = \E\xi_0$. Противоречит ли этот результат теореме? Нет, потому что $\tau$ не ограничено. Если мы перейдём от такого момента к $\tau' = \min\{100, \tau\}$, то теорема будет снова выполнена.
\end{example}

\begin{definition}
	Пусть $T = \R_+$, $(F_t)_{t \ge 0}$ --- фильтрация. Отображение $\tau \colon \Omega \to T \cup \{+\infty\}$ называется \textit{опциональным моментом}, если
	\[
		\forall t \in \R_+\ \ \{\omega \in \Omega \colon \tau(\omega) < t\} \in \F_t
	\]
\end{definition}

\begin{theorem} (об остановке, непрерывный случай, без доказательства)
	Пусть $T = \R_+$, $\xi$ --- мартингал относительно фильтрации $(\F_t)_{t \ge 0}$, причём траектории $\xi$ непрерывны справа, $\tau$ --- ограниченный опциональный момент. Тогда
	\[
		\E\xi_\tau = \E\xi_0
	\]
\end{theorem}

\begin{problem} (о разорении игрока)
	Пусть $\{\xi_n\}_{n = 1}^\infty$ --- независимые случайные величины, $\xi_n \colon \Omega \to \{\pm 1\}$, $P(\xi_n = 1) = p$, $p + q = 1$ и $a, x, b \in \Z, a < x < b$. Положим $S_0 = x$, $S_n = x + \xi_1 \plusdots \xi_n$.
	
	$x$ --- это стартовый капитал человека, $b$ --- капитал самого богатого банка, $a$ --- это некоторый минимальный порог. Если в результате действий на рынке игрок получает $x' = b$, то он выводит деньги и разоряет банк. Если же баланс игрока доходит до минимального порога $x' = a$, то он проигрывает.
	
	С какой вероятностью игрок проиграет?
\end{problem}

\begin{solution}
	Положим за $\tau$ --- момент завершения игры:
	\[
		\tau(\omega) = \min\{n \in \N_0 \colon S_n \in \{a, b\}\}
	\]
	Должно быть понятно, что $\tau$ --- это марковский момент. Является ли это моментом остановки? Если $\tau(\omega) = +\infty$, то значит, что $a < S_n < b$ для всех $n \in \N_0$. Но тогда
	\[
		\frac{S_n}{\sqrt{n}} \xrightarrow[n \to \infty]{} 0
	\]
	Отсюда, если $\tau$ принимает это значение на множестве ненулевой меры, возникает противоречие с Центральной Предельной Теоремой. Теперь, рассмотрим 2 ситуации:
	\begin{itemize}
		\item $p = q = 1 / 2$. Тогда $(S_n)_{n \in \N_0}$ --- мартингал относительно фильтрации, порождённой $\{\xi_n\}_{n = 1}^\infty$. Хотелось бы воспользоваться теоремой об остановке, но пока не доказано, что $\tau$ ограничен. Тем не менее, она применима для $\tau_k = \min\{k, \tau\}$. Тогда $\E S_{\tau_k} = \E S_0 = x$, при этом $\tau_k$ --- возрастающая последовательность, подходящая к $\tau$, а потому $S_{\tau_k} \to\aal{P} S_\tau$. При этом имеется сходимость матожиданий по теореме Лебега, так как $|S_{\tau_k}| \le \max\{|a|, |b|\}$, а значит
		\[
			\E S_\tau = \lim_{k \to \infty} \E S_{\tau_k} = x
		\]
		При этом $\E S_\tau = aP(S_\tau = a) + bP(S_\tau = b) = x$. Отсюда
		\[
			P(S_\tau = a) = \frac{b - x}{b - a}
		\]
		
		\item $p \neq q$. Можно рассмотреть $\Pi_n = (p / q)^{S_n}$ на роль мартингала. Проверим это:
		\begin{multline*}
			\E(\Pi_{n + 1} | \F_n) = \E\ps{\ps{\frac{p}{q}}^{S_{n + 1}} \Big| \F_n} = \Pi_n\E((p / q)^{\xi_{n + 1}} | \F_n) =
			\\
			[\text{независимость $\xi_n$}] = \Pi_n \E(p / q)^{\xi_{n + 1}} = [\text{матож считаем по опр.}]= \Pi_n
		\end{multline*}
		С учётом оценки $|\Pi_{\tau_k}| \le \min\{(p / q)^a, (p / q)^b\}$, получаем равенство $\E\Pi_\tau = \E\Pi_0 = (p / q)^x$. В итоге
		\[
			P(\Pi_\tau = a) = \frac{(p / q)^b - (p / q)^x}{(p / q)^b - (p / q)^a}
		\]
	\end{itemize}
\end{solution}

\begin{note}
	Из анализа $p = q = 1 / 2$ видно, что игрок будет разоряться в не более чем $50\%$ случаев только тогда, когда $x \approx \frac{1}{2}(a + b)$. Если, скажем, $a = 0$, то игроку нужно иметь половину денег банка.
\end{note}

\section{Марковские процессы}

\begin{note}
	До этого момента, все процессы строились на базе случайных величин. Естественно, мир этим не ограничивается, и марковские процессы оказываются более осмысленными, если их строить на основе случайных элементов.
	
	Далее $(\Omega, \F, P)$ --- вероятностное пространство, $(E, \cE)$ --- измеримое пространство.
\end{note}

\begin{reminder}
	\textit{Случайным элементом $\xi \colon \Omega \to E$} называется любое измеримое относительно $\F|\cE$ отображение.
\end{reminder}

\begin{note}
	Определение случайного процесса не меняется, но отсюда и далее $T \subseteq \R$.
\end{note}

\begin{note}
	Пусть $\xi$ --- это случайный процесс. Тогда разумно говорить о следующих $\sigma$-алгебрах:
	\begin{itemize}
		\item $\F_{\le t} = \sigma(\xi_s, s \le t)$ --- \textit{$\sigma$-алгебра прошлого}
		
		\item $\F_{= t} = \sigma(\xi_t)$ --- \textit{$\sigma$-алгебра настоящего}
		
		\item $\F_{\ge t} = \sigma(\xi_s, s \ge t)$ --- \textit{$\sigma$-алгебра будущего}
	\end{itemize}
\end{note}

\begin{reminder}
	\textit{Условной вероятностью события} $A \in \F$ относительно под-$\sigma$-алгебры $\cC$ называется следующее УМО:
	\[
	P(A | \cC) = \E(\chi_A | \cC)
	\]
\end{reminder}

\begin{definition}
	Случайный процесс $\xi$ называется \textit{марковским}, если
	\[
		\forall t \in T\ \forall A \in \F_{\le t}, B \in \F_{\ge t}\ \ P(A \cap B | \F_{= t}) = P(A | \F_{= t}) \cdot P(B | \F_{= t})
	\]
\end{definition}

\begin{note}
	Смысл этого определения в том, что прошлое и будущее марковского процесса независимы относительно настоящего момента.
\end{note}
